{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207ce239-13e0-4a81-b059-b18af42b0382",
   "metadata": {},
   "source": [
    "Train a transformer model to convert decimal numbers from roman literals, ex:\n",
    "\n",
    "LVII=57\n",
    "\n",
    "https://en.wikipedia.org/wiki/Roman_numerals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea61e43d-787a-49d4-88f0-3f8561ed5c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Train, empty_config, LogFlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475dfd33-91bc-4ad5-840a-744ad238f128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New random seed 2050019466\n",
      "Initializing new model roman2dec\n",
      "Dataset train_path: ../data/roman2decimal10000.txt, val_path: None, train_split: 0.8999, vocab_size: 19\n",
      "Model params: 0.90M\n"
     ]
    }
   ],
   "source": [
    "ben = Train('roman2dec')\n",
    "\n",
    "# set datasets\n",
    "ben.set_datasets('padlinechar', train_path='../data/roman2decimal10000.txt', train_split=(9000-1)/10000) # -1 because numbers start at 1\n",
    "\n",
    "# set config settings\n",
    "cfg = empty_config()\n",
    "cfg.train.log_period=0\n",
    "cfg.model.set(n_layer=8, n_head=8, n_embd=96, block_size=26)\n",
    "cfg.sample.set(top=1, max_batch_size=256) # top_k(1) - always pick the best item\n",
    "cfg.train.set(sample_period=-5)\n",
    "cfg.trainer.set(batch_size=128)\n",
    "\n",
    "# and init a new model with config\n",
    "if ben.can_resume() and False:\n",
    "    ben.init_resume(cfg)\n",
    "else:\n",
    "    ben.init_new(cfg)\n",
    "# print(do.get_config().dump(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b9399c-5ff0-4069-823e-28c5d38054d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MMMMMMMMM=9000\\nMMMMMMMMMI=9001\\nMMMMMMMMMII=9002\\nMMMMMMMMMIII=9003\\nMMMM'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ben.val_dataset.get_src_data()[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cccbcff-9607-4509-9e07-40e4b431087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Batches per epoch: 70\n",
      "iter 0 (0.000 epoch): loss train=2.4947, val=2.6246, eval->2.6246\n",
      "==> Saving model at iter=0, eval loss->2.6246 \n",
      "9X\n",
      ".CUDA max memory used: 380.12M\n",
      "...................................................................................................iter 100 (1.422 epoch): loss train=1.1859, val=1.3579, eval->1.3579\n",
      "==> Saving model at iter=100, eval loss->1.3579 \n",
      "....................................................................................................iter 200 (2.845 epoch): loss train=0.8872, val=1.0967, eval->1.0967\n",
      "==> Saving model at iter=200, eval loss->1.0967 \n",
      "....................................................................................................iter 300 (4.267 epoch): loss train=0.7197, val=0.9320, eval->0.9320\n",
      "==> Saving model at iter=300, eval loss->0.9320 \n",
      "....................................................................................................iter 400 (5.690 epoch): loss train=0.6164, val=0.8905, eval->0.8905\n",
      "==> Saving model at iter=400, eval loss->0.8905 \n",
      "....................................................................................................iter 500 (7.112 epoch): loss train=0.5334, val=0.9015, eval->0.9015\n",
      "6\n",
      "....................................................................................................iter 600 (8.534 epoch): loss train=0.4753, val=0.8361, eval->0.8361\n",
      "==> Saving model at iter=600, eval loss->0.8361 \n",
      "....................................................................................................iter 700 (9.957 epoch): loss train=0.4330, val=0.7836, eval->0.7836\n",
      "==> Saving model at iter=700, eval loss->0.7836 \n",
      "....................................................................................................iter 800 (11.379 epoch): loss train=0.3969, val=0.8145, eval->0.8145\n",
      "....................................................................................................iter 900 (12.801 epoch): loss train=0.3710, val=0.8611, eval->0.8611\n",
      "....................................................................................................iter 1000 (14.224 epoch): loss train=0.3597, val=0.8521, eval->0.8521\n",
      "4MMMMMMMCCCLXXXXII=8382\n",
      "....................................................................................................iter 1100 (15.646 epoch): loss train=0.3538, val=0.9207, eval->0.9207\n",
      "....................................................................................................iter 1200 (17.069 epoch): loss train=0.3461, val=0.8406, eval->0.8406\n",
      "....................................................................................................iter 1300 (18.491 epoch): loss train=0.3424, val=0.9087, eval->0.9087\n",
      "....................................................................................................iter 1400 (19.913 epoch): loss train=0.3439, val=0.8526, eval->0.8526\n",
      "....................................................................................................iter 1500 (21.336 epoch): loss train=0.3407, val=0.8390, eval->0.8390\n",
      "XXII=22\n",
      "....................................................................................................iter 1600 (22.758 epoch): loss train=0.3401, val=0.9057, eval->0.9057\n",
      "....................................................................................................iter 1700 (24.180 epoch): loss train=0.3402, val=0.8949, eval->0.8949\n",
      "....................................................................................................iter 1800 (25.603 epoch): loss train=0.3399, val=0.9170, eval->0.9170\n",
      "....................................................................................................iter 1900 (27.025 epoch): loss train=0.3380, val=0.9703, eval->0.9703\n",
      "....................................................................................................iter 2000 (28.448 epoch): loss train=0.3371, val=0.9426, eval->0.9426\n",
      "6\n",
      "....................................................................................................iter 2100 (29.870 epoch): loss train=0.3363, val=0.9948, eval->0.9948\n",
      "....................................................................................................iter 2200 (31.292 epoch): loss train=0.3362, val=1.0210, eval->1.0210\n",
      "....................................................................................................iter 2300 (32.715 epoch): loss train=0.3360, val=1.0274, eval->1.0274\n",
      "....................................................................................................iter 2400 (34.137 epoch): loss train=0.3359, val=1.0601, eval->1.0601\n",
      "....................................................................................................iter 2500 (35.560 epoch): loss train=0.3347, val=0.9905, eval->0.9905\n",
      "5\n",
      "....................................................................................................iter 2600 (36.982 epoch): loss train=0.3361, val=0.9988, eval->0.9988\n",
      "....................................................................................................iter 2700 (38.404 epoch): loss train=0.3359, val=1.0550, eval->1.0550\n",
      "....................................................................................................iter 2800 (39.827 epoch): loss train=0.3344, val=1.0521, eval->1.0521\n",
      "....................................................................................................iter 2900 (41.249 epoch): loss train=0.3344, val=1.0241, eval->1.0241\n",
      "....................................................................................................iter 3000 (42.671 epoch): loss train=0.3352, val=1.0396, eval->1.0396\n",
      "=10\n",
      "....................................................................................................iter 3100 (44.094 epoch): loss train=0.3345, val=1.0257, eval->1.0257\n",
      "....................................................................................................iter 3200 (45.516 epoch): loss train=0.3350, val=1.0474, eval->1.0474\n",
      "....................................................................................................iter 3300 (46.939 epoch): loss train=0.3350, val=1.0655, eval->1.0655\n",
      "............................................................................................."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mben\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43miter_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\work\\ai\\mesa\\gptbench\\gptbench\\train.py:168\u001b[0m, in \u001b[0;36mTrain.train\u001b[1;34m(self, trainer_batch_size, iter_count, batch_end_callback, **over_train_config_kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mset_callback(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_batch_end\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m trainer: batch_end_callback(trainer, \u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# run the optimization\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_iter_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miter_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m saved_train_config\n",
      "File \u001b[1;32mc:\\work\\ai\\mesa\\gptbench\\gptbench\\trainer.py:186\u001b[0m, in \u001b[0;36mTrainer.run\u001b[1;34m(self, run_sample_count, run_iter_count)\u001b[0m\n\u001b[0;32m    183\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# forward the model\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# backprop and update the parameters\u001b[39;00m\n\u001b[0;32m    189\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\a\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\work\\ai\\mesa\\gptbench\\gptbench\\model.py:257\u001b[0m, in \u001b[0;36mGPT.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m    255\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdrop(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[1;32m--> 257\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[0;32m    259\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\a\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\work\\ai\\mesa\\gptbench\\gptbench\\model.py:421\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 421\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlpf(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\a\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\work\\ai\\mesa\\gptbench\\gptbench\\model.py:395\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    393\u001b[0m att \u001b[38;5;241m=\u001b[39m att\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias[:,:,:T,:T] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    394\u001b[0m att \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(att, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 395\u001b[0m att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_dropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43matt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    396\u001b[0m y \u001b[38;5;241m=\u001b[39m att \u001b[38;5;241m@\u001b[39m v \u001b[38;5;66;03m# (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\u001b[39;00m\n\u001b[0;32m    397\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(B, T, C) \u001b[38;5;66;03m# re-assemble all head outputs side by side\u001b[39;00m\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\a\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\a\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\a\\Lib\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ben.train(iter_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4151573-9621-4875-b6dc-5d2ca138bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "['13000=: MMMMMMMMMMMMM != MMMMMMMMMMMM', '13001=: MMMMMMMMMMMMMI != MMMMMMMMMMMMI', '13002=: MMMMMMMMMMMMMII != MMMMMMMMMMMMII', '13003=: MMMMMMMMMMMMMIII != MMMMMMMMMMMMIII', '13004=: MMMMMMMMMMMMMIV != MMMMMMMMMMMMIV', '13005=: MMMMMMMMMMMMMV != MMMMMMMMMMMMV', '13006=: MMMMMMMMMMMMMVI != MMMMMMMMMMMMVI', '13007=: MMMMMMMMMMMMMVII != MMMMMMMMMMMMVII', '13008=: MMMMMMMMMMMMMVIII != MMMMMMMMMMMMVIII', '13009=: MMMMMMMMMMMMMIX != MMMMMMMMMMMMIX', '13010=: MMMMMMMMMMMMMX != MMMMMMMMMMMMCC', '13011=: MMMMMMMMMMMMMXI != MMMMMMMMMMMMCCI', '13012=: MMMMMMMMMMMMMXII != MMMMMMMMMMMMCCII', '13013=: MMMMMMMMMMMMMXIII != MMMMMMMMMMMMXIII', '13014=: MMMMMMMMMMMMMXIV != MMMMMMMMMMMMXIV', '13015=: MMMMMMMMMMMMMXV != MMMMMMMMMMMMXV', '13016=: MMMMMMMMMMMMMXVI != MMMMMMMMMMMMCCVI', '13017=: MMMMMMMMMMMMMXVII != MMMMMMMMMMMMCCVII', '13018=: MMMMMMMMMMMMMXVIII != MMMMMMMMMMMMXVIII', '13019=: MMMMMMMMMMMMMXIX != MMMMMMMMMMMMCCIX']\n"
     ]
    }
   ],
   "source": [
    "ds = ben.val_dataset\n",
    "q,a=ds.sample_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "errs = []\n",
    "def test(q,a,g):\n",
    "    global errs\n",
    "    \n",
    "    res = float(a == g)\n",
    "    if not res:\n",
    "        errs += [f\"{q}: {a} != {g}\"]\n",
    "    return res\n",
    "    \n",
    "print(ben.measure_accuracy(q,a, test_fn=test))\n",
    "print(len(errs),errs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e04716-56ca-4fb0-8c71-3208b23dea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994443827091899\n",
      "5 ['II=: 2 != 1', 'III=: 3 != 1', 'IV=: 4 != 1', 'IX=: 9 != 10', 'MM=: 2000 != 1000']\n"
     ]
    }
   ],
   "source": [
    "ds = ben.train_dataset\n",
    "q,a=ds.sample_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "errs = []\n",
    "def test(q,a,g):\n",
    "    global errs\n",
    "    \n",
    "    res = float(a == g)\n",
    "    if not res:\n",
    "        errs += [f\"{q}: {a} != {g}\"]\n",
    "    return res\n",
    "    \n",
    "print(ben.measure_accuracy(q,a, test_fn=test))\n",
    "print(len(errs), errs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1237b12-c59d-4a18-80e7-8c5ce8c75471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 434176,\n",
       " 'train_loss': 0.33498454093933105,\n",
       " 'val_loss': 1.0654536485671997,\n",
       " 'eval_loss': 1.0654536485671997}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ben.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ecc29b5-bdb6-49e8-9f63-34fd9c51dab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New random seed 766280474\n",
      "Loading checkpoint from ./models/roman2dec/\n",
      "Checkpoint: iter=700 (9.957 epoch), loss train=0.4330 val=0.7836 eval->0.7836\n",
      "Dataset train_path: ../data/roman2decimal10000.txt, val_path: None, train_split: 0.8999, vocab_size: 19\n",
      "Model params: 0.90M\n"
     ]
    }
   ],
   "source": [
    "ben.init_resume(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ab1e89-8b73-4df0-ad4f-efefdd8e5541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1001 ['MMMMMMMMM=: 9000 != 8000', 'MMMMMMMMMI=: 9001 != 8009', 'MMMMMMMMMII=: 9002 != 8002', 'MMMMMMMMMIII=: 9003 != 8003', 'MMMMMMMMMIV=: 9004 != 8004', 'MMMMMMMMMV=: 9005 != 8005', 'MMMMMMMMMVI=: 9006 != 8006', 'MMMMMMMMMVII=: 9007 != 8007', 'MMMMMMMMMVIII=: 9008 != 8008', 'MMMMMMMMMIX=: 9009 != 801', 'MMMMMMMMMX=: 9010 != 8010', 'MMMMMMMMMXI=: 9011 != 8019', 'MMMMMMMMMXII=: 9012 != 8013', 'MMMMMMMMMXIII=: 9013 != 8013', 'MMMMMMMMMXIV=: 9014 != 8014', 'MMMMMMMMMXV=: 9015 != 8015', 'MMMMMMMMMXVI=: 9016 != 8016', 'MMMMMMMMMXVII=: 9017 != 8017', 'MMMMMMMMMXVIII=: 9018 != 8018', 'MMMMMMMMMXIX=: 9019 != 8099']\n"
     ]
    }
   ],
   "source": [
    "ds = ben.val_dataset\n",
    "q,a=ds.sample_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "errs = []\n",
    "def test(q,a,g):\n",
    "    global errs\n",
    "    \n",
    "    res = float(a == g)\n",
    "    if not res:\n",
    "        errs += [f\"{q}: {a} != {g}\"]\n",
    "    return res\n",
    "    \n",
    "print(ben.measure_accuracy(q,a, test_fn=test))\n",
    "print(len(errs),errs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c799511a-aef3-41af-96ea-6d2ced319aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7003000333370375\n",
      "2697 ['I=: 1 != 10', 'II=: 2 != 9', 'IV=: 4 != 104', 'V=: 5 != 50', 'VI=: 6 != 106', 'VII=: 7 != 107', 'VIII=: 8 != 1', 'IX=: 9 != 109', 'XII=: 12 != 41', 'XIII=: 13 != 43', 'XIV=: 14 != 41', 'XV=: 15 != 40', 'XVI=: 16 != 41', 'XVII=: 17 != 43', 'XVIII=: 18 != 48', 'XIX=: 19 != 99', 'XX=: 20 != 19', 'XXI=: 21 != 49', 'XXII=: 22 != 42', 'XXIII=: 23 != 43']\n"
     ]
    }
   ],
   "source": [
    "ds = ben.train_dataset\n",
    "q,a=ds.sample_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "errs = []\n",
    "def test(q,a,g):\n",
    "    global errs\n",
    "    \n",
    "    res = float(a == g)\n",
    "    if not res:\n",
    "        errs += [f\"{q}: {a} != {g}\"]\n",
    "    return res\n",
    "    \n",
    "print(ben.measure_accuracy(q,a, test_fn=test))\n",
    "print(len(errs), errs[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
