{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207ce239-13e0-4a81-b059-b18af42b0382",
   "metadata": {},
   "source": [
    "Train a transformer model to convert decimal numbers from roman literals, ex:\n",
    "\n",
    "LVII=57\n",
    "\n",
    "https://en.wikipedia.org/wiki/Roman_numerals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea61e43d-787a-49d4-88f0-3f8561ed5c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Train, empty_config, LogFlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475dfd33-91bc-4ad5-840a-744ad238f128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new model roman2dec\n",
      "Dataset train_path: ../data/roman2decimal10k.txt, val_path: None, train_split: 0.8999, vocab_size: 19\n",
      "Model params: 0.59M\n"
     ]
    }
   ],
   "source": [
    "ben = Train('roman2dec', seed=0xb0bb1a5)\n",
    "\n",
    "# set datasets\n",
    "ben.set_datasets(class_name='charline', \n",
    "                 train_path='../data/roman2decimal10k.txt', \n",
    "                 train_split=(9000-1)/10000,\n",
    "                 pre_shuffle=True) # -1 because numbers start at 1\n",
    "\n",
    "# set config settings\n",
    "cfg = empty_config()\n",
    "cfg.model.set(n_layer=6, n_head=6, n_embd=90, block_size=32)\n",
    "cfg.sample.set(top=1, max_batch_size=256) # top_k(1) - always pick the best item\n",
    "cfg.train.set(log_period=0, sample_period=-5)\n",
    "cfg.trainer.set(batch_size=128)\n",
    "\n",
    "# and init a new model with config\n",
    "ben.init_new(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b9399c-5ff0-4069-823e-28c5d38054d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MMMMMMMMDCCCV=8805',\n",
       " 'MMMMMMDCCLXXIV=6774',\n",
       " 'MMMMMMDXIII=6513',\n",
       " 'MCMXI=1911',\n",
       " 'MMMMMMMMMDCVII=9607',\n",
       " 'MMMMMMMMMDXXXVI=9536',\n",
       " 'MMMMDCIII=4603',\n",
       " 'MMMMLXXIX=4079',\n",
       " 'MMDCCLXXXV=2785',\n",
       " 'MMMMMMMMCDXXII=8422',\n",
       " 'MMMMMMMCCLXIX=7269',\n",
       " 'MMDXXVII=2527',\n",
       " 'MMMMMDCCX=5710',\n",
       " 'MMMCDXLII=3442',\n",
       " 'MMMMMMDLXXXIV=6584',\n",
       " 'MDCCVII=1707',\n",
       " 'MMMMMMCMLXXIX=6979',\n",
       " 'MMMCCIII=3203',\n",
       " 'DCLXV=665',\n",
       " 'MMMMMMMMCCCLVI=8356']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ben.val_dataset.get_data()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cccbcff-9607-4509-9e07-40e4b431087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iters per epoch: 70\n",
      "iter 0 (0.000 epoch): loss train=2.5839, val=2.5864, eval->2.5864\n",
      "==> Saving model at iter=0, eval loss->2.5864 \n",
      "VVMMMMMMC5MMMMMMMMMMMMMDMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM\n",
      "CUDA max memory used: 331.81M\n",
      "....................................................................................................iter 100 (1.422 epoch): loss train=1.1103, val=1.1166, eval->1.1166\n",
      "==> Saving model at iter=100, eval loss->1.1166 \n",
      "....................................................................................................iter 200 (2.845 epoch): loss train=0.7847, val=0.7901, eval->0.7901\n",
      "==> Saving model at iter=200, eval loss->0.7901 \n",
      "....................................................................................................iter 300 (4.267 epoch): loss train=0.6495, val=0.6543, eval->0.6543\n",
      "==> Saving model at iter=300, eval loss->0.6543 \n",
      "....................................................................................................iter 400 (5.690 epoch): loss train=0.5529, val=0.5563, eval->0.5563\n",
      "==> Saving model at iter=400, eval loss->0.5563 \n",
      "....................................................................................................iter 500 (7.112 epoch): loss train=0.4787, val=0.4819, eval->0.4819\n",
      "==> Saving model at iter=500, eval loss->0.4819 \n",
      "LXXII=48\n",
      "....................................................................................................iter 600 (8.534 epoch): loss train=0.4253, val=0.4289, eval->0.4289\n",
      "==> Saving model at iter=600, eval loss->0.4289 \n",
      "....................................................................................................iter 700 (9.957 epoch): loss train=0.3863, val=0.3885, eval->0.3885\n",
      "==> Saving model at iter=700, eval loss->0.3885 \n",
      "....................................................................................................iter 800 (11.379 epoch): loss train=0.3520, val=0.3548, eval->0.3548\n",
      "==> Saving model at iter=800, eval loss->0.3548 \n",
      "....................................................................................................iter 900 (12.801 epoch): loss train=0.3286, val=0.3311, eval->0.3311\n",
      "==> Saving model at iter=900, eval loss->0.3311 \n",
      "....................................................................................................iter 1000 (14.224 epoch): loss train=0.3095, val=0.3117, eval->0.3117\n",
      "==> Saving model at iter=1000, eval loss->0.3117 \n",
      "2MMMMMMMMCCCXXXII=9332\n",
      "....................................................................................................iter 1100 (15.646 epoch): loss train=0.3032, val=0.3048, eval->0.3048\n",
      "==> Saving model at iter=1100, eval loss->0.3048 \n",
      "....................................................................................................iter 1200 (17.069 epoch): loss train=0.2935, val=0.2950, eval->0.2950\n",
      "==> Saving model at iter=1200, eval loss->0.2950 \n",
      "....................................................................................................iter 1300 (18.491 epoch): loss train=0.2901, val=0.2930, eval->0.2930\n",
      "==> Saving model at iter=1300, eval loss->0.2930 \n",
      "....................................................................................................iter 1400 (19.913 epoch): loss train=0.2857, val=0.2878, eval->0.2878\n",
      "==> Saving model at iter=1400, eval loss->0.2878 \n",
      "....................................................................................................iter 1500 (21.336 epoch): loss train=0.2838, val=0.2860, eval->0.2860\n",
      "==> Saving model at iter=1500, eval loss->0.2860 \n",
      "4\n",
      "....................................................................................................iter 1600 (22.758 epoch): loss train=0.2815, val=0.2833, eval->0.2833\n",
      "==> Saving model at iter=1600, eval loss->0.2833 \n",
      "....................................................................................................iter 1700 (24.180 epoch): loss train=0.2815, val=0.2840, eval->0.2840\n",
      "....................................................................................................iter 1800 (25.603 epoch): loss train=0.2802, val=0.2831, eval->0.2831\n",
      "==> Saving model at iter=1800, eval loss->0.2831 \n",
      "....................................................................................................iter 1900 (27.025 epoch): loss train=0.2795, val=0.2826, eval->0.2826\n",
      "==> Saving model at iter=1900, eval loss->0.2826 \n",
      "....................................................................................................iter 2000 (28.448 epoch): loss train=0.2796, val=0.2831, eval->0.2831\n",
      "1MMMMMMMMCCCXCIII=9393\n",
      "....................................................................................................iter 2100 (29.870 epoch): loss train=0.2786, val=0.2811, eval->0.2811\n",
      "==> Saving model at iter=2100, eval loss->0.2811 \n",
      "....................................................................................................iter 2200 (31.292 epoch): loss train=0.2772, val=0.2801, eval->0.2801\n",
      "==> Saving model at iter=2200, eval loss->0.2801 \n",
      "....................................................................................................iter 2300 (32.715 epoch): loss train=0.2776, val=0.2797, eval->0.2797\n",
      "==> Saving model at iter=2300, eval loss->0.2797 \n",
      "....................................................................................................iter 2400 (34.137 epoch): loss train=0.2778, val=0.2802, eval->0.2802\n",
      "....................................................................................................iter 2500 (35.560 epoch): loss train=0.2765, val=0.2794, eval->0.2794\n",
      "==> Saving model at iter=2500, eval loss->0.2794 \n",
      "CCCXXXVIII=338\n",
      "....................................................................................................iter 2600 (36.982 epoch): loss train=0.2780, val=0.2801, eval->0.2801\n",
      "....................................................................................................iter 2700 (38.404 epoch): loss train=0.2761, val=0.2792, eval->0.2792\n",
      "==> Saving model at iter=2700, eval loss->0.2792 \n",
      "....................................................................................................iter 2800 (39.827 epoch): loss train=0.2777, val=0.2798, eval->0.2798\n",
      "....................................................................................................iter 2900 (41.249 epoch): loss train=0.2771, val=0.2795, eval->0.2795\n",
      "....................................................................................................iter 3000 (42.671 epoch): loss train=0.2767, val=0.2796, eval->0.2796\n",
      "\u0000\n",
      "....................................................................................................iter 3100 (44.094 epoch): loss train=0.2786, val=0.2802, eval->0.2802\n",
      "....................................................................................................iter 3200 (45.516 epoch): loss train=0.2764, val=0.2787, eval->0.2787\n",
      "==> Saving model at iter=3200, eval loss->0.2787 \n",
      "....................................................................................................iter 3300 (46.939 epoch): loss train=0.2761, val=0.2789, eval->0.2789\n",
      "....................................................................................................iter 3400 (48.361 epoch): loss train=0.2764, val=0.2785, eval->0.2785\n",
      "==> Saving model at iter=3400, eval loss->0.2785 \n",
      "....................................................................................................iter 3500 (49.783 epoch): loss train=0.2759, val=0.2785, eval->0.2785\n",
      "6\n",
      "....................................................................................................iter 3600 (51.206 epoch): loss train=0.2763, val=0.2790, eval->0.2790\n",
      "....................................................................................................iter 3700 (52.628 epoch): loss train=0.2762, val=0.2796, eval->0.2796\n",
      "....................................................................................................iter 3800 (54.050 epoch): loss train=0.2756, val=0.2788, eval->0.2788\n",
      "....................................................................................................iter 3900 (55.473 epoch): loss train=0.2767, val=0.2782, eval->0.2782\n",
      "==> Saving model at iter=3900, eval loss->0.2782 \n",
      "....................................................................................................iter 4000 (56.895 epoch): loss train=0.2756, val=0.2785, eval->0.2785\n",
      "VII=7\n",
      "....................................................................................................iter 4100 (58.318 epoch): loss train=0.2755, val=0.2784, eval->0.2784\n",
      "....................................................................................................iter 4200 (59.740 epoch): loss train=0.2761, val=0.2783, eval->0.2783\n",
      "....................................................................................................iter 4300 (61.162 epoch): loss train=0.2758, val=0.2782, eval->0.2782\n",
      "==> Saving model at iter=4300, eval loss->0.2782 \n",
      "....................................................................................................iter 4400 (62.585 epoch): loss train=0.2759, val=0.2784, eval->0.2784\n",
      "....................................................................................................iter 4500 (64.007 epoch): loss train=0.2756, val=0.2780, eval->0.2780\n",
      "==> Saving model at iter=4500, eval loss->0.2780 \n",
      "=00\n",
      "....................................................................................................iter 4600 (65.429 epoch): loss train=0.2757, val=0.2782, eval->0.2782\n",
      "....................................................................................................iter 4700 (66.852 epoch): loss train=0.2764, val=0.2788, eval->0.2788\n",
      "....................................................................................................iter 4800 (68.274 epoch): loss train=0.2759, val=0.2784, eval->0.2784\n",
      "....................................................................................................iter 4900 (69.697 epoch): loss train=0.2763, val=0.2787, eval->0.2787\n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "ben.train(iter_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4151573-9621-4875-b6dc-5d2ca138bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999000999000999\n",
      "1/1001 errors: ['VI==6 != 5']\n"
     ]
    }
   ],
   "source": [
    "ds = ben.val_dataset\n",
    "q,a=ds.get_data_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "errs = []\n",
    "def test(q,a,g):\n",
    "    global errs\n",
    "    \n",
    "    res = float(a == g)\n",
    "    if not res:\n",
    "        errs += [f\"{q}={a} != {g}\"]\n",
    "    return res\n",
    "    \n",
    "print(ben.measure_accuracy(q,a, test_fn=test))\n",
    "print(f'{len(errs)}/{len(ds)} errors: {errs[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7e04716-56ca-4fb0-8c71-3208b23dea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993332592510279\n",
      "6 ['III==3 != 2', 'VIII==8 != 08', 'MMMMMMMMMM==10000 != 9000', 'VII==7 != 07', 'IX==9 != 1', 'II==2 != 1']\n"
     ]
    }
   ],
   "source": [
    "ds = ben.train_dataset\n",
    "q,a=ds.get_data_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "errs = []\n",
    "print(ben.measure_accuracy(q,a, test_fn=test))\n",
    "print(f'{len(errs)}/{len(ds)} errors: {errs[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a9c72-85d6-4092-b570-aecc30500e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a bit more?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
