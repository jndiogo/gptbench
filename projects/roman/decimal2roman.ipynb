{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207ce239-13e0-4a81-b059-b18af42b0382",
   "metadata": {},
   "source": [
    "Train a transformer model to convert decimal numbers to roman numerals, ex:\n",
    "56=LVI\n",
    "\n",
    "We'll pass '56=' as the starting text and look at what the model outputs after the '=' character.\n",
    "\n",
    "To create training+validation data, run in the ../dataprep folder:\n",
    "```\n",
    "python prepare_roman.py ../data/decimal2roman10k.txt 10000 --sep=\\n\n",
    "```\n",
    "The script creates decimal2roman10k.txt with entries in the form decimal=roman, one per line.\n",
    "\n",
    "About roman numerals: https://en.wikipedia.org/wiki/Roman_numerals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea61e43d-787a-49d4-88f0-3f8561ed5c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Train, empty_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475dfd33-91bc-4ad5-840a-744ad238f128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new model dec2roman\n",
      "Dataset train_path: ../data/decimal2roman10k.txt, val_path: None, train_split: 0.8999, vocab_size: 19\n",
      "Model params: 0.59M\n"
     ]
    }
   ],
   "source": [
    "ben = Train('dec2roman', seed=0xbeebaca)\n",
    "\n",
    "# set training log periods to avoid cluttering the training output\n",
    "ben.set_train_log_periods(sample_period=500, dot_period=1, loss_period=0)\n",
    "\n",
    "# set datasets: shuffle the datasets before splitting\n",
    "ben.set_datasets(class_name='charline', \n",
    "                 train_path='../data/decimal2roman10k.txt', \n",
    "                 train_split=(9000-1)/10000,\n",
    "                 pre_shuffle=True)\n",
    "\n",
    "# set config settings\n",
    "cfg = empty_config()\n",
    "cfg.model.set(n_layer=6, n_head=6, n_embd=90, block_size=32)\n",
    "cfg.trainer.set(batch_size=128)\n",
    "cfg.sample.set(top=1, max_batch_size=256) # top_k(1) - always pick the best item\n",
    "\n",
    "# and init a new model with config\n",
    "ben.init_new(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9b9399c-5ff0-4069-823e-28c5d38054d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2209=MMCCIX',\n",
       " '5913=MMMMMCMXIII',\n",
       " '507=DVII',\n",
       " '8029=MMMMMMMMXXIX',\n",
       " '3685=MMMDCLXXXV',\n",
       " '7422=MMMMMMMCDXXII',\n",
       " '8805=MMMMMMMMDCCCV',\n",
       " '8390=MMMMMMMMCCCXC',\n",
       " '4128=MMMMCXXVIII',\n",
       " '7937=MMMMMMMCMXXXVII',\n",
       " '4076=MMMMLXXVI',\n",
       " '8075=MMMMMMMMLXXV',\n",
       " '5783=MMMMMDCCLXXXIII',\n",
       " '6607=MMMMMMDCVII',\n",
       " '3620=MMMDCXX',\n",
       " '6623=MMMMMMDCXXIII',\n",
       " '651=DCLI',\n",
       " '2822=MMDCCCXXII',\n",
       " '7117=MMMMMMMCXVII',\n",
       " '9709=MMMMMMMMMDCCIX']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a peek at the validation dataset\n",
    "ben.val_dataset.get_data()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a6db11-3bf2-4063-a733-f126c7355dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iters per epoch: 70\n",
      "Iter 0 (0.000 epoch): loss train=2.6152, val=2.6174, eval->2.6174\n",
      "==> Saving model at iter=0, eval loss->2.6174 \n",
      "Sampling: D\n",
      "CUDA max memory used: 331.81M\n",
      "...................................................................................................\n",
      "Iter 100 (1.422 epoch): loss train=1.1048, val=1.1043, eval->1.1043\n",
      "==> Saving model at iter=100, eval loss->1.1043 \n",
      "...................................................................................................\n",
      "Iter 200 (2.845 epoch): loss train=0.7043, val=0.7023, eval->0.7023\n",
      "==> Saving model at iter=200, eval loss->0.7023 \n",
      "...................................................................................................\n",
      "Iter 300 (4.267 epoch): loss train=0.5250, val=0.5249, eval->0.5249\n",
      "==> Saving model at iter=300, eval loss->0.5249 \n",
      "...................................................................................................\n",
      "Iter 400 (5.690 epoch): loss train=0.4119, val=0.4125, eval->0.4125\n",
      "==> Saving model at iter=400, eval loss->0.4125 \n",
      "...................................................................................................\n",
      "Iter 500 (7.112 epoch): loss train=0.3333, val=0.3324, eval->0.3324\n",
      "==> Saving model at iter=500, eval loss->0.3324 \n",
      "Sampling: 8030=MMMMMMMMMXXX\n",
      "...................................................................................................\n",
      "Iter 600 (8.534 epoch): loss train=0.2782, val=0.2787, eval->0.2787\n",
      "==> Saving model at iter=600, eval loss->0.2787 \n",
      "...................................................................................................\n",
      "Iter 700 (9.957 epoch): loss train=0.2572, val=0.2564, eval->0.2564\n",
      "==> Saving model at iter=700, eval loss->0.2564 \n",
      "...................................................................................................\n",
      "Iter 800 (11.379 epoch): loss train=0.2446, val=0.2455, eval->0.2455\n",
      "==> Saving model at iter=800, eval loss->0.2455 \n",
      "...................................................................................................\n",
      "Iter 900 (12.801 epoch): loss train=0.2381, val=0.2378, eval->0.2378\n",
      "==> Saving model at iter=900, eval loss->0.2378 \n",
      "...................................................................................................\n",
      "Iter 1000 (14.224 epoch): loss train=0.2326, val=0.2321, eval->0.2321\n",
      "==> Saving model at iter=1000, eval loss->0.2321 \n",
      "Sampling: 4444=MMMMCDXLIV\n",
      "...................................................................................................\n",
      "Iter 1100 (15.646 epoch): loss train=0.2289, val=0.2286, eval->0.2286\n",
      "==> Saving model at iter=1100, eval loss->0.2286 \n",
      "...................................................................................................\n",
      "Iter 1200 (17.069 epoch): loss train=0.2267, val=0.2265, eval->0.2265\n",
      "==> Saving model at iter=1200, eval loss->0.2265 \n",
      "...................................................................................................\n",
      "Iter 1300 (18.491 epoch): loss train=0.2247, val=0.2247, eval->0.2247\n",
      "==> Saving model at iter=1300, eval loss->0.2247 \n",
      "...................................................................................................\n",
      "Iter 1400 (19.913 epoch): loss train=0.2232, val=0.2235, eval->0.2235\n",
      "==> Saving model at iter=1400, eval loss->0.2235 \n",
      "...................................................................................................\n",
      "Iter 1500 (21.336 epoch): loss train=0.2232, val=0.2237, eval->0.2237\n",
      "Sampling: 7335=MMMMMMMCCCXXXV\n",
      "...................................................................................................\n",
      "Iter 1600 (22.758 epoch): loss train=0.2229, val=0.2233, eval->0.2233\n",
      "==> Saving model at iter=1600, eval loss->0.2233 \n",
      "...................................................................................................\n",
      "Iter 1700 (24.180 epoch): loss train=0.2217, val=0.2224, eval->0.2224\n",
      "==> Saving model at iter=1700, eval loss->0.2224 \n",
      "...................................................................................................\n",
      "Iter 1800 (25.603 epoch): loss train=0.2214, val=0.2216, eval->0.2216\n",
      "==> Saving model at iter=1800, eval loss->0.2216 \n",
      "...................................................................................................\n",
      "Iter 1900 (27.025 epoch): loss train=0.2210, val=0.2213, eval->0.2213\n",
      "==> Saving model at iter=1900, eval loss->0.2213 \n",
      "...................................................................................................\n",
      "Iter 2000 (28.448 epoch): loss train=0.2206, val=0.2213, eval->0.2213\n",
      "Sampling: 3999=MMMCMXCIX\n",
      "...................................................................................................\n",
      "Iter 2100 (29.870 epoch): loss train=0.2204, val=0.2208, eval->0.2208\n",
      "==> Saving model at iter=2100, eval loss->0.2208 \n",
      "...................................................................................................\n",
      "Iter 2200 (31.292 epoch): loss train=0.2206, val=0.2210, eval->0.2210\n",
      "...................................................................................................\n",
      "Iter 2300 (32.715 epoch): loss train=0.2204, val=0.2208, eval->0.2208\n",
      "...................................................................................................\n",
      "Iter 2400 (34.137 epoch): loss train=0.2201, val=0.2204, eval->0.2204\n",
      "==> Saving model at iter=2400, eval loss->0.2204 \n",
      "...................................................................................................\n",
      "Iter 2500 (35.560 epoch): loss train=0.2208, val=0.2208, eval->0.2208\n",
      "Sampling: 1008=MVIII\n",
      "...................................................................................................\n",
      "Iter 2600 (36.982 epoch): loss train=0.2205, val=0.2205, eval->0.2205\n",
      "...................................................................................................\n",
      "Iter 2700 (38.404 epoch): loss train=0.2198, val=0.2204, eval->0.2204\n",
      "==> Saving model at iter=2700, eval loss->0.2204 \n",
      "...................................................................................................\n",
      "Iter 2800 (39.827 epoch): loss train=0.2198, val=0.2203, eval->0.2203\n",
      "==> Saving model at iter=2800, eval loss->0.2203 \n",
      "...................................................................................................\n",
      "Iter 2900 (41.249 epoch): loss train=0.2199, val=0.2202, eval->0.2202\n",
      "==> Saving model at iter=2900, eval loss->0.2202 \n",
      "..................................................................................................."
     ]
    }
   ],
   "source": [
    "# let's train for this many iters:\n",
    "ben.train(iter_count=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "752255de-ed0e-41e6-ad15-ec780026390f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ./checkpoints/dec2roman/\n",
      "Checkpoint: iter=2900 (41.249 epoch), loss train=0.2199 val=0.2202 eval->0.2202\n",
      "Dataset train_path: ../data/decimal2roman10k.txt, val_path: None, train_split: 0.8999, vocab_size: 19\n",
      "Model params: 0.59M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_samples': 371200,\n",
       " 'train_loss': 0.2198736071586609,\n",
       " 'val_loss': 0.22023971378803253,\n",
       " 'eval_loss': 0.22023971378803253}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load the best saved checkpoint. Train and validation losses are almost equal which is good.\n",
    "ben.load()\n",
    "ben.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4151573-9621-4875-b6dc-5d2ca138bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998001998001998\n",
      "2/1001 errors: ['331=CCCXXXI != CCCXXI', '4=IV != I']\n"
     ]
    }
   ],
   "source": [
    "# To capture  accuracy test entries, we could simply pass a log_list and receive the bad (or good) results.\n",
    "# But we can also pass a custom test function, that can also capture accuracy test entries:\n",
    "ds = ben.val_dataset\n",
    "q,a=ds.get_data_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "errs = []\n",
    "def test(q,a,g):\n",
    "    global errs\n",
    "    \n",
    "    res = float(a == g)\n",
    "    if not res:\n",
    "        errs += [f\"{q}{a} != {g}\"]\n",
    "    return res\n",
    "    \n",
    "print(ben.measure_accuracy(q,a, test_fn=test))\n",
    "print(f'{len(errs)}/{len(ds)} errors: {errs[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a03c77c-f16a-485e-a5e9-b69e641a8002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9958884320480054\n",
      "37/8999 errors: ['831=DCCCXXXI != DCCCXXI', '37=XXXVII != XXVII', '21=XXI != XII', '79=LXXIX != LXIX', '381=CCCLXXXI != CCCLXXI', '33=XXXIII != XXIII', '36=XXXVI != XXVI', '881=DCCCLXXXI != DCCCLXXI', '39=XXXIX != XXIX', '96=XCVI != XVI', '9=IX != I', '3=III != II', '989=CMLXXXIX != CMLXXIX', '89=LXXXIX != XXXIX', '31=XXXI != XXII', '26=XXVI != XVI', '46=XLVI != XVIV', '38=XXXVIII != XXVIII', '75=LXXV != LXV', '481=CDLXXXI != CDLXXI']\n"
     ]
    }
   ],
   "source": [
    "# Almost 100% accuracy, 2 errors out of 1001 entries. Not perfect but not bad for validation data, which was unseen during traning.\n",
    "# What about the train dataset's accuracy?\n",
    "ds = ben.train_dataset\n",
    "q,a=ds.get_data_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "errs = []\n",
    "print(ben.measure_accuracy(q,a, test_fn=test))\n",
    "print(f'{len(errs)}/{len(ds)} errors: {errs[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54f89b27-d8d1-4b36-878c-fefcb884ce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17=XVII\n",
      "225=CCXXV\n",
      "999=CMXCIX\n",
      "9999=MMMMMMMMMCMXCIX\n"
     ]
    }
   ],
   "source": [
    "# Also near 100% accuracy.\n",
    "#Let's take a few samples:\n",
    "ben.sample('17=')\n",
    "ben.sample('225=')\n",
    "ben.sample('999=')\n",
    "ben.sample('9999=')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6befd94-161c-4027-8292-252b8e9864bd",
   "metadata": {},
   "source": [
    "Would more training get us to 100% accuracy?\n",
    "\n",
    "Also see the roman2decimal notebook for the inverse mapping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
