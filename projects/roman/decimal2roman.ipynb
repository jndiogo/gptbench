{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207ce239-13e0-4a81-b059-b18af42b0382",
   "metadata": {},
   "source": [
    "Train a transformer model to convert decimal numbers to roman literals, ex:\n",
    "56=LVI\n",
    "https://en.wikipedia.org/wiki/Roman_numerals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea61e43d-787a-49d4-88f0-3f8561ed5c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Train, empty_config, LogFlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475dfd33-91bc-4ad5-840a-744ad238f128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New random seed 3355343838\n",
      "Initializing new model dec2roman\n",
      "Dataset train_path: ../data/decimal2roman15000.txt, val_path: None, train_split: 0.8666, vocab_size: 19\n",
      "Model params: 0.90M\n"
     ]
    }
   ],
   "source": [
    "ben = Train('dec2roman')\n",
    "\n",
    "# set datasets\n",
    "ben.set_datasets('padlinechar', train_path='../data/decimal2roman15000.txt', train_split=(13000-1)/15000) # -1 because numbers start at 1\n",
    "\n",
    "# set config settings\n",
    "cfg = empty_config()\n",
    "cfg.train.log_period=0\n",
    "cfg.model.set(n_layer=8, n_head=8, n_embd=96, block_size=32)\n",
    "cfg.sample.set(top=1, max_batch_size=256) # top_k(1) - always pick the best item\n",
    "cfg.train.set(sample_period=-5)\n",
    "cfg.trainer.set(batch_size=128)\n",
    "\n",
    "# and init a new model with config\n",
    "if ben.can_load() and False:\n",
    "    ben.load(cfg)\n",
    "else:\n",
    "    ben.init_new(cfg)\n",
    "# print(do.get_config().dump(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b9399c-5ff0-4069-823e-28c5d38054d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13000=MMMMMMMMMMMMM\\n13001=MMMMMMMMMMMMMI\\n13002=MMMMMMMMMMMMMII\\n13003=M'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ben.val_dataset.get_src_data()[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50a6db11-3bf2-4063-a733-f126c7355dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Batches per epoch: 101\n",
      "iter 100 (0.985 epoch): loss train=1.0587, val=1.2057, eval->1.2057\n",
      "==> Saving model at iter=100, eval loss->1.2057 \n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "ben.train(iter_count=100)\n",
    "ben.save('locus19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5881c03c-034e-4c4f-b491-b01ba9dd3d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'locus19'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ben.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7cccbcff-9607-4509-9e07-40e4b431087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Batches per epoch: 101\n",
      "iter 0 (0.000 epoch): loss train=2.4836, val=2.6190, eval->2.6190\n",
      "==> Saving model at iter=0, eval loss->2.6190 \n",
      "L84==MMM8MM8\n",
      ".CUDA max memory used: 545.82M\n",
      "...................................................................................................iter 100 (0.985 epoch): loss train=1.0064, val=1.1340, eval->1.1340\n",
      "==> Saving model at iter=100, eval loss->1.1340 \n",
      "....................................................................................................iter 200 (1.969 epoch): loss train=0.6987, val=0.8803, eval->0.8803\n",
      "==> Saving model at iter=200, eval loss->0.8803 \n",
      "....................................................................................................iter 300 (2.954 epoch): loss train=0.5517, val=0.7506, eval->0.7506\n",
      "==> Saving model at iter=300, eval loss->0.7506 \n",
      "....................................................................................................iter 400 (3.939 epoch): loss train=0.4692, val=0.7291, eval->0.7291\n",
      "==> Saving model at iter=400, eval loss->0.7291 \n",
      "....................................................................................................iter 500 (4.923 epoch): loss train=0.3994, val=0.7385, eval->0.7385\n",
      "L111=MMCI\n",
      "....................................................................................................iter 600 (5.908 epoch): loss train=0.3542, val=0.7349, eval->0.7349\n",
      "....................................................................................................iter 700 (6.893 epoch): loss train=0.3052, val=0.6755, eval->0.6755\n",
      "==> Saving model at iter=700, eval loss->0.6755 \n",
      "....................................................................................................iter 800 (7.878 epoch): loss train=0.2856, val=0.6962, eval->0.6962\n",
      "....................................................................................................iter 900 (8.862 epoch): loss train=0.2621, val=0.7098, eval->0.7098\n",
      "....................................................................................................iter 1000 (9.847 epoch): loss train=0.2551, val=0.6836, eval->0.6836\n",
      "5243=MMMMMCCXLIII\n",
      "....................................................................................................iter 1100 (10.832 epoch): loss train=0.2487, val=0.7870, eval->0.7870\n",
      "....................................................................................................iter 1200 (11.816 epoch): loss train=0.2478, val=0.7218, eval->0.7218\n",
      "....................................................................................................iter 1300 (12.801 epoch): loss train=0.2409, val=0.7829, eval->0.7829\n",
      "....................................................................................................iter 1400 (13.786 epoch): loss train=0.2402, val=0.8235, eval->0.8235\n",
      "....................................................................................................iter 1500 (14.770 epoch): loss train=0.2385, val=0.8067, eval->0.8067\n",
      "12225=MMMMMMMMMMMMCCXXV\n",
      "....................................................................................................iter 1600 (15.755 epoch): loss train=0.2374, val=0.7736, eval->0.7736\n",
      "....................................................................................................iter 1700 (16.740 epoch): loss train=0.2358, val=0.8555, eval->0.8555\n",
      "....................................................................................................iter 1800 (17.724 epoch): loss train=0.2342, val=0.9134, eval->0.9134\n",
      "....................................................................................................iter 1900 (18.709 epoch): loss train=0.2343, val=0.8379, eval->0.8379\n",
      "....................................................................................................iter 2000 (19.694 epoch): loss train=0.2339, val=0.8660, eval->0.8660\n",
      "10598=MMMMMMMMMMDXCVIII\n",
      "....................................................................................................iter 2100 (20.679 epoch): loss train=0.2337, val=0.8379, eval->0.8379\n",
      "....................................................................................................iter 2200 (21.663 epoch): loss train=0.2339, val=1.0408, eval->1.0408\n",
      "....................................................................................................iter 2300 (22.648 epoch): loss train=0.2334, val=0.9293, eval->0.9293\n",
      "....................................................................................................iter 2400 (23.633 epoch): loss train=0.2335, val=0.8576, eval->0.8576\n",
      "....................................................................................................iter 2500 (24.617 epoch): loss train=0.2335, val=1.0266, eval->1.0266\n",
      "8111=MMMMMMMMCXI\n",
      "....................................................................................................iter 2600 (25.602 epoch): loss train=0.2336, val=0.9654, eval->0.9654\n",
      "....................................................................................................iter 2700 (26.587 epoch): loss train=0.2327, val=0.9953, eval->0.9953\n",
      "....................................................................................................iter 2800 (27.571 epoch): loss train=0.2327, val=1.0411, eval->1.0411\n",
      "....................................................................................................iter 2900 (28.556 epoch): loss train=0.2331, val=0.8370, eval->0.8370\n",
      "....................................................................................................iter 3000 (29.541 epoch): loss train=0.2332, val=0.9239, eval->0.9239\n",
      "=I\n",
      "....................................................................................................iter 3100 (30.525 epoch): loss train=0.2327, val=1.0674, eval->1.0674\n",
      "....................................................................................................iter 3200 (31.510 epoch): loss train=0.2328, val=1.0288, eval->1.0288\n",
      "....................................................................................................iter 3300 (32.495 epoch): loss train=0.2325, val=1.0165, eval->1.0165\n",
      "....................................................................................................iter 3400 (33.479 epoch): loss train=0.2329, val=1.0656, eval->1.0656\n",
      "....................................................................................................iter 3500 (34.464 epoch): loss train=0.2328, val=1.0072, eval->1.0072\n",
      "M183=MMCLXXXIII\n",
      "....................................................................................................iter 3600 (35.449 epoch): loss train=0.2329, val=1.0854, eval->1.0854\n",
      "....................................................................................................iter 3700 (36.434 epoch): loss train=0.2325, val=1.0437, eval->1.0437\n",
      "....................................................................................................iter 3800 (37.418 epoch): loss train=0.2330, val=1.1399, eval->1.1399\n",
      "....................................................................................................iter 3900 (38.403 epoch): loss train=0.2328, val=1.2154, eval->1.2154\n",
      ".........................................."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mben\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43miter_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\work\\ai\\mesa\\gptbench\\gptbench\\train.py:168\u001b[0m, in \u001b[0;36mTrain.train\u001b[1;34m(self, trainer_batch_size, iter_count, batch_end_callback, **over_train_config_kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mset_callback(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_batch_end\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m trainer: batch_end_callback(trainer, \u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# run the optimization\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_iter_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miter_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m saved_train_config\n",
      "File \u001b[1;32mc:\\work\\ai\\mesa\\gptbench\\gptbench\\trainer.py:190\u001b[0m, in \u001b[0;36mTrainer.run\u001b[1;34m(self, run_sample_count, run_iter_count)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# backprop and update the parameters\u001b[39;00m\n\u001b[0;32m    189\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 190\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mgrad_norm_clip)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\a\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\mambaforge\\envs\\a\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ben.train(iter_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4151573-9621-4875-b6dc-5d2ca138bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "['13000=: MMMMMMMMMMMMM != MMMMMMMMMMMM', '13001=: MMMMMMMMMMMMMI != MMMMMMMMMMMMI', '13002=: MMMMMMMMMMMMMII != MMMMMMMMMMMMII', '13003=: MMMMMMMMMMMMMIII != MMMMMMMMMMMMIII', '13004=: MMMMMMMMMMMMMIV != MMMMMMMMMMMMIV', '13005=: MMMMMMMMMMMMMV != MMMMMMMMMMMMV', '13006=: MMMMMMMMMMMMMVI != MMMMMMMMMMMMVI', '13007=: MMMMMMMMMMMMMVII != MMMMMMMMMMMMVII', '13008=: MMMMMMMMMMMMMVIII != MMMMMMMMMMMMVIII', '13009=: MMMMMMMMMMMMMIX != MMMMMMMMMMMMIX', '13010=: MMMMMMMMMMMMMX != MMMMMMMMMMMMCC', '13011=: MMMMMMMMMMMMMXI != MMMMMMMMMMMMCCI', '13012=: MMMMMMMMMMMMMXII != MMMMMMMMMMMMCCII', '13013=: MMMMMMMMMMMMMXIII != MMMMMMMMMMMMXIII', '13014=: MMMMMMMMMMMMMXIV != MMMMMMMMMMMMXIV', '13015=: MMMMMMMMMMMMMXV != MMMMMMMMMMMMXV', '13016=: MMMMMMMMMMMMMXVI != MMMMMMMMMMMMCCVI', '13017=: MMMMMMMMMMMMMXVII != MMMMMMMMMMMMCCVII', '13018=: MMMMMMMMMMMMMXVIII != MMMMMMMMMMMMXVIII', '13019=: MMMMMMMMMMMMMXIX != MMMMMMMMMMMMCCIX']\n"
     ]
    }
   ],
   "source": [
    "ds = ben.val_dataset\n",
    "q,a=ds.sample_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "errs = []\n",
    "def test(q,a,g):\n",
    "    global errs\n",
    "    \n",
    "    res = float(a == g)\n",
    "    if not res:\n",
    "        errs += [f\"{q}: {a} != {g}\"]\n",
    "    return res\n",
    "    \n",
    "print(ben.measure_accuracy(q,a, test_fn=test))\n",
    "print(len(errs), errs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a03c77c-f16a-485e-a5e9-b69e641a8002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9989229940764675\n",
      "['3=: III != II', '8=: VIII != VII', '10=: X != ', '12=: XII != XI', '13=: XIII != XII', '16=: XVI != XV', '18=: XVIII != XVII', '40=: XL != X', '44=: XLIV != XLIX', '46=: XLVI != XLV', '54=: LIV != LIX', '56=: LVI != LV', '94=: XCIV != XCIX', '126=: CXXVI != CXXV']\n"
     ]
    }
   ],
   "source": [
    "ds = ben.train_dataset\n",
    "q,a=ds.sample_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "errs = []\n",
    "def test(q,a,g):\n",
    "    global errs\n",
    "    \n",
    "    res = float(a == g)\n",
    "    if not res:\n",
    "        errs += [f\"{q}: {a} != {g}\"]\n",
    "    return res\n",
    "    \n",
    "print(ben.measure_accuracy(q,a, test_fn=test))\n",
    "print(len(errs), errs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ecc29b5-bdb6-49e8-9f63-34fd9c51dab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New random seed 640745423\n",
      "Loading checkpoint from ./models/num2roman/\n",
      "Checkpoint: iter=700 (6.893 epoch), loss train=0.3052 val=0.6755 eval->0.6755\n",
      "Dataset train_path: ../data/roman15000.txt, val_path: None, train_split: 0.8666, vocab_size: 19\n",
      "Model params: 0.90M\n"
     ]
    }
   ],
   "source": [
    "# now resuming from best validation checkpoint\n",
    "ben.init_resume(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1be952d1-1204-4139-805c-9e7cf3e85e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "2001 ['13000=: MMMMMMMMMMMMM != MMMMMMMMMMMCC', '13001=: MMMMMMMMMMMMMI != MMMMMMMMMMMCCC', '13002=: MMMMMMMMMMMMMII != MMMMMMMMMMMCCII', '13003=: MMMMMMMMMMMMMIII != MMMMMMMMMMMCCIII', '13004=: MMMMMMMMMMMMMIV != MMMMMMMMMMMCCIV', '13005=: MMMMMMMMMMMMMV != MMMMMMMMMMMCCC', '13006=: MMMMMMMMMMMMMVI != MMMMMMMMMMMCCVI', '13007=: MMMMMMMMMMMMMVII != MMMMMMMMMMMCCVII', '13008=: MMMMMMMMMMMMMVIII != MMMMMMMMMMMMCCVIII', '13009=: MMMMMMMMMMMMMIX != MMMMMMMMMMMCCC', '13010=: MMMMMMMMMMMMMX != MMMMMMMMMMMCCC', '13011=: MMMMMMMMMMMMMXI != MMMMMMMMMMMCCCI', '13012=: MMMMMMMMMMMMMXII != MMMMMMMMMMMMCCCII', '13013=: MMMMMMMMMMMMMXIII != MMMMMMMMMMMCCCIII', '13014=: MMMMMMMMMMMMMXIV != MMMMMMMMMMMMCCIV', '13015=: MMMMMMMMMMMMMXV != MMMMMMMMMMMMCCL', '13016=: MMMMMMMMMMMMMXVI != MMMMMMMMMMMCCCVI', '13017=: MMMMMMMMMMMMMXVII != MMMMMMMMMMMMCCLVII', '13018=: MMMMMMMMMMMMMXVIII != MMMMMMMMMMMMCCLXIII', '13019=: MMMMMMMMMMMMMXIX != MMMMMMMMMMMCCCIX']\n"
     ]
    }
   ],
   "source": [
    "ds = ben.val_dataset\n",
    "q,a=ds.sample_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "errs = []\n",
    "def test(q,a,g):\n",
    "    global errs\n",
    "    \n",
    "    res = float(a == g)\n",
    "    if not res:\n",
    "        errs += [f\"{q}: {a} != {g}\"]\n",
    "    return res\n",
    "    \n",
    "print(ben.measure_accuracy(q,a, test_fn=test))\n",
    "print(len(errs), errs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "263a56ec-cb40-418a-b1fe-97097bad8781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5991999384568044\n",
      "5210 ['1=: I != XI', '2=: II != XI', '3=: III != XI', '4=: IV != XI', '5=: V != XV', '6=: VI != XV', '7=: VII != LXIX', '8=: VIII != LXXII', '9=: IX != X7=MMMMMMMXVII', '12=: XII != XIX', '13=: XIII != XXII', '15=: XV != LI', '16=: XVI != LXII', '17=: XVII != LXXII', '18=: XVIII != LXXXII', '20=: XX != XV', '21=: XXI != XIX', '23=: XXIII != XXXII', '24=: XXIV != XLII', '25=: XXV != LIX']\n"
     ]
    }
   ],
   "source": [
    "ds = ben.train_dataset\n",
    "q,a=ds.sample_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "errs = []\n",
    "def test(q,a,g):\n",
    "    global errs\n",
    "    \n",
    "    res = float(a == g)\n",
    "    if not res:\n",
    "        errs += [f\"{q}: {a} != {g}\"]\n",
    "    return res\n",
    "    \n",
    "print(ben.measure_accuracy(q,a, test_fn=test))\n",
    "print(len(errs), errs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "643e6c42-7827-44fd-a2e1-26a9da8cc8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New random seed 1529726838\n",
      "Initializing new model dec2roman2\n",
      "Dataset train_path: ../data/decimal2roman15000.txt, val_path: None, train_split: 0.8666, vocab_size: 19\n",
      "Model params: 3.57M\n"
     ]
    }
   ],
   "source": [
    "ben = Train('dec2roman2')\n",
    "\n",
    "# set datasets\n",
    "ben.set_datasets('padlinechar', train_path='../data/decimal2roman15000.txt', train_split=(13000-1)/15000) # -1 because numbers start at 1\n",
    "\n",
    "# set config settings\n",
    "cfg = empty_config()\n",
    "cfg.train.log_period=0\n",
    "cfg.model.set(n_layer=8, n_head=8, n_embd=192, block_size=32, dropout=0.25)\n",
    "cfg.sample.set(top=1, max_batch_size=256) # top_k(1) - always pick the best item\n",
    "cfg.train.set(sample_period=-5)\n",
    "cfg.trainer.set(batch_size=128)\n",
    "\n",
    "# and init a new model with config\n",
    "if ben.can_resume() and False:\n",
    "    ben.init_resume(cfg)\n",
    "else:\n",
    "    ben.init_new(cfg)\n",
    "# print(do.get_config().dump(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b83ec374-9dc0-47d4-b8a5-89b9a9be7f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Batches per epoch: 101\n",
      "iter 0 (0.000 epoch): loss train=1.8668, val=2.2132, eval->2.2132\n",
      "==> Saving model at iter=0, eval loss->2.2132 \n",
      "=\n",
      ".CUDA max memory used: 861.27M\n",
      "...................................................................................................iter 100 (0.985 epoch): loss train=0.5731, val=0.7859, eval->0.7859\n",
      "==> Saving model at iter=100, eval loss->0.7859 \n",
      "....................................................................................................iter 200 (1.969 epoch): loss train=0.4833, val=0.6533, eval->0.6533\n",
      "==> Saving model at iter=200, eval loss->0.6533 \n",
      "....................................................................................................iter 300 (2.954 epoch): loss train=0.4139, val=0.6942, eval->0.6942\n",
      "....................................................................................................iter 400 (3.939 epoch): loss train=0.3139, val=0.7174, eval->0.7174\n",
      "....................................................................................................iter 500 (4.923 epoch): loss train=0.2787, val=0.6840, eval->0.6840\n",
      "D088=MMMMMMMMMLXXXVIII\n",
      "....................................................................................................iter 600 (5.908 epoch): loss train=0.2575, val=0.6685, eval->0.6685\n",
      "....................................................................................................iter 700 (6.893 epoch): loss train=0.2495, val=0.6717, eval->0.6717\n",
      "....................................................................................................iter 800 (7.878 epoch): loss train=0.2441, val=0.7291, eval->0.7291\n",
      "....................................................................................................iter 900 (8.862 epoch): loss train=0.2379, val=0.7594, eval->0.7594\n",
      "....................................................................................................iter 1000 (9.847 epoch): loss train=0.2378, val=0.7354, eval->0.7354\n",
      "D009=MMMMMMMIX\n",
      "....................................................................................................iter 1100 (10.832 epoch): loss train=0.2364, val=0.8014, eval->0.8014\n",
      "....................................................................................................iter 1200 (11.816 epoch): loss train=0.2348, val=0.8366, eval->0.8366\n",
      "....................................................................................................iter 1300 (12.801 epoch): loss train=0.2341, val=0.8184, eval->0.8184\n",
      "....................................................................................................iter 1400 (13.786 epoch): loss train=0.2338, val=0.8243, eval->0.8243\n",
      "....................................................................................................iter 1500 (14.770 epoch): loss train=0.2343, val=0.9036, eval->0.9036\n",
      "V174=MMMMMCLXXIV\n",
      "....................................................................................................iter 1600 (15.755 epoch): loss train=0.2344, val=1.0063, eval->1.0063\n",
      "....................................................................................................iter 1700 (16.740 epoch): loss train=0.2337, val=1.0043, eval->1.0043\n",
      "....................................................................................................iter 1800 (17.724 epoch): loss train=0.2335, val=0.9324, eval->0.9324\n",
      "....................................................................................................iter 1900 (18.709 epoch): loss train=0.2329, val=0.9735, eval->0.9735\n",
      "....................................................................................................iter 2000 (19.694 epoch): loss train=0.2336, val=1.0759, eval->1.0759\n",
      "6660=MMMMMMDCLX\n",
      "....................................................................................................iter 2100 (20.679 epoch): loss train=0.2335, val=0.9927, eval->0.9927\n",
      "....................................................................................................iter 2200 (21.663 epoch): loss train=0.2328, val=1.0588, eval->1.0588\n",
      "....................................................................................................iter 2300 (22.648 epoch): loss train=0.2327, val=1.2066, eval->1.2066\n",
      "....................................................................................................iter 2400 (23.633 epoch): loss train=0.2332, val=1.0477, eval->1.0477\n",
      "....................................................................................................iter 2500 (24.617 epoch): loss train=0.2336, val=1.0963, eval->1.0963\n",
      "L274=MMMMMMMMMCCLXXIV\n",
      "....................................................................................................iter 2600 (25.602 epoch): loss train=0.2326, val=1.0938, eval->1.0938\n",
      "....................................................................................................iter 2700 (26.587 epoch): loss train=0.2325, val=1.0262, eval->1.0262\n",
      "....................................................................................................iter 2800 (27.571 epoch): loss train=0.2329, val=1.0682, eval->1.0682\n",
      "....................................................................................................iter 2900 (28.556 epoch): loss train=0.2332, val=1.0877, eval->1.0877\n",
      "....................................................................................................iter 3000 (29.541 epoch): loss train=0.2328, val=1.0712, eval->1.0712\n",
      "D273=MMMMMMMMMCCLXXIII\n",
      "....................................................................................................iter 3100 (30.525 epoch): loss train=0.2325, val=1.1487, eval->1.1487\n",
      "....................................................................................................iter 3200 (31.510 epoch): loss train=0.2327, val=1.0247, eval->1.0247\n",
      "....................................................................................................iter 3300 (32.495 epoch): loss train=0.2328, val=1.0134, eval->1.0134\n",
      "....................................................................................................iter 3400 (33.479 epoch): loss train=0.2329, val=1.0087, eval->1.0087\n",
      "....................................................................................................iter 3500 (34.464 epoch): loss train=0.2327, val=1.0582, eval->1.0582\n",
      "X\n",
      "....................................................................................................iter 3600 (35.449 epoch): loss train=0.2324, val=1.1559, eval->1.1559\n",
      "....................................................................................................iter 3700 (36.434 epoch): loss train=0.2326, val=1.0998, eval->1.0998\n",
      "....................................................................................................iter 3800 (37.418 epoch): loss train=0.2328, val=1.1426, eval->1.1426\n",
      "....................................................................................................iter 3900 (38.403 epoch): loss train=0.2326, val=1.0360, eval->1.0360\n",
      "....................................................................................................iter 4000 (39.388 epoch): loss train=0.2324, val=1.0553, eval->1.0553\n",
      "8644=MMMMMMMMDCXLIV\n",
      "....................................................................................................iter 4100 (40.372 epoch): loss train=0.2337, val=1.1127, eval->1.1127\n",
      "....................................................................................................iter 4200 (41.357 epoch): loss train=0.2322, val=1.1375, eval->1.1375\n",
      "....................................................................................................iter 4300 (42.342 epoch): loss train=0.2327, val=1.0923, eval->1.0923\n",
      "....................................................................................................iter 4400 (43.326 epoch): loss train=0.2324, val=1.2344, eval->1.2344\n",
      "....................................................................................................iter 4500 (44.311 epoch): loss train=0.2323, val=1.0593, eval->1.0593\n",
      "VI\n",
      "....................................................................................................iter 4600 (45.296 epoch): loss train=0.2324, val=1.2117, eval->1.2117\n",
      "....................................................................................................iter 4700 (46.280 epoch): loss train=0.2330, val=1.1324, eval->1.1324\n",
      "....................................................................................................iter 4800 (47.265 epoch): loss train=0.2327, val=1.0785, eval->1.0785\n",
      "....................................................................................................iter 4900 (48.250 epoch): loss train=0.2323, val=1.0528, eval->1.0528\n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "ben.train(iter_count=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
