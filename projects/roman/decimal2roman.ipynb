{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207ce239-13e0-4a81-b059-b18af42b0382",
   "metadata": {},
   "source": [
    "Train a transformer model to convert decimal numbers to roman literals, ex:\n",
    "\n",
    "56=LVI\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Roman_numerals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea61e43d-787a-49d4-88f0-3f8561ed5c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Train, empty_config, LogFlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475dfd33-91bc-4ad5-840a-744ad238f128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new model dec2roman\n",
      "Dataset train_path: ../data/decimal2roman10k.txt, val_path: None, train_split: 0.8999, vocab_size: 19\n",
      "Model params: 0.59M\n"
     ]
    }
   ],
   "source": [
    "ben = Train('dec2roman', seed=0xbeebaca)\n",
    "\n",
    "# set datasets\n",
    "ben.set_datasets(class_name='charline', \n",
    "                 train_path='../data/decimal2roman10k.txt', \n",
    "                 train_split=(9000-1)/10000,\n",
    "                 pre_shuffle=True) # -1 because numbers start at 1\n",
    "\n",
    "# set config settings\n",
    "cfg = empty_config()\n",
    "cfg.train.log_period=0\n",
    "cfg.model.set(n_layer=6, n_head=6, n_embd=90, block_size=32)\n",
    "cfg.sample.set(top=1, max_batch_size=256) # top_k(1) - always pick the best item\n",
    "cfg.train.set(sample_period=-5)\n",
    "cfg.trainer.set(batch_size=128)\n",
    "\n",
    "# and init a new model with config\n",
    "ben.init_new(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b9399c-5ff0-4069-823e-28c5d38054d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2209=MMCCIX',\n",
       " '5913=MMMMMCMXIII',\n",
       " '507=DVII',\n",
       " '8029=MMMMMMMMXXIX',\n",
       " '3685=MMMDCLXXXV',\n",
       " '7422=MMMMMMMCDXXII',\n",
       " '8805=MMMMMMMMDCCCV',\n",
       " '8390=MMMMMMMMCCCXC',\n",
       " '4128=MMMMCXXVIII',\n",
       " '7937=MMMMMMMCMXXXVII',\n",
       " '4076=MMMMLXXVI',\n",
       " '8075=MMMMMMMMLXXV',\n",
       " '5783=MMMMMDCCLXXXIII',\n",
       " '6607=MMMMMMDCVII',\n",
       " '3620=MMMDCXX',\n",
       " '6623=MMMMMMDCXXIII',\n",
       " '651=DCLI',\n",
       " '2822=MMDCCCXXII',\n",
       " '7117=MMMMMMMCXVII',\n",
       " '9709=MMMMMMMMMDCCIX']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ben.val_dataset.get_data()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a6db11-3bf2-4063-a733-f126c7355dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iters per epoch: 70\n",
      "iter 0 (0.000 epoch): loss train=2.6152, val=2.6174, eval->2.6174\n",
      "==> Saving model at iter=0, eval loss->2.6174 \n",
      "D\n",
      "CUDA max memory used: 331.81M\n",
      "....................................................................................................iter 100 (1.422 epoch): loss train=1.1048, val=1.1043, eval->1.1043\n",
      "==> Saving model at iter=100, eval loss->1.1043 \n",
      "....................................................................................................iter 200 (2.845 epoch): loss train=0.7043, val=0.7023, eval->0.7023\n",
      "==> Saving model at iter=200, eval loss->0.7023 \n",
      "....................................................................................................iter 300 (4.267 epoch): loss train=0.5250, val=0.5249, eval->0.5249\n",
      "==> Saving model at iter=300, eval loss->0.5249 \n",
      "....................................................................................................iter 400 (5.690 epoch): loss train=0.4119, val=0.4125, eval->0.4125\n",
      "==> Saving model at iter=400, eval loss->0.4125 \n",
      "....................................................................................................iter 500 (7.112 epoch): loss train=0.3333, val=0.3324, eval->0.3324\n",
      "==> Saving model at iter=500, eval loss->0.3324 \n",
      "8030=MMMMMMMMMXXX\n",
      "....................................................................................................iter 600 (8.534 epoch): loss train=0.2782, val=0.2787, eval->0.2787\n",
      "==> Saving model at iter=600, eval loss->0.2787 \n",
      "....................................................................................................iter 700 (9.957 epoch): loss train=0.2572, val=0.2564, eval->0.2564\n",
      "==> Saving model at iter=700, eval loss->0.2564 \n",
      "....................................................................................................iter 800 (11.379 epoch): loss train=0.2446, val=0.2455, eval->0.2455\n",
      "==> Saving model at iter=800, eval loss->0.2455 \n",
      "....................................................................................................iter 900 (12.801 epoch): loss train=0.2381, val=0.2378, eval->0.2378\n",
      "==> Saving model at iter=900, eval loss->0.2378 \n",
      "....................................................................................................iter 1000 (14.224 epoch): loss train=0.2326, val=0.2321, eval->0.2321\n",
      "==> Saving model at iter=1000, eval loss->0.2321 \n",
      "4444=MMMMCDXLIV\n",
      "....................................................................................................iter 1100 (15.646 epoch): loss train=0.2289, val=0.2286, eval->0.2286\n",
      "==> Saving model at iter=1100, eval loss->0.2286 \n",
      "....................................................................................................iter 1200 (17.069 epoch): loss train=0.2267, val=0.2265, eval->0.2265\n",
      "==> Saving model at iter=1200, eval loss->0.2265 \n",
      "....................................................................................................iter 1300 (18.491 epoch): loss train=0.2247, val=0.2247, eval->0.2247\n",
      "==> Saving model at iter=1300, eval loss->0.2247 \n",
      "....................................................................................................iter 1400 (19.913 epoch): loss train=0.2232, val=0.2235, eval->0.2235\n",
      "==> Saving model at iter=1400, eval loss->0.2235 \n",
      "....................................................................................................iter 1500 (21.336 epoch): loss train=0.2232, val=0.2237, eval->0.2237\n",
      "7335=MMMMMMMCCCXXXV\n",
      "....................................................................................................iter 1600 (22.758 epoch): loss train=0.2229, val=0.2233, eval->0.2233\n",
      "==> Saving model at iter=1600, eval loss->0.2233 \n",
      "....................................................................................................iter 1700 (24.180 epoch): loss train=0.2217, val=0.2224, eval->0.2224\n",
      "==> Saving model at iter=1700, eval loss->0.2224 \n",
      "....................................................................................................iter 1800 (25.603 epoch): loss train=0.2214, val=0.2216, eval->0.2216\n",
      "==> Saving model at iter=1800, eval loss->0.2216 \n",
      "....................................................................................................iter 1900 (27.025 epoch): loss train=0.2210, val=0.2213, eval->0.2213\n",
      "==> Saving model at iter=1900, eval loss->0.2213 \n",
      "....................................................................................................iter 2000 (28.448 epoch): loss train=0.2206, val=0.2213, eval->0.2213\n",
      "3999=MMMCMXCIX\n",
      "....................................................................................................iter 2100 (29.870 epoch): loss train=0.2204, val=0.2208, eval->0.2208\n",
      "==> Saving model at iter=2100, eval loss->0.2208 \n",
      "....................................................................................................iter 2200 (31.292 epoch): loss train=0.2206, val=0.2210, eval->0.2210\n",
      "....................................................................................................iter 2300 (32.715 epoch): loss train=0.2204, val=0.2208, eval->0.2208\n",
      "....................................................................................................iter 2400 (34.137 epoch): loss train=0.2201, val=0.2204, eval->0.2204\n",
      "==> Saving model at iter=2400, eval loss->0.2204 \n",
      "....................................................................................................iter 2500 (35.560 epoch): loss train=0.2208, val=0.2208, eval->0.2208\n",
      "1008=MVIII\n",
      "....................................................................................................iter 2600 (36.982 epoch): loss train=0.2205, val=0.2205, eval->0.2205\n",
      "....................................................................................................iter 2700 (38.404 epoch): loss train=0.2198, val=0.2204, eval->0.2204\n",
      "==> Saving model at iter=2700, eval loss->0.2204 \n",
      "....................................................................................................iter 2800 (39.827 epoch): loss train=0.2198, val=0.2203, eval->0.2203\n",
      "==> Saving model at iter=2800, eval loss->0.2203 \n",
      "....................................................................................................iter 2900 (41.249 epoch): loss train=0.2199, val=0.2202, eval->0.2202\n",
      "==> Saving model at iter=2900, eval loss->0.2202 \n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "ben.train(iter_count=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "752255de-ed0e-41e6-ad15-ec780026390f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ./models/dec2roman/\n",
      "Checkpoint: iter=2900 (41.249 epoch), loss train=0.2199 val=0.2202 eval->0.2202\n",
      "Dataset train_path: ../data/decimal2roman10k.txt, val_path: None, train_split: 0.8999, vocab_size: 19\n",
      "Model params: 0.59M\n"
     ]
    }
   ],
   "source": [
    "ben.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4151573-9621-4875-b6dc-5d2ca138bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998001998001998\n",
      "2/1001 errors: ['331=CCCXXXI != CCCXXI', '4=IV != I']\n"
     ]
    }
   ],
   "source": [
    "ds = ben.val_dataset\n",
    "q,a=ds.get_data_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "errs = []\n",
    "def test(q,a,g):\n",
    "    global errs\n",
    "    \n",
    "    res = float(a == g)\n",
    "    if not res:\n",
    "        errs += [f\"{q}{a} != {g}\"]\n",
    "    return res\n",
    "    \n",
    "print(ben.measure_accuracy(q,a, test_fn=test))\n",
    "print(f'{len(errs)}/{len(ds)} errors: {errs[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a03c77c-f16a-485e-a5e9-b69e641a8002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9958884320480054\n",
      "37/8999 errors: ['831=DCCCXXXI != DCCCXXI', '37=XXXVII != XXVII', '21=XXI != XII', '79=LXXIX != LXIX', '381=CCCLXXXI != CCCLXXI', '33=XXXIII != XXIII', '36=XXXVI != XXVI', '881=DCCCLXXXI != DCCCLXXI', '39=XXXIX != XXIX', '96=XCVI != XVI', '9=IX != I', '989=CMLXXXIX != CMLXXIX', '89=LXXXIX != XXXIX', '3=III != II', '31=XXXI != XXII', '26=XXVI != XVI', '46=XLVI != XVIV', '38=XXXVIII != XXVIII', '75=LXXV != LXV', '481=CDLXXXI != CDLXXI']\n"
     ]
    }
   ],
   "source": [
    "ds = ben.train_dataset\n",
    "q,a=ds.get_data_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "errs = []\n",
    "print(ben.measure_accuracy(q,a, test_fn=test))\n",
    "print(f'{len(errs)}/{len(ds)} errors: {errs[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c9061cc-45c8-46be-81c8-d40d211e81c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225=CCXXV\n"
     ]
    }
   ],
   "source": [
    "ben.sample('225=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d49469b5-daa0-4cb4-930e-71c236d5bfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Resumed optimizer state\n",
      "Iters per epoch: 70\n",
      "CUDA max memory used: 336.55M\n",
      "....................................................................................................iter 3000 (42.671 epoch): loss train=0.2197, val=0.2206, eval->0.2206\n",
      "300=CCC\n",
      "....................................................................................................iter 3100 (44.094 epoch): loss train=0.2198, val=0.2211, eval->0.2211\n",
      "....................................................................................................iter 3200 (45.516 epoch): loss train=0.2199, val=0.2209, eval->0.2209\n",
      "....................................................................................................iter 3300 (46.939 epoch): loss train=0.2196, val=0.2212, eval->0.2212\n",
      "....................................................................................................iter 3400 (48.361 epoch): loss train=0.2196, val=0.2219, eval->0.2219\n",
      "....................................................................................................iter 3500 (49.783 epoch): loss train=0.2200, val=0.2217, eval->0.2217\n",
      "DXI\n",
      "....................................................................................................iter 3600 (51.206 epoch): loss train=0.2195, val=0.2216, eval->0.2216\n",
      "....................................................................................................iter 3700 (52.628 epoch): loss train=0.2196, val=0.2213, eval->0.2213\n",
      "....................................................................................................iter 3800 (54.050 epoch): loss train=0.2196, val=0.2220, eval->0.2220\n",
      "....................................................................................................iter 3900 (55.473 epoch): loss train=0.2196, val=0.2215, eval->0.2215\n",
      "....................................................................................................iter 4000 (56.895 epoch): loss train=0.2194, val=0.2220, eval->0.2220\n",
      "8459=MMMMMMMMCDLIX\n",
      "....................................................................................................iter 4100 (58.318 epoch): loss train=0.2197, val=0.2220, eval->0.2220\n",
      "....................................................................................................iter 4200 (59.740 epoch): loss train=0.2194, val=0.2213, eval->0.2213\n",
      "....................................................................................................iter 4300 (61.162 epoch): loss train=0.2197, val=0.2223, eval->0.2223\n",
      "....................................................................................................iter 4400 (62.585 epoch): loss train=0.2194, val=0.2222, eval->0.2222\n",
      "....................................................................................................iter 4500 (64.007 epoch): loss train=0.2195, val=0.2225, eval->0.2225\n",
      "0767=MMMMMDCCLXVII\n",
      "....................................................................................................iter 4600 (65.429 epoch): loss train=0.2194, val=0.2234, eval->0.2234\n",
      "....................................................................................................iter 4700 (66.852 epoch): loss train=0.2195, val=0.2215, eval->0.2215\n",
      "....................................................................................................iter 4800 (68.274 epoch): loss train=0.2195, val=0.2217, eval->0.2217\n",
      "....................................................................................................iter 4900 (69.697 epoch): loss train=0.2195, val=0.2236, eval->0.2236\n",
      "....................................................................................................iter 5000 (71.119 epoch): loss train=0.2194, val=0.2227, eval->0.2227\n",
      "5999=MMMMMCMXCIX\n",
      "....................................................................................................iter 5100 (72.541 epoch): loss train=0.2194, val=0.2215, eval->0.2215\n",
      "....................................................................................................iter 5200 (73.964 epoch): loss train=0.2194, val=0.2208, eval->0.2208\n",
      "....................................................................................................iter 5300 (75.386 epoch): loss train=0.2196, val=0.2203, eval->0.2203\n",
      "....................................................................................................iter 5400 (76.809 epoch): loss train=0.2193, val=0.2239, eval->0.2239\n",
      "....................................................................................................iter 5500 (78.231 epoch): loss train=0.2193, val=0.2224, eval->0.2224\n",
      "I\n",
      "....................................................................................................iter 5600 (79.653 epoch): loss train=0.2194, val=0.2219, eval->0.2219\n",
      "....................................................................................................iter 5700 (81.076 epoch): loss train=0.2193, val=0.2221, eval->0.2221\n",
      "....................................................................................................iter 5800 (82.498 epoch): loss train=0.2193, val=0.2220, eval->0.2220\n",
      "....................................................................................................iter 5900 (83.920 epoch): loss train=0.2193, val=0.2214, eval->0.2214\n",
      "....................................................................................................iter 6000 (85.343 epoch): loss train=0.2194, val=0.2228, eval->0.2228\n",
      "\u0000\n",
      "....................................................................................................iter 6100 (86.765 epoch): loss train=0.2194, val=0.2214, eval->0.2214\n",
      "....................................................................................................iter 6200 (88.188 epoch): loss train=0.2194, val=0.2223, eval->0.2223\n",
      "....................................................................................................iter 6300 (89.610 epoch): loss train=0.2194, val=0.2217, eval->0.2217\n",
      "....................................................................................................iter 6400 (91.032 epoch): loss train=0.2193, val=0.2230, eval->0.2230\n",
      "....................................................................................................iter 6500 (92.455 epoch): loss train=0.2194, val=0.2216, eval->0.2216\n",
      "0400=MMMMMCD\n",
      "....................................................................................................iter 6600 (93.877 epoch): loss train=0.2193, val=0.2227, eval->0.2227\n",
      "....................................................................................................iter 6700 (95.299 epoch): loss train=0.2193, val=0.2215, eval->0.2215\n",
      "....................................................................................................iter 6800 (96.722 epoch): loss train=0.2193, val=0.2258, eval->0.2258\n",
      "....................................................................................................iter 6900 (98.144 epoch): loss train=0.2193, val=0.2218, eval->0.2218\n",
      "....................................................................................................iter 7000 (99.567 epoch): loss train=0.2193, val=0.2230, eval->0.2230\n",
      "6177=MMMMMMCLXXVII\n",
      "....................................................................................................iter 7100 (100.989 epoch): loss train=0.2193, val=0.2223, eval->0.2223\n",
      "....................................................................................................iter 7200 (102.411 epoch): loss train=0.2194, val=0.2222, eval->0.2222\n",
      "....................................................................................................iter 7300 (103.834 epoch): loss train=0.2193, val=0.2209, eval->0.2209\n",
      "....................................................................................................iter 7400 (105.256 epoch): loss train=0.2194, val=0.2228, eval->0.2228\n",
      "....................................................................................................iter 7500 (106.679 epoch): loss train=0.2193, val=0.2234, eval->0.2234\n",
      "X\n",
      "....................................................................................................iter 7600 (108.101 epoch): loss train=0.2196, val=0.2231, eval->0.2231\n",
      "....................................................................................................iter 7700 (109.523 epoch): loss train=0.2193, val=0.2222, eval->0.2222\n",
      "....................................................................................................iter 7800 (110.946 epoch): loss train=0.2193, val=0.2223, eval->0.2223\n",
      "....................................................................................................iter 7900 (112.368 epoch): loss train=0.2193, val=0.2258, eval->0.2258\n",
      "....................................................................................................iter 8000 (113.790 epoch): loss train=0.2193, val=0.2213, eval->0.2213\n",
      "LI\n",
      "....................................................................................................iter 8100 (115.213 epoch): loss train=0.2193, val=0.2238, eval->0.2238\n",
      "....................................................................................................iter 8200 (116.635 epoch): loss train=0.2192, val=0.2222, eval->0.2222\n",
      "....................................................................................................iter 8300 (118.058 epoch): loss train=0.2193, val=0.2223, eval->0.2223\n",
      "....................................................................................................iter 8400 (119.480 epoch): loss train=0.2193, val=0.2227, eval->0.2227\n",
      "....................................................................................................iter 8500 (120.902 epoch): loss train=0.2192, val=0.2240, eval->0.2240\n",
      "I\n",
      "....................................................................................................iter 8600 (122.325 epoch): loss train=0.2192, val=0.2226, eval->0.2226\n",
      "....................................................................................................iter 8700 (123.747 epoch): loss train=0.2193, val=0.2224, eval->0.2224\n",
      "....................................................................................................iter 8800 (125.169 epoch): loss train=0.2192, val=0.2215, eval->0.2215\n",
      "....................................................................................................iter 8900 (126.592 epoch): loss train=0.2193, val=0.2219, eval->0.2219\n",
      "....................................................................................................iter 9000 (128.014 epoch): loss train=0.2193, val=0.2232, eval->0.2232\n",
      "8228=MMMMMMMMCCXXVIII\n",
      "....................................................................................................iter 9100 (129.437 epoch): loss train=0.2193, val=0.2249, eval->0.2249\n",
      "....................................................................................................iter 9200 (130.859 epoch): loss train=0.2193, val=0.2248, eval->0.2248\n",
      "....................................................................................................iter 9300 (132.281 epoch): loss train=0.2192, val=0.2218, eval->0.2218\n",
      "....................................................................................................iter 9400 (133.704 epoch): loss train=0.2193, val=0.2224, eval->0.2224\n",
      "....................................................................................................iter 9500 (135.126 epoch): loss train=0.2193, val=0.2207, eval->0.2207\n",
      "3850=MMMDCCCL\n",
      "....................................................................................................iter 9600 (136.549 epoch): loss train=0.2192, val=0.2231, eval->0.2231\n",
      "....................................................................................................iter 9700 (137.971 epoch): loss train=0.2192, val=0.2235, eval->0.2235\n",
      "....................................................................................................iter 9800 (139.393 epoch): loss train=0.2192, val=0.2222, eval->0.2222\n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "# bit more training, up to 10k iters\n",
    "ben.train(iter_count=7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23f2cb65-9ae1-4e5c-901f-ada26689b57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 1267072,\n",
       " 'train_loss': 0.21919450163841248,\n",
       " 'val_loss': 0.22224929928779602,\n",
       " 'eval_loss': 0.22224929928779602}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No better state was saved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
