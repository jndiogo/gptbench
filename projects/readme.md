# Projects

These projects include examples with Jupyter notebooks and/or python scripts:

- [add](add/): Can the model learn how to add two 2 digit numbers? How well will it generalize for unseen sequences?

- [continue](continue/): Complete English text directly from a pretrained GPT2 checkpoint or training a model from scratch

- [fine-tuning](fine-tuning/): Fine-tuning experiments on GPT-2

- [misc](misc/): A folder for miscellaneous experiments: GPT-2 perplexity measurement, next token probabilities

- [prompting](prompting/): Translate French to English by prompting

- [roman](roman/): Train a transformer model to convert decimal numbers to/from roman numerals, ex: 56=LVI or IC=99

- [sequence](sequence/): Can a relatively small model complete number sequences like 1,2,3?


Directory [data](data/) contains data files loaded into the datasets.

Datasets are either included or are generated/downloaded by scripts at [dataprep](dataprep/).