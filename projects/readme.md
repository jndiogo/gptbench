# Projects

These projects include examples with Jupyter notebooks or python scripts:

- [add](add/): Can the model learn how to add two 2 digit numbers? How well will it generalize for unseen sequences?

- [continue](continue/): Complete English text directly from a pretrained GPT2 checkpoint or training a model from scratch

- [misc](misc/): A folder for miscellaneous experiments

- [perplexity](perplexity/): Calculate perplexity of GPT-2 model over wikitext-2

- [roman](roman/): Train a transformer model to convert decimal numbers to/from roman numerals, ex: 56=LVI or IC=99

- [sequence](sequence/): Can a relatively small model complete number sequences like 1,2,3?


Directory [data](data/) contains data files loaded into the datasets.

Datasets are either included or are generated or downloaded by scripts at [dataprep](dataprep/).