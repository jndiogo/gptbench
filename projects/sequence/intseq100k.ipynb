{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4637d54f-2278-492c-bb2d-7277fc1aee63",
   "metadata": {},
   "source": [
    "Can a relatively small model complete number sequences like 1,2,3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98692b76-fe53-4b5a-a27f-71e8436b4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Train, empty_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b7ef1f3-de29-4e45-a589-e06075a8fae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new model intseq100k\n",
      "Dataset train_path: ../data/intseq100k.txt, val_path: None, train_split: 0.8, vocab_size: 11\n",
      "Model params: 0.60M\n"
     ]
    }
   ],
   "source": [
    "ben = Train('intseq100k', seed=0xadaba5e)\n",
    "\n",
    "# set training log periods to avoid cluttering the training output\n",
    "ben.set_train_log_periods(sample_period=500, dot_period=1, loss_period=0)\n",
    "\n",
    "# set train and validation datasets\n",
    "ben.set_datasets(class_name='char', \n",
    "                 train_path='../data/intseq100k.txt', \n",
    "                 train_split=0.8)\n",
    "\n",
    "# set config settings\n",
    "cfg = empty_config()\n",
    "cfg.model.set(n_layer=6, n_head=6, n_embd=90, block_size=64)\n",
    "cfg.trainer.set(batch_size=128)\n",
    "cfg.sample.set(top=1, max_batch_size=256) # top=1 means top_k(1) - always pick the best item\n",
    "\n",
    "# and init a new model with config. set force_new to False to try resuming a previous checkpoint of this name\n",
    "force_new = True\n",
    "if ben.can_load() and not force_new:\n",
    "    ben.load(cfg)\n",
    "else:\n",
    "    ben.init_new(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d93421a-3b54-4433-b197-87fcd5f0498e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Train:',\n",
       " '0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24',\n",
       " 'Val',\n",
       " '0370 80371 80372 80373 80374 80375 80376 80377 80378 80379 80380')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequences in train and validation datasets:\n",
    "'Train:', ben.train_dataset.encdec(0), 'Val', ben.val_dataset.encdec(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77257e34-b661-48aa-8fa9-4d2747e9f0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary used in both datasets:\n",
    "ben.val_dataset.get_vocab_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f509b5f1-cee1-445b-ab2a-20d32ad78a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iters per epoch: 3680\n",
      "Iter 0 (0.000 epoch): loss train=2.3681, val=2.4267, eval->2.4267\n",
      "==> Saving model at iter=0, eval loss->2.4267 \n",
      "Sampling:   3 3 4333 3 323 436 33133137323 4323 33233 3233 33 33132333 3 31313131313131313131313131313131313131\n",
      "CUDA max memory used: 746.01M\n",
      "...................................................................................................\n",
      "Iter 100 (0.027 epoch): loss train=1.9029, val=2.0760, eval->2.0760\n",
      "==> Saving model at iter=100, eval loss->2.0760 \n",
      "...................................................................................................\n",
      "Iter 200 (0.054 epoch): loss train=1.5966, val=1.8549, eval->1.8549\n",
      "==> Saving model at iter=200, eval loss->1.8549 \n",
      "...................................................................................................\n",
      "Iter 300 (0.082 epoch): loss train=1.2658, val=1.4586, eval->1.4586\n",
      "==> Saving model at iter=300, eval loss->1.4586 \n",
      "...................................................................................................\n",
      "Iter 400 (0.109 epoch): loss train=0.9363, val=1.1205, eval->1.1205\n",
      "==> Saving model at iter=400, eval loss->1.1205 \n",
      "...................................................................................................\n",
      "Iter 500 (0.136 epoch): loss train=0.6120, val=0.7376, eval->0.7376\n",
      "==> Saving model at iter=500, eval loss->0.7376 \n",
      "Sampling: 6 66677 66678 66679 66679 66670 66671 66672 66673 66674 66675 6667777678 67678 67679 66679 66679 6667\n",
      "...................................................................................................\n",
      "Iter 600 (0.163 epoch): loss train=0.4231, val=0.4032, eval->0.4032\n",
      "==> Saving model at iter=600, eval loss->0.4032 \n",
      "...................................................................................................\n",
      "Iter 700 (0.190 epoch): loss train=0.3246, val=0.2971, eval->0.2971\n",
      "==> Saving model at iter=700, eval loss->0.2971 \n",
      "...................................................................................................\n",
      "Iter 800 (0.217 epoch): loss train=0.2476, val=0.2534, eval->0.2534\n",
      "==> Saving model at iter=800, eval loss->0.2534 \n",
      "...................................................................................................\n",
      "Iter 900 (0.245 epoch): loss train=0.2177, val=0.2264, eval->0.2264\n",
      "==> Saving model at iter=900, eval loss->0.2264 \n",
      "...................................................................................................\n",
      "Iter 1000 (0.272 epoch): loss train=0.2028, val=0.2097, eval->0.2097\n",
      "==> Saving model at iter=1000, eval loss->0.2097 \n",
      "Sampling: 3 22234 22235 22236 22237 22238 22239 22240 22241 22242 22243 22244 22245 22246 22247 22248 22249 222\n",
      "...................................................................................................\n",
      "Iter 1100 (0.299 epoch): loss train=0.1995, val=0.2036, eval->0.2036\n",
      "==> Saving model at iter=1100, eval loss->0.2036 \n",
      "...................................................................................................\n",
      "Iter 1200 (0.326 epoch): loss train=0.1864, val=0.2010, eval->0.2010\n",
      "==> Saving model at iter=1200, eval loss->0.2010 \n",
      "...................................................................................................\n",
      "Iter 1300 (0.353 epoch): loss train=0.1839, val=0.2010, eval->0.2010\n",
      "...................................................................................................\n",
      "Iter 1400 (0.380 epoch): loss train=0.1815, val=0.1972, eval->0.1972\n",
      "==> Saving model at iter=1400, eval loss->0.1972 \n",
      "...................................................................................................\n",
      "Iter 1500 (0.408 epoch): loss train=0.1817, val=0.1973, eval->0.1973\n",
      "Sampling: 0 33011 33012 33013 33014 33015 33016 33017 33018 33019 33020 33021 33022 33023 33024 33025 33026 330\n",
      "...................................................................................................\n",
      "Iter 1600 (0.435 epoch): loss train=0.1775, val=0.2008, eval->0.2008\n",
      "...................................................................................................\n",
      "Iter 1700 (0.462 epoch): loss train=0.1774, val=0.1991, eval->0.1991\n",
      "...................................................................................................\n",
      "Iter 1800 (0.489 epoch): loss train=0.1782, val=0.1993, eval->0.1993\n",
      "...................................................................................................\n",
      "Iter 1900 (0.516 epoch): loss train=0.1756, val=0.2019, eval->0.2019\n",
      "...................................................................................................\n",
      "Iter 2000 (0.543 epoch): loss train=0.1759, val=0.2010, eval->0.2010\n",
      "Sampling: 2 22233 22234 22235 22236 22237 22238 22239 22240 22241 22242 22243 22244 22245 22246 22247 22248 222\n",
      "...................................................................................................\n",
      "Iter 2100 (0.571 epoch): loss train=0.1743, val=0.1986, eval->0.1986\n",
      "...................................................................................................\n",
      "Iter 2200 (0.598 epoch): loss train=0.1743, val=0.1997, eval->0.1997\n",
      "...................................................................................................\n",
      "Iter 2300 (0.625 epoch): loss train=0.1758, val=0.1988, eval->0.1988\n",
      "...................................................................................................\n",
      "Iter 2400 (0.652 epoch): loss train=0.1744, val=0.2009, eval->0.2009\n",
      "...................................................................................................\n",
      "Iter 2500 (0.679 epoch): loss train=0.1737, val=0.1988, eval->0.1988\n",
      "Sampling: 1 11122 11123 11124 11125 11126 11127 11128 11129 11130 11131 11132 11133 11134 11135 11136 11137 111\n",
      "...................................................................................................\n",
      "Iter 2600 (0.707 epoch): loss train=0.1737, val=0.1992, eval->0.1992\n",
      "...................................................................................................\n",
      "Iter 2700 (0.734 epoch): loss train=0.1733, val=0.2033, eval->0.2033\n",
      "...................................................................................................\n",
      "Iter 2800 (0.761 epoch): loss train=0.1734, val=0.1998, eval->0.1998\n",
      "...................................................................................................\n",
      "Iter 2900 (0.788 epoch): loss train=0.1738, val=0.2052, eval->0.2052\n",
      "...................................................................................................\n",
      "Iter 3000 (0.815 epoch): loss train=0.1726, val=0.2008, eval->0.2008\n",
      "Sampling: 6 66677 66678 66679 66680 66681 66682 66683 66684 66685 66686 66687 66688 66689 66690 66691 66692 666\n",
      "...................................................................................................\n",
      "Iter 3100 (0.842 epoch): loss train=0.1724, val=0.2052, eval->0.2052\n",
      "...................................................................................................\n",
      "Iter 3200 (0.870 epoch): loss train=0.1714, val=0.2044, eval->0.2044\n",
      "...................................................................................................\n",
      "Iter 3300 (0.897 epoch): loss train=0.1729, val=0.2013, eval->0.2013\n",
      "...................................................................................................\n",
      "Iter 3400 (0.924 epoch): loss train=0.1732, val=0.2049, eval->0.2049\n",
      "...................................................................................................\n",
      "Iter 3500 (0.951 epoch): loss train=0.1720, val=0.2034, eval->0.2034\n",
      "Sampling: 5 66666 66667 66668 66669 66670 66671 66672 66673 66674 66675 66676 66677 66678 66679 66680 66681 666\n",
      "...................................................................................................\n",
      "Iter 3600 (0.978 epoch): loss train=0.1714, val=0.2042, eval->0.2042\n",
      "...................................................................................................\n",
      "Iter 3700 (1.005 epoch): loss train=0.1713, val=0.2045, eval->0.2045\n",
      "...................................................................................................\n",
      "Iter 3800 (1.033 epoch): loss train=0.1718, val=0.2030, eval->0.2030\n",
      "...................................................................................................\n",
      "Iter 3900 (1.060 epoch): loss train=0.1716, val=0.2026, eval->0.2026\n",
      "...................................................................................................\n",
      "Iter 4000 (1.087 epoch): loss train=0.1720, val=0.2037, eval->0.2037\n",
      "Sampling: 5 66566 66567 66568 66569 66570 66571 66572 66573 66574 66575 66576 66577 66578 66579 66580 66581 665\n",
      "...................................................................................................\n",
      "Iter 4100 (1.114 epoch): loss train=0.1713, val=0.2026, eval->0.2026\n",
      "...................................................................................................\n",
      "Iter 4200 (1.141 epoch): loss train=0.1716, val=0.2048, eval->0.2048\n",
      "...................................................................................................\n",
      "Iter 4300 (1.168 epoch): loss train=0.1729, val=0.2047, eval->0.2047\n",
      "...................................................................................................\n",
      "Iter 4400 (1.196 epoch): loss train=0.1714, val=0.2025, eval->0.2025\n",
      "...................................................................................................\n",
      "Iter 4500 (1.223 epoch): loss train=0.1710, val=0.2053, eval->0.2053\n",
      "Sampling: 6 66667 66668 66669 66670 66671 66672 66673 66674 66675 66676 66677 66678 66679 66680 66681 66682 666\n",
      "...................................................................................................\n",
      "Iter 4600 (1.250 epoch): loss train=0.1708, val=0.2048, eval->0.2048\n",
      "...................................................................................................\n",
      "Iter 4700 (1.277 epoch): loss train=0.1709, val=0.2068, eval->0.2068\n",
      "...................................................................................................\n",
      "Iter 4800 (1.304 epoch): loss train=0.1713, val=0.2146, eval->0.2146\n",
      "...................................................................................................\n",
      "Iter 4900 (1.332 epoch): loss train=0.1705, val=0.2118, eval->0.2118\n",
      "..................................................................................................."
     ]
    }
   ],
   "source": [
    "# let's train:\n",
    "ben.train(iter_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69991b96-ad59-41b2-8343-264d0b42cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the last sampling lines above, we can see that it can complete sequences quite well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbf0bb23-0532-4348-8004-d227b23fe565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_samples': 563072,\n",
       "  'train_loss': 0.17054523527622223,\n",
       "  'val_loss': 0.20620623230934143,\n",
       "  'eval_loss': 0.20620623230934143},\n",
       " {'n_samples': 230400,\n",
       "  'train_loss': 0.1777949184179306,\n",
       "  'val_loss': 0.19670288264751434,\n",
       "  'eval_loss': 0.19670288264751434})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How are the current and lowest/best loss states?\n",
    "ben.state, ben.last_saved_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebf03cf7-b914-43eb-bcc0-16d5b13c4b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ./checkpoints/intseq100k/\n",
      "Checkpoint: iter=1800 (0.489 epoch), loss train=0.1778 val=0.1967 eval->0.1967\n",
      "Dataset train_path: ../data/intseq100k.txt, val_path: None, train_split: 0.8, vocab_size: 11\n",
      "Model params: 0.60M\n"
     ]
    }
   ],
   "source": [
    "# let's load the last (best) saved:\n",
    "ben.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c18b3a0b-2205-4665-a7dc-b8e950005732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85000 85001 85002 85003 85004 85005 85006 85007 85008 85009 85010 85011 85012 85013 85014 85015 85016 85017 8501\n"
     ]
    }
   ],
   "source": [
    "# let's try completing some sequences:\n",
    "ben.sample('85000 85001 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8856b36-c6aa-4e62-8f1b-f2be972375f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040 3041 \n"
     ]
    }
   ],
   "source": [
    "ben.sample('3019 3020 3021 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb7fbbc6-9c1a-498a-9720-57d966273a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 \n"
     ]
    }
   ],
   "source": [
    "ben.sample('719 720 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "571fec1d-3b2b-4e69-8bac-7676a7f46f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719 77720 77721 77722 77723 77724 77725 77726 77727 77728 77729 77730 77731 77732 77733 77734 77735 7773\n"
     ]
    }
   ],
   "source": [
    "ben.sample('719 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f5f9956-66d7-4fbd-80e3-823a456d0b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 \n"
     ]
    }
   ],
   "source": [
    "# right above it was completing 77719, not 719, so a preceding space character in important\n",
    "ben.sample(' 719 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd07429a-5c0d-4f8d-9953-e1acaa2015ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56 57 58 59 60 61 62 63 64 65 66 77 68 77 77 78 779 70 71 72 73 7 74 73 75 74 76 77 77 78 79 80 80 81 81 82 8\n"
     ]
    }
   ],
   "source": [
    "ben.sample(' 56 57 58 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92f82438-3cbf-46d6-99bf-65b57bdd7851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56 57 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 500 601 602 \n"
     ]
    }
   ],
   "source": [
    "ben.sample(' 56 57 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31e75f56-5582-4f23-bfae-06db15690fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56 57 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 500 601 602 \n"
     ]
    }
   ],
   "source": [
    "ben.sample(' 56 57 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85354bcd-f0f3-46cb-bd9d-8f8615ec7b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 2 3 4 5 6 7 8 40 10 41 5 5 5 6 5 6 6 7 7 6 1 7 1 1 1 1 2 1 2 1 1 2 2 1 2 2 1 2 2 2 3 1 2 3 2 4 4 2 5 3 2 4 3 3 4 4\n"
     ]
    }
   ],
   "source": [
    "ben.sample(' 1 2 3 4 5 6 7 8 ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5d6b6-5994-4828-ac90-8e46f3d891cc",
   "metadata": {},
   "source": [
    "It seems to have problems with lower digits sequences, perhaps because there are less samples?\n",
    "\n",
    "Next: try sequences of odd or even numbers. Prime number sequences?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
