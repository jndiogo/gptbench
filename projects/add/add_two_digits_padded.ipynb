{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0fbbaf-9565-40a9-b378-9f40175390d7",
   "metadata": {},
   "source": [
    "Learn to add two 2-digit numbers with fixed-size padded at the right blocks.\n",
    "\n",
    "increasing tropout seems to help\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3ce45a-3722-4f8b-ba0b-32ec4a64805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Sample, LogFlag, Train, empty_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de1360-3928-4c16-b436-4a2b69d5d337",
   "metadata": {},
   "source": [
    "To create train and validation dataset:\n",
    "python prepare_addition.py ../data/add2.txt 2 --sep=\"\\n\" --split=0.9\n",
    "\n",
    "Creates add2.train.txt and add2.val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de89611-55a3-4aa3-a3ca-a829932bf16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'90+0=90\\n90+1=91\\n90+2=92\\n90+3=93\\n90+4=94\\n90+5=95\\n90+6=96\\n90+7=97\\n90+8=98\\n90+9=99\\n90+10=100\\n90+11=101\\n90+12=102\\n90+13=103\\n90+14=104\\n90+15=105\\n90+16=106\\n90+17=107\\n90+18=108\\n90+19=109\\n90+20=110\\n90+21=111\\n90+22=112\\n90+23=113\\n90+24=114\\n90+25=115\\n90+26=116\\n90+27=117\\n90+28=118\\n90+29=119\\n90+30=120\\n90+31=121\\n90+32=122\\n90+33=123\\n90+34=124\\n90+35=125\\n90+36=126\\n90+37=127\\n90+38=128\\n90+39=129\\n90+40=130\\n90+41=131\\n90+42=132\\n90+43=133\\n90+44=134\\n90+45=135\\n90+46=136\\n90+47=137\\n90+48=138\\n90+49=139\\n90+50=140\\n90+51=141\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/add2.val.txt', 'r', newline=None) as f:\n",
    "    val_data = f.read()\n",
    "val_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e9b4f8-5e0c-4ac6-94eb-75cb182c4e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New random seed 768391787\n",
      "Loading checkpoint from ./models/add2pad/\n",
      "Checkpoint: iter=1900 (27.022 epoch), loss train=0.4691 val=0.7730 eval->0.7730\n",
      "Dataset train_path: ../data/add2.train.txt, val_path: ../data/add2.val.txt, train_split: None, vocab_size: 13\n",
      "Model params: 0.59M\n"
     ]
    }
   ],
   "source": [
    "ben = Train('add2pad', log_mask=LogFlag.ALL)\n",
    "\n",
    "# set datasets\n",
    "ben.set_datasets('padlinechar', train_path='../data/add2.train.txt', val_path='../data/add2.val.txt')\n",
    "\n",
    "# set config settings\n",
    "cfg = empty_config()\n",
    "cfg.train.log_period=0\n",
    "cfg.model.set(n_layer=6, n_head=6, n_embd=90, block_size=16)\n",
    "cfg.sample.set(top=1, max_batch_size=256) # top_k(1) - always pick the best item\n",
    "cfg.train.set(sample_period=-5)\n",
    "cfg.trainer.set(batch_size=128)\n",
    "\n",
    "# and init a new model with config\n",
    "if ben.can_resume():\n",
    "    ben.init_resume(cfg)\n",
    "else:\n",
    "    ben.init_new(cfg)\n",
    "# print(do.get_config().dump(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e9ba82-16e3-4729-b2c1-30ed84cc588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ben.line_dataset_split_qa(ben.val_dataset, 0, 20, sep='=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bef4335d-bda5-40ba-b06a-5d0724bfdd9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.542"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ben.val_dataset\n",
    "q,a=ds.sample_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "def test(q,a,g):\n",
    "    res = float(a == g)\n",
    "    if not res:\n",
    "        print(f\"{q}: {a} != {g}\")\n",
    "    return res\n",
    "    \n",
    "ben.measure_accuracy(q,a, test_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a2d4269-e604-4434-9703-1ba7763504f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9272222222222222"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ben.train_dataset\n",
    "q,a=ds.sample_split(0, len(ds), sep='=', sep_included=-1)\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ee8f456-b596-4213-859a-8dd98ceab01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=['30+16=', '91+2=', '11+11=']\n",
    "a=['46','93','22']\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7bce5c-e7ce-47b4-9fa4-90cd76004738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['46', '109', '22']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_text='30+16=|92+17=|11+11='\n",
    "ans=[]\n",
    "ben.sample(start_text, dest=ans, emit_start=False)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f14bcfb4-7811-42ff-b221-06f4f87df91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_text='1+1='\n",
    "ans=[]\n",
    "ben.sample(start_text, dest=ans, emit_after='=')\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "164f8c0b-cf38-420e-8855-840aa493e6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1900.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ben.state['n_samples']/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "017e1187-4122-47da-a813-50d722ff0c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Resuming optimizer state\n",
      "Batches per epoch: 70\n",
      ".CUDA max memory used: 170.00M\n",
      "...................................................................................................iter 2000 (28.444 epoch): loss train=0.4626, val=0.8079, eval->0.8079\n",
      "21+4=25\n",
      "....................................................................................................iter 2100 (29.867 epoch): loss train=0.4588, val=0.8446, eval->0.8446\n",
      "....................................................................................................iter 2200 (31.289 epoch): loss train=0.4554, val=0.9151, eval->0.9151\n",
      "....................................................................................................iter 2300 (32.711 epoch): loss train=0.4531, val=0.8183, eval->0.8183\n",
      "....................................................................................................iter 2400 (34.133 epoch): loss train=0.4496, val=0.8165, eval->0.8165\n",
      "....................................................................................................iter 2500 (35.556 epoch): loss train=0.4495, val=0.8657, eval->0.8657\n",
      "\u00002+4=45\n",
      "....................................................................................................iter 2600 (36.978 epoch): loss train=0.4476, val=0.8909, eval->0.8909\n",
      "....................................................................................................iter 2700 (38.400 epoch): loss train=0.4451, val=0.9399, eval->0.9399\n",
      "....................................................................................................iter 2800 (39.822 epoch): loss train=0.4447, val=0.8858, eval->0.8858\n",
      "....................................................................................................iter 2900 (41.244 epoch): loss train=0.4460, val=0.9383, eval->0.9383\n",
      "....................................................................................................iter 3000 (42.667 epoch): loss train=0.4427, val=0.8936, eval->0.8936\n",
      "60+49=109\n",
      "....................................................................................................iter 3100 (44.089 epoch): loss train=0.4410, val=0.8768, eval->0.8768\n",
      "....................................................................................................iter 3200 (45.511 epoch): loss train=0.4414, val=0.9091, eval->0.9091\n",
      "....................................................................................................iter 3300 (46.933 epoch): loss train=0.4399, val=0.9054, eval->0.9054\n",
      "....................................................................................................iter 3400 (48.356 epoch): loss train=0.4408, val=0.9493, eval->0.9493\n",
      "....................................................................................................iter 3500 (49.778 epoch): loss train=0.4395, val=0.9813, eval->0.9813\n",
      "52+54=106\n",
      "....................................................................................................iter 3600 (51.200 epoch): loss train=0.4399, val=0.8039, eval->0.8039\n",
      "....................................................................................................iter 3700 (52.622 epoch): loss train=0.4391, val=0.9533, eval->0.9533\n",
      "....................................................................................................iter 3800 (54.044 epoch): loss train=0.4393, val=0.9474, eval->0.9474\n",
      "....................................................................................................iter 3900 (55.467 epoch): loss train=0.4381, val=0.9075, eval->0.9075\n",
      "....................................................................................................iter 4000 (56.889 epoch): loss train=0.4389, val=0.9327, eval->0.9327\n",
      "87+89=176\n",
      "....................................................................................................iter 4100 (58.311 epoch): loss train=0.4382, val=0.9111, eval->0.9111\n",
      "....................................................................................................iter 4200 (59.733 epoch): loss train=0.4377, val=0.9755, eval->0.9755\n",
      "....................................................................................................iter 4300 (61.156 epoch): loss train=0.4369, val=0.9946, eval->0.9946\n",
      "....................................................................................................iter 4400 (62.578 epoch): loss train=0.4369, val=0.9057, eval->0.9057\n",
      "....................................................................................................iter 4500 (64.000 epoch): loss train=0.4370, val=1.0106, eval->1.0106\n",
      "26+69=95\n",
      "....................................................................................................iter 4600 (65.422 epoch): loss train=0.4373, val=0.9877, eval->0.9877\n",
      "....................................................................................................iter 4700 (66.844 epoch): loss train=0.4371, val=1.0284, eval->1.0284\n",
      "....................................................................................................iter 4800 (68.267 epoch): loss train=0.4363, val=0.9793, eval->0.9793\n",
      "....................................................................................................iter 4900 (69.689 epoch): loss train=0.4369, val=0.7999, eval->0.7999\n",
      "....................................................................................................iter 5000 (71.111 epoch): loss train=0.4368, val=1.0046, eval->1.0046\n",
      "+7=7=34\n",
      "....................................................................................................iter 5100 (72.533 epoch): loss train=0.4370, val=0.8716, eval->0.8716\n",
      "....................................................................................................iter 5200 (73.956 epoch): loss train=0.4363, val=1.0303, eval->1.0303\n",
      "....................................................................................................iter 5300 (75.378 epoch): loss train=0.4368, val=0.9522, eval->0.9522\n",
      "....................................................................................................iter 5400 (76.800 epoch): loss train=0.4362, val=1.0131, eval->1.0131\n",
      "....................................................................................................iter 5500 (78.222 epoch): loss train=0.4371, val=1.0056, eval->1.0056\n",
      "9+85=94\n",
      "....................................................................................................iter 5600 (79.644 epoch): loss train=0.4370, val=1.0191, eval->1.0191\n",
      "....................................................................................................iter 5700 (81.067 epoch): loss train=0.4356, val=0.9880, eval->0.9880\n",
      "....................................................................................................iter 5800 (82.489 epoch): loss train=0.4362, val=1.0261, eval->1.0261\n",
      "....................................................................................................iter 5900 (83.911 epoch): loss train=0.4361, val=1.0391, eval->1.0391\n",
      "....................................................................................................iter 6000 (85.333 epoch): loss train=0.4359, val=1.0161, eval->1.0161\n",
      "0+1=1\n",
      "....................................................................................................iter 6100 (86.756 epoch): loss train=0.4361, val=1.0418, eval->1.0418\n",
      "....................................................................................................iter 6200 (88.178 epoch): loss train=0.4364, val=1.0157, eval->1.0157\n",
      "....................................................................................................iter 6300 (89.600 epoch): loss train=0.4364, val=1.0226, eval->1.0226\n",
      "....................................................................................................iter 6400 (91.022 epoch): loss train=0.4358, val=1.0344, eval->1.0344\n",
      "....................................................................................................iter 6500 (92.444 epoch): loss train=0.4360, val=1.0363, eval->1.0363\n",
      "0+2=2\n",
      "....................................................................................................iter 6600 (93.867 epoch): loss train=0.4357, val=1.0412, eval->1.0412\n",
      "....................................................................................................iter 6700 (95.289 epoch): loss train=0.4358, val=1.0191, eval->1.0191\n",
      "....................................................................................................iter 6800 (96.711 epoch): loss train=0.4356, val=1.0568, eval->1.0568\n",
      "....................................................................................................iter 6900 (98.133 epoch): loss train=0.4357, val=1.0982, eval->1.0982\n",
      "....................................................................................................iter 7000 (99.556 epoch): loss train=0.4354, val=1.0681, eval->1.0681\n",
      "=8+99=127\n",
      "....................................................................................................iter 7100 (100.978 epoch): loss train=0.4351, val=1.0603, eval->1.0603\n",
      "....................................................................................................iter 7200 (102.400 epoch): loss train=0.4356, val=1.0973, eval->1.0973\n",
      "....................................................................................................iter 7300 (103.822 epoch): loss train=0.4353, val=0.9893, eval->0.9893\n",
      "....................................................................................................iter 7400 (105.244 epoch): loss train=0.4355, val=1.0732, eval->1.0732\n",
      "....................................................................................................iter 7500 (106.667 epoch): loss train=0.4355, val=1.0703, eval->1.0703\n",
      "0+7=7\n",
      "....................................................................................................iter 7600 (108.089 epoch): loss train=0.4355, val=1.0395, eval->1.0395\n",
      "....................................................................................................iter 7700 (109.511 epoch): loss train=0.4355, val=1.0358, eval->1.0358\n",
      "....................................................................................................iter 7800 (110.933 epoch): loss train=0.4354, val=1.0804, eval->1.0804\n",
      "....................................................................................................iter 7900 (112.356 epoch): loss train=0.4352, val=1.0666, eval->1.0666\n",
      "....................................................................................................iter 8000 (113.778 epoch): loss train=0.4361, val=1.1186, eval->1.1186\n",
      "78+8=86\n",
      "....................................................................................................iter 8100 (115.200 epoch): loss train=0.4356, val=1.0371, eval->1.0371\n",
      "....................................................................................................iter 8200 (116.622 epoch): loss train=0.4361, val=1.0807, eval->1.0807\n",
      "....................................................................................................iter 8300 (118.044 epoch): loss train=0.4355, val=1.0027, eval->1.0027\n",
      "....................................................................................................iter 8400 (119.467 epoch): loss train=0.4355, val=1.1079, eval->1.1079\n",
      "....................................................................................................iter 8500 (120.889 epoch): loss train=0.4357, val=1.1064, eval->1.1064\n",
      "32+11=43\n",
      "....................................................................................................iter 8600 (122.311 epoch): loss train=0.4353, val=1.0781, eval->1.0781\n",
      "....................................................................................................iter 8700 (123.733 epoch): loss train=0.4357, val=1.1124, eval->1.1124\n",
      "....................................................................................................iter 8800 (125.156 epoch): loss train=0.4362, val=1.0698, eval->1.0698\n",
      "....................................................................................................iter 8900 (126.578 epoch): loss train=0.4353, val=1.0674, eval->1.0674\n",
      "....................................................................................................iter 9000 (128.000 epoch): loss train=0.4363, val=1.0586, eval->1.0586\n",
      "+1=1=12\n",
      "....................................................................................................iter 9100 (129.422 epoch): loss train=0.4353, val=1.0790, eval->1.0790\n",
      "....................................................................................................iter 9200 (130.844 epoch): loss train=0.4350, val=1.1145, eval->1.1145\n",
      "....................................................................................................iter 9300 (132.267 epoch): loss train=0.4353, val=1.0533, eval->1.0533\n",
      "....................................................................................................iter 9400 (133.689 epoch): loss train=0.4354, val=1.0575, eval->1.0575\n",
      "....................................................................................................iter 9500 (135.111 epoch): loss train=0.4350, val=1.1089, eval->1.1089\n",
      "=9+77=1\n",
      "....................................................................................................iter 9600 (136.533 epoch): loss train=0.4354, val=1.0860, eval->1.0860\n",
      "....................................................................................................iter 9700 (137.956 epoch): loss train=0.4355, val=1.0456, eval->1.0456\n",
      "....................................................................................................iter 9800 (139.378 epoch): loss train=0.4359, val=1.0935, eval->1.0935\n",
      "....................................................................................................iter 9900 (140.800 epoch): loss train=0.4353, val=1.1022, eval->1.1022\n",
      "....................................................................................................iter 10000 (142.222 epoch): loss train=0.4348, val=1.0858, eval->1.0858\n",
      "=1+20=51\n",
      "....................................................................................................iter 10100 (143.644 epoch): loss train=0.4355, val=1.0812, eval->1.0812\n",
      "....................................................................................................iter 10200 (145.067 epoch): loss train=0.4348, val=1.1045, eval->1.1045\n",
      "....................................................................................................iter 10300 (146.489 epoch): loss train=0.4355, val=1.1038, eval->1.1038\n",
      "....................................................................................................iter 10400 (147.911 epoch): loss train=0.4353, val=1.0943, eval->1.0943\n",
      "....................................................................................................iter 10500 (149.333 epoch): loss train=0.4355, val=1.1450, eval->1.1450\n",
      "40+22=62\n",
      "....................................................................................................iter 10600 (150.756 epoch): loss train=0.4348, val=1.1270, eval->1.1270\n",
      "....................................................................................................iter 10700 (152.178 epoch): loss train=0.4353, val=1.0976, eval->1.0976\n",
      "....................................................................................................iter 10800 (153.600 epoch): loss train=0.4346, val=1.0576, eval->1.0576\n",
      "....................................................................................................iter 10900 (155.022 epoch): loss train=0.4352, val=1.0712, eval->1.0712\n",
      "....................................................................................................iter 11000 (156.444 epoch): loss train=0.4349, val=1.0940, eval->1.0940\n",
      "38+70=108\n",
      "....................................................................................................iter 11100 (157.867 epoch): loss train=0.4349, val=1.0713, eval->1.0713\n",
      "....................................................................................................iter 11200 (159.289 epoch): loss train=0.4356, val=1.1262, eval->1.1262\n",
      "....................................................................................................iter 11300 (160.711 epoch): loss train=0.4346, val=1.1154, eval->1.1154\n",
      "....................................................................................................iter 11400 (162.133 epoch): loss train=0.4352, val=1.1191, eval->1.1191\n",
      "....................................................................................................iter 11500 (163.556 epoch): loss train=0.4351, val=1.1495, eval->1.1495\n",
      "3+47=50\n",
      "....................................................................................................iter 11600 (164.978 epoch): loss train=0.4354, val=1.1440, eval->1.1440\n",
      "....................................................................................................iter 11700 (166.400 epoch): loss train=0.4354, val=1.1370, eval->1.1370\n",
      "....................................................................................................iter 11800 (167.822 epoch): loss train=0.4349, val=1.1171, eval->1.1171\n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "ben.train(iter_count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6efc26c8-78e4-41a8-bac5-e8ccebc18529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New random seed 485812905\n",
      "Initializing new model add2paddrop\n",
      "Dataset train_path: ../data/add2.train.txt, val_path: ../data/add2.val.txt, train_split: None, vocab_size: 13\n",
      "Model params: 0.59M\n",
      "seed: 0 (int) \n",
      "sample: \n",
      "    max_len: 100 (int) \n",
      "    count: 1 (int) \n",
      "    start_text: None (NoneType) \n",
      "    start_text_sep: | (str) \n",
      "    emit_start: True (bool) \n",
      "    emit_after: None (NoneType) \n",
      "    emit_before: None (NoneType) \n",
      "    flush: True (bool) \n",
      "    eot_stop: 0 (int) \n",
      "    top: 1.0 (float) \n",
      "    temp: 1.0 (float) \n",
      "    max_batch_size: 256 (int) \n",
      "    multiline_prompt: False (bool) \n",
      "train: \n",
      "    eval_period: 100 (int) \n",
      "    eval_type: 1.0 (float) \n",
      "    eval_iters: 100 (int) \n",
      "    eval_save_checkpt: 1 (int) \n",
      "    eval_save_loss: csv,tensorboard (str) \n",
      "    sample_period: -5.0 (float) \n",
      "    log_period: 0.0 (float) \n",
      "dataset: \n",
      "    class_name: padlinechar (str) \n",
      "    train_path: ../data/add2.train.txt (str) \n",
      "    train_split: None (NoneType) \n",
      "    val_path: ../data/add2.val.txt (str) \n",
      "    params: None (NoneType) \n",
      "model: \n",
      "    device: auto (str) \n",
      "    dtype: float32 (str) \n",
      "    n_layer: 6 (int) \n",
      "    n_head: 6 (int) \n",
      "    n_embd: 90 (int) \n",
      "    vocab_size: 13 (int) \n",
      "    block_size: 16 (int) \n",
      "    dropout: 0.25 (float) \n",
      "trainer: \n",
      "    n_workers: 0 (int) \n",
      "    batch_size: 128 (int) \n",
      "    max_samples: None (NoneType) \n",
      "    grad_norm_clip: 1.0 (float) \n",
      "    optimizer: adamw (str) \n",
      "    learning_rate: 0.0001 (float) \n",
      "    adamw_beta1: 0.9 (float) \n",
      "    adamw_beta2: 0.95 (float) \n",
      "    adamw_weight_decay: 0.1 (float) \n"
     ]
    }
   ],
   "source": [
    "ben = Train('add2paddrop', log_mask=LogFlag.ALL)\n",
    "\n",
    "# set datasets\n",
    "ben.set_datasets('padlinechar', train_path='../data/add2.train.txt', val_path='../data/add2.val.txt')\n",
    "\n",
    "# set config settings\n",
    "cfg = empty_config()\n",
    "cfg.train.log_period=0\n",
    "cfg.model.set(n_layer=6, n_head=6, n_embd=90, block_size=16, dropout=0.25)\n",
    "cfg.sample.set(top=1, max_batch_size=256) # top_k(1) - always pick the best item\n",
    "cfg.train.set(sample_period=-5)\n",
    "cfg.trainer.set(batch_size=128)\n",
    "\n",
    "# and init a new model with config\n",
    "if ben.can_resume():\n",
    "    ben.init_resume(cfg)\n",
    "else:\n",
    "    ben.init_new(cfg)\n",
    "\n",
    "print(ben.get_config().dump(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41bc9045-21e9-44a6-a3ac-a883658426df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Batches per epoch: 70\n",
      "iter 0 (0.000 epoch): loss train=2.2006, val=2.2309, eval->2.2309\n",
      "==> Saving model at iter=0, eval loss->2.2309 \n",
      "57\n",
      ".CUDA max memory used: 165.54M\n",
      "...................................................................................................iter 100 (1.422 epoch): loss train=1.0424, val=1.0786, eval->1.0786\n",
      "==> Saving model at iter=100, eval loss->1.0786 \n",
      "....................................................................................................iter 200 (2.844 epoch): loss train=0.8437, val=0.8878, eval->0.8878\n",
      "==> Saving model at iter=200, eval loss->0.8878 \n",
      "....................................................................................................iter 300 (4.267 epoch): loss train=0.7831, val=0.8435, eval->0.8435\n",
      "==> Saving model at iter=300, eval loss->0.8435 \n",
      "....................................................................................................iter 400 (5.689 epoch): loss train=0.7550, val=0.8419, eval->0.8419\n",
      "==> Saving model at iter=400, eval loss->0.8419 \n",
      "....................................................................................................iter 500 (7.111 epoch): loss train=0.7281, val=0.8391, eval->0.8391\n",
      "==> Saving model at iter=500, eval loss->0.8391 \n",
      "+4+5=65\n",
      "....................................................................................................iter 600 (8.533 epoch): loss train=0.7090, val=0.8520, eval->0.8520\n",
      "....................................................................................................iter 700 (9.956 epoch): loss train=0.6958, val=0.8620, eval->0.8620\n",
      "....................................................................................................iter 800 (11.378 epoch): loss train=0.6804, val=0.8275, eval->0.8275\n",
      "==> Saving model at iter=800, eval loss->0.8275 \n",
      "....................................................................................................iter 900 (12.800 epoch): loss train=0.6690, val=0.9066, eval->0.9066\n",
      "....................................................................................................iter 1000 (14.222 epoch): loss train=0.6614, val=0.9409, eval->0.9409\n",
      "81+11=101\n",
      "....................................................................................................iter 1100 (15.644 epoch): loss train=0.6550, val=1.0091, eval->1.0091\n",
      "....................................................................................................iter 1200 (17.067 epoch): loss train=0.6458, val=0.9813, eval->0.9813\n",
      "....................................................................................................iter 1300 (18.489 epoch): loss train=0.6460, val=0.9770, eval->0.9770\n",
      "....................................................................................................iter 1400 (19.911 epoch): loss train=0.6334, val=0.9187, eval->0.9187\n",
      "....................................................................................................iter 1500 (21.333 epoch): loss train=0.6296, val=0.9141, eval->0.9141\n",
      "=\n",
      "....................................................................................................iter 1600 (22.756 epoch): loss train=0.6252, val=0.9065, eval->0.9065\n",
      "....................................................................................................iter 1700 (24.178 epoch): loss train=0.6190, val=0.9802, eval->0.9802\n",
      "....................................................................................................iter 1800 (25.600 epoch): loss train=0.6114, val=0.9059, eval->0.9059\n",
      "....................................................................................................iter 1900 (27.022 epoch): loss train=0.5916, val=1.0505, eval->1.0505\n",
      "....................................................................................................iter 2000 (28.444 epoch): loss train=0.5471, val=0.8533, eval->0.8533\n",
      "=2+94=165\n",
      "....................................................................................................iter 2100 (29.867 epoch): loss train=0.5325, val=0.8644, eval->0.8644\n",
      "....................................................................................................iter 2200 (31.289 epoch): loss train=0.5221, val=0.9097, eval->0.9097\n",
      "....................................................................................................iter 2300 (32.711 epoch): loss train=0.5122, val=0.8092, eval->0.8092\n",
      "==> Saving model at iter=2300, eval loss->0.8092 \n",
      "....................................................................................................iter 2400 (34.133 epoch): loss train=0.5085, val=0.6886, eval->0.6886\n",
      "==> Saving model at iter=2400, eval loss->0.6886 \n",
      "....................................................................................................iter 2500 (35.556 epoch): loss train=0.4990, val=0.8110, eval->0.8110\n",
      "51+31=82\n",
      "....................................................................................................iter 2600 (36.978 epoch): loss train=0.4906, val=0.8819, eval->0.8819\n",
      "....................................................................................................iter 2700 (38.400 epoch): loss train=0.4862, val=0.8773, eval->0.8773\n",
      "....................................................................................................iter 2800 (39.822 epoch): loss train=0.4808, val=0.8226, eval->0.8226\n",
      "....................................................................................................iter 2900 (41.244 epoch): loss train=0.4835, val=0.7624, eval->0.7624\n",
      "....................................................................................................iter 3000 (42.667 epoch): loss train=0.4775, val=0.8723, eval->0.8723\n",
      "22+24=46\n",
      "....................................................................................................iter 3100 (44.089 epoch): loss train=0.4716, val=0.8618, eval->0.8618\n",
      "....................................................................................................iter 3200 (45.511 epoch): loss train=0.4732, val=0.8314, eval->0.8314\n",
      "....................................................................................................iter 3300 (46.933 epoch): loss train=0.4686, val=0.7918, eval->0.7918\n",
      "....................................................................................................iter 3400 (48.356 epoch): loss train=0.4628, val=0.9212, eval->0.9212\n",
      "....................................................................................................iter 3500 (49.778 epoch): loss train=0.4606, val=0.8874, eval->0.8874\n",
      "30+90=120\n",
      "....................................................................................................iter 3600 (51.200 epoch): loss train=0.4614, val=0.8952, eval->0.8952\n",
      "....................................................................................................iter 3700 (52.622 epoch): loss train=0.4582, val=0.9303, eval->0.9303\n",
      "....................................................................................................iter 3800 (54.044 epoch): loss train=0.4613, val=0.9418, eval->0.9418\n",
      "....................................................................................................iter 3900 (55.467 epoch): loss train=0.4545, val=0.7914, eval->0.7914\n",
      "....................................................................................................iter 4000 (56.889 epoch): loss train=0.4540, val=0.9072, eval->0.9072\n",
      "9+1=10\n",
      "....................................................................................................iter 4100 (58.311 epoch): loss train=0.4523, val=0.9037, eval->0.9037\n",
      "....................................................................................................iter 4200 (59.733 epoch): loss train=0.4521, val=0.8697, eval->0.8697\n",
      "....................................................................................................iter 4300 (61.156 epoch): loss train=0.4523, val=0.9100, eval->0.9100\n",
      "....................................................................................................iter 4400 (62.578 epoch): loss train=0.4498, val=0.8493, eval->0.8493\n",
      "....................................................................................................iter 4500 (64.000 epoch): loss train=0.4475, val=0.9366, eval->0.9366\n",
      "85+45=130\n",
      "....................................................................................................iter 4600 (65.422 epoch): loss train=0.4472, val=0.9748, eval->0.9748\n",
      "....................................................................................................iter 4700 (66.844 epoch): loss train=0.4491, val=0.8923, eval->0.8923\n",
      "....................................................................................................iter 4800 (68.267 epoch): loss train=0.4475, val=0.8772, eval->0.8772\n",
      "....................................................................................................iter 4900 (69.689 epoch): loss train=0.4446, val=0.9685, eval->0.9685\n",
      "....................................................................................................iter 5000 (71.111 epoch): loss train=0.4459, val=0.8719, eval->0.8719\n",
      "23+7=30\n",
      "....................................................................................................iter 5100 (72.533 epoch): loss train=0.4434, val=0.8722, eval->0.8722\n",
      "....................................................................................................iter 5200 (73.956 epoch): loss train=0.4418, val=0.8864, eval->0.8864\n",
      "....................................................................................................iter 5300 (75.378 epoch): loss train=0.4434, val=0.9937, eval->0.9937\n",
      "....................................................................................................iter 5400 (76.800 epoch): loss train=0.4416, val=0.9302, eval->0.9302\n",
      "....................................................................................................iter 5500 (78.222 epoch): loss train=0.4425, val=0.9574, eval->0.9574\n",
      "=0+90=1\n",
      "....................................................................................................iter 5600 (79.644 epoch): loss train=0.4429, val=1.0325, eval->1.0325\n",
      "....................................................................................................iter 5700 (81.067 epoch): loss train=0.4417, val=0.9661, eval->0.9661\n",
      "....................................................................................................iter 5800 (82.489 epoch): loss train=0.4423, val=0.9608, eval->0.9608\n",
      "....................................................................................................iter 5900 (83.911 epoch): loss train=0.4402, val=1.0483, eval->1.0483\n",
      "....................................................................................................iter 6000 (85.333 epoch): loss train=0.4415, val=1.0377, eval->1.0377\n",
      "0+16=16\n",
      "....................................................................................................iter 6100 (86.756 epoch): loss train=0.4413, val=1.0362, eval->1.0362\n",
      "....................................................................................................iter 6200 (88.178 epoch): loss train=0.4394, val=0.9831, eval->0.9831\n",
      "....................................................................................................iter 6300 (89.600 epoch): loss train=0.4395, val=0.9433, eval->0.9433\n",
      "....................................................................................................iter 6400 (91.022 epoch): loss train=0.4387, val=1.0520, eval->1.0520\n",
      "....................................................................................................iter 6500 (92.444 epoch): loss train=0.4382, val=1.0321, eval->1.0321\n",
      "3+1=4\n",
      "....................................................................................................iter 6600 (93.867 epoch): loss train=0.4386, val=0.9360, eval->0.9360\n",
      "....................................................................................................iter 6700 (95.289 epoch): loss train=0.4383, val=1.0993, eval->1.0993\n",
      "....................................................................................................iter 6800 (96.711 epoch): loss train=0.4398, val=1.0390, eval->1.0390\n",
      "....................................................................................................iter 6900 (98.133 epoch): loss train=0.4387, val=1.0190, eval->1.0190\n",
      "....................................................................................................iter 7000 (99.556 epoch): loss train=0.4378, val=1.0760, eval->1.0760\n",
      "14+39=53\n",
      "....................................................................................................iter 7100 (100.978 epoch): loss train=0.4386, val=0.9419, eval->0.9419\n",
      "....................................................................................................iter 7200 (102.400 epoch): loss train=0.4382, val=1.0617, eval->1.0617\n",
      "....................................................................................................iter 7300 (103.822 epoch): loss train=0.4379, val=1.0880, eval->1.0880\n",
      "....................................................................................................iter 7400 (105.244 epoch): loss train=0.4375, val=1.0589, eval->1.0589\n",
      "....................................................................................................iter 7500 (106.667 epoch): loss train=0.4374, val=1.0645, eval->1.0645\n",
      "38+82=120\n",
      "....................................................................................................iter 7600 (108.089 epoch): loss train=0.4370, val=1.0004, eval->1.0004\n",
      "....................................................................................................iter 7700 (109.511 epoch): loss train=0.4374, val=1.0953, eval->1.0953\n",
      "....................................................................................................iter 7800 (110.933 epoch): loss train=0.4382, val=1.0821, eval->1.0821\n",
      "....................................................................................................iter 7900 (112.356 epoch): loss train=0.4368, val=1.0328, eval->1.0328\n",
      "....................................................................................................iter 8000 (113.778 epoch): loss train=0.4368, val=1.0499, eval->1.0499\n",
      "63+2=65\n",
      "....................................................................................................iter 8100 (115.200 epoch): loss train=0.4366, val=1.0772, eval->1.0772\n",
      "....................................................................................................iter 8200 (116.622 epoch): loss train=0.4371, val=1.0930, eval->1.0930\n",
      "....................................................................................................iter 8300 (118.044 epoch): loss train=0.4366, val=1.1455, eval->1.1455\n",
      "....................................................................................................iter 8400 (119.467 epoch): loss train=0.4365, val=1.0170, eval->1.0170\n",
      "....................................................................................................iter 8500 (120.889 epoch): loss train=0.4365, val=1.0920, eval->1.0920\n",
      "0+15=15\n",
      "....................................................................................................iter 8600 (122.311 epoch): loss train=0.4362, val=1.0915, eval->1.0915\n",
      "....................................................................................................iter 8700 (123.733 epoch): loss train=0.4362, val=1.1266, eval->1.1266\n",
      "....................................................................................................iter 8800 (125.156 epoch): loss train=0.4364, val=1.1004, eval->1.1004\n",
      "....................................................................................................iter 8900 (126.578 epoch): loss train=0.4361, val=1.1110, eval->1.1110\n",
      "....................................................................................................iter 9000 (128.000 epoch): loss train=0.4361, val=1.1049, eval->1.1049\n",
      "9+85=94\n",
      "....................................................................................................iter 9100 (129.422 epoch): loss train=0.4361, val=1.1378, eval->1.1378\n",
      "....................................................................................................iter 9200 (130.844 epoch): loss train=0.4372, val=1.1351, eval->1.1351\n",
      "....................................................................................................iter 9300 (132.267 epoch): loss train=0.4358, val=1.1638, eval->1.1638\n",
      "....................................................................................................iter 9400 (133.689 epoch): loss train=0.4358, val=1.0883, eval->1.0883\n",
      "....................................................................................................iter 9500 (135.111 epoch): loss train=0.4359, val=1.1599, eval->1.1599\n",
      "5+79=84\n",
      "....................................................................................................iter 9600 (136.533 epoch): loss train=0.4355, val=1.1034, eval->1.1034\n",
      "....................................................................................................iter 9700 (137.956 epoch): loss train=0.4359, val=1.1002, eval->1.1002\n",
      "....................................................................................................iter 9800 (139.378 epoch): loss train=0.4361, val=1.0898, eval->1.0898\n",
      "....................................................................................................iter 9900 (140.800 epoch): loss train=0.4363, val=1.0710, eval->1.0710\n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "ben.train(iter_count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b0ec6d6-9196-4994-86ca-482a4c800ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90+21=: 111 != 21\n",
      "90+30=: 120 != 110\n",
      "90+40=: 130 != 120\n",
      "91+9=: 100 != 90\n",
      "91+19=: 110 != 20\n",
      "91+20=: 111 != 21\n",
      "91+28=: 119 != 29\n",
      "91+49=: 140 != 130\n",
      "92+8=: 100 != 90\n",
      "92+18=: 110 != 20\n",
      "92+19=: 111 != 21\n",
      "92+20=: 112 != 22\n",
      "92+21=: 113 != 23\n",
      "92+22=: 114 != 24\n",
      "92+23=: 115 != 25\n",
      "92+24=: 116 != 26\n",
      "92+25=: 117 != 27\n",
      "92+26=: 118 != 28\n",
      "92+27=: 119 != 29\n",
      "93+7=: 100 != 90\n",
      "93+17=: 110 != 20\n",
      "93+18=: 111 != 21\n",
      "93+19=: 112 != 22\n",
      "93+20=: 113 != 23\n",
      "93+21=: 114 != 24\n",
      "93+22=: 115 != 25\n",
      "93+23=: 116 != 26\n",
      "93+24=: 117 != 27\n",
      "93+25=: 118 != 28\n",
      "93+26=: 119 != 29\n",
      "94+6=: 100 != 90\n",
      "94+7=: 101 != 91\n",
      "94+16=: 110 != 20\n",
      "94+17=: 111 != 21\n",
      "94+18=: 112 != 22\n",
      "94+19=: 113 != 23\n",
      "94+20=: 114 != 24\n",
      "94+21=: 115 != 25\n",
      "94+22=: 116 != 26\n",
      "94+23=: 117 != 27\n",
      "94+24=: 118 != 28\n",
      "94+25=: 119 != 29\n",
      "94+42=: 136 != 126\n",
      "94+43=: 137 != 127\n",
      "95+5=: 100 != 90\n",
      "95+6=: 101 != 91\n",
      "95+7=: 102 != 92\n",
      "95+15=: 110 != 20\n",
      "95+16=: 111 != 21\n",
      "95+17=: 112 != 22\n",
      "95+18=: 113 != 23\n",
      "95+19=: 114 != 24\n",
      "95+20=: 115 != 25\n",
      "95+21=: 116 != 26\n",
      "95+22=: 117 != 27\n",
      "95+23=: 118 != 28\n",
      "95+24=: 119 != 29\n",
      "95+42=: 137 != 127\n",
      "95+43=: 138 != 128\n",
      "96+6=: 102 != 92\n",
      "96+7=: 103 != 93\n",
      "96+14=: 110 != 20\n",
      "96+15=: 111 != 21\n",
      "96+16=: 112 != 22\n",
      "96+17=: 113 != 23\n",
      "96+18=: 114 != 24\n",
      "96+19=: 115 != 25\n",
      "96+20=: 116 != 26\n",
      "96+21=: 117 != 27\n",
      "96+22=: 118 != 28\n",
      "96+23=: 119 != 29\n",
      "97+6=: 103 != 93\n",
      "97+7=: 104 != 94\n",
      "97+9=: 106 != 107\n",
      "97+13=: 110 != 20\n",
      "97+14=: 111 != 21\n",
      "97+15=: 112 != 22\n",
      "97+16=: 113 != 23\n",
      "97+17=: 114 != 24\n",
      "97+18=: 115 != 25\n",
      "97+19=: 116 != 26\n",
      "97+20=: 117 != 27\n",
      "97+21=: 118 != 28\n",
      "97+22=: 119 != 29\n",
      "97+49=: 146 != 136\n",
      "98+6=: 104 != 94\n",
      "98+7=: 105 != 95\n",
      "98+8=: 106 != 96\n",
      "98+9=: 107 != 108\n",
      "98+12=: 110 != 20\n",
      "98+13=: 111 != 21\n",
      "98+14=: 112 != 22\n",
      "98+15=: 113 != 23\n",
      "98+16=: 114 != 24\n",
      "98+17=: 115 != 25\n",
      "98+18=: 116 != 26\n",
      "98+19=: 117 != 27\n",
      "98+34=: 132 != 122\n",
      "98+39=: 137 != 127\n",
      "98+42=: 140 != 130\n",
      "98+43=: 141 != 131\n",
      "98+44=: 142 != 132\n",
      "98+45=: 143 != 133\n",
      "98+46=: 144 != 134\n",
      "98+47=: 145 != 135\n",
      "98+48=: 146 != 136\n",
      "98+49=: 147 != 137\n",
      "99+5=: 104 != 94\n",
      "99+6=: 105 != 95\n",
      "99+7=: 106 != 96\n",
      "99+8=: 107 != 97\n",
      "99+31=: 130 != 120\n",
      "99+32=: 131 != 121\n",
      "99+33=: 132 != 122\n",
      "99+34=: 133 != 123\n",
      "99+35=: 134 != 124\n",
      "99+36=: 135 != 125\n",
      "99+37=: 136 != 126\n",
      "99+38=: 137 != 127\n",
      "99+39=: 138 != 128\n",
      "99+40=: 139 != 129\n",
      "99+41=: 140 != 130\n",
      "99+42=: 141 != 131\n",
      "99+43=: 142 != 132\n",
      "99+44=: 143 != 133\n",
      "99+45=: 144 != 134\n",
      "99+46=: 145 != 135\n",
      "99+47=: 146 != 136\n",
      "99+48=: 147 != 137\n",
      "99+49=: 148 != 138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ben.val_dataset\n",
    "q,a=ds.sample_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "def test(q,a,g):\n",
    "    res = float(a == g)\n",
    "    if not res:\n",
    "        print(f\"{q}: {a} != {g}\")\n",
    "    return res\n",
    "    \n",
    "ben.measure_accuracy(q,a, test_fn=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae3c2a9f-ecf1-4ba9-99ad-998dea03d380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2+8=: 10 != 11\n",
      "3+7=: 10 != 11\n",
      "3+8=: 11 != 12\n",
      "4+3=: 7 != 8\n",
      "4+4=: 8 != 9\n",
      "4+6=: 10 != 11\n",
      "5+3=: 8 != 9\n",
      "5+5=: 10 != 11\n",
      "6+4=: 10 != 11\n",
      "7+3=: 10 != 11\n",
      "7+4=: 11 != 12\n",
      "8+2=: 10 != 11\n",
      "8+3=: 11 != 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9985555555555555"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ben.train_dataset\n",
    "q,a=ds.sample_split(0, len(ds), sep='=', sep_included=-1)\n",
    "\n",
    "def test(q,a,g):\n",
    "    res = float(a == g)\n",
    "    if not res:\n",
    "        print(f\"{q}: {a} != {g}\")\n",
    "    return res\n",
    "    \n",
    "ben.measure_accuracy(q,a, test_fn=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e58af77-1cb3-4556-a7f5-0027fb232535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New random seed 2657223947\n",
      "Initializing new model add2paddrop2\n",
      "Dataset train_path: ../data/add2.train.txt, val_path: ../data/add2.val.txt, train_split: None, vocab_size: 13\n",
      "Model params: 2.35M\n",
      "seed: 0 (int) \n",
      "sample: \n",
      "    max_len: 100 (int) \n",
      "    count: 1 (int) \n",
      "    start_text: None (NoneType) \n",
      "    start_text_sep: | (str) \n",
      "    emit_start: True (bool) \n",
      "    emit_after: None (NoneType) \n",
      "    emit_before: None (NoneType) \n",
      "    flush: True (bool) \n",
      "    eot_stop: 0 (int) \n",
      "    top: 1.0 (float) \n",
      "    temp: 1.0 (float) \n",
      "    max_batch_size: 256 (int) \n",
      "    multiline_prompt: False (bool) \n",
      "train: \n",
      "    eval_period: 100 (int) \n",
      "    eval_type: 1.0 (float) \n",
      "    eval_iters: 100 (int) \n",
      "    eval_save_checkpt: 1 (int) \n",
      "    eval_save_loss: csv,tensorboard (str) \n",
      "    sample_period: -5.0 (float) \n",
      "    log_period: 0.0 (float) \n",
      "dataset: \n",
      "    class_name: padlinechar (str) \n",
      "    train_path: ../data/add2.train.txt (str) \n",
      "    train_split: None (NoneType) \n",
      "    val_path: ../data/add2.val.txt (str) \n",
      "    params: None (NoneType) \n",
      "model: \n",
      "    device: auto (str) \n",
      "    dtype: float32 (str) \n",
      "    n_layer: 6 (int) \n",
      "    n_head: 6 (int) \n",
      "    n_embd: 180 (int) \n",
      "    vocab_size: 13 (int) \n",
      "    block_size: 16 (int) \n",
      "    dropout: 0.25 (float) \n",
      "trainer: \n",
      "    n_workers: 0 (int) \n",
      "    batch_size: 128 (int) \n",
      "    max_samples: None (NoneType) \n",
      "    grad_norm_clip: 1.0 (float) \n",
      "    optimizer: adamw (str) \n",
      "    learning_rate: 0.0001 (float) \n",
      "    adamw_beta1: 0.9 (float) \n",
      "    adamw_beta2: 0.95 (float) \n",
      "    adamw_weight_decay: 0.1 (float) \n",
      "Training\n",
      "Batches per epoch: 70\n",
      "iter 0 (0.000 epoch): loss train=1.7056, val=1.7800, eval->1.7800\n",
      "==> Saving model at iter=0, eval loss->1.7800 \n",
      "=\n",
      ".CUDA max memory used: 307.66M\n",
      "...................................................................................................iter 100 (1.422 epoch): loss train=0.7770, val=0.8553, eval->0.8553\n",
      "==> Saving model at iter=100, eval loss->0.8553 \n",
      "....................................................................................................iter 200 (2.844 epoch): loss train=0.7298, val=0.7804, eval->0.7804\n",
      "==> Saving model at iter=200, eval loss->0.7804 \n",
      "....................................................................................................iter 300 (4.267 epoch): loss train=0.6973, val=0.7563, eval->0.7563\n",
      "==> Saving model at iter=300, eval loss->0.7563 \n",
      "....................................................................................................iter 400 (5.689 epoch): loss train=0.6681, val=0.9176, eval->0.9176\n",
      "....................................................................................................iter 500 (7.111 epoch): loss train=0.6542, val=0.9495, eval->0.9495\n",
      "27+41=67\n",
      "....................................................................................................iter 600 (8.533 epoch): loss train=0.6374, val=0.8894, eval->0.8894\n",
      "....................................................................................................iter 700 (9.956 epoch): loss train=0.6040, val=0.8011, eval->0.8011\n",
      "....................................................................................................iter 800 (11.378 epoch): loss train=0.5458, val=0.8269, eval->0.8269\n",
      "....................................................................................................iter 900 (12.800 epoch): loss train=0.5235, val=0.8387, eval->0.8387\n",
      "....................................................................................................iter 1000 (14.222 epoch): loss train=0.5154, val=0.7657, eval->0.7657\n",
      "\u00004+9=83\n",
      "....................................................................................................iter 1100 (15.644 epoch): loss train=0.5022, val=0.8518, eval->0.8518\n",
      "....................................................................................................iter 1200 (17.067 epoch): loss train=0.4884, val=0.7720, eval->0.7720\n",
      "....................................................................................................iter 1300 (18.489 epoch): loss train=0.4816, val=0.9686, eval->0.9686\n",
      "....................................................................................................iter 1400 (19.911 epoch): loss train=0.4737, val=0.8724, eval->0.8724\n",
      "....................................................................................................iter 1500 (21.333 epoch): loss train=0.4663, val=0.9350, eval->0.9350\n",
      "3+35=48\n",
      "....................................................................................................iter 1600 (22.756 epoch): loss train=0.4644, val=0.9804, eval->0.9804\n",
      "....................................................................................................iter 1700 (24.178 epoch): loss train=0.4587, val=1.0129, eval->1.0129\n",
      "....................................................................................................iter 1800 (25.600 epoch): loss train=0.4513, val=1.0059, eval->1.0059\n",
      "....................................................................................................iter 1900 (27.022 epoch): loss train=0.4517, val=1.0240, eval->1.0240\n",
      "....................................................................................................iter 2000 (28.444 epoch): loss train=0.4531, val=1.0211, eval->1.0211\n",
      "89+50=139\n",
      "....................................................................................................iter 2100 (29.867 epoch): loss train=0.4452, val=0.9897, eval->0.9897\n",
      "....................................................................................................iter 2200 (31.289 epoch): loss train=0.4446, val=0.9906, eval->0.9906\n",
      "....................................................................................................iter 2300 (32.711 epoch): loss train=0.4433, val=0.9952, eval->0.9952\n",
      "....................................................................................................iter 2400 (34.133 epoch): loss train=0.4425, val=1.0007, eval->1.0007\n",
      "....................................................................................................iter 2500 (35.556 epoch): loss train=0.4411, val=0.9716, eval->0.9716\n",
      "56+9=65\n",
      "....................................................................................................iter 2600 (36.978 epoch): loss train=0.4415, val=1.0116, eval->1.0116\n",
      "....................................................................................................iter 2700 (38.400 epoch): loss train=0.4402, val=0.9974, eval->0.9974\n",
      "....................................................................................................iter 2800 (39.822 epoch): loss train=0.4393, val=1.0022, eval->1.0022\n",
      "....................................................................................................iter 2900 (41.244 epoch): loss train=0.4393, val=1.0242, eval->1.0242\n",
      "....................................................................................................iter 3000 (42.667 epoch): loss train=0.4384, val=1.0023, eval->1.0023\n",
      "53+5=58\n",
      "....................................................................................................iter 3100 (44.089 epoch): loss train=0.4380, val=1.0462, eval->1.0462\n",
      "....................................................................................................iter 3200 (45.511 epoch): loss train=0.4380, val=1.0301, eval->1.0301\n",
      "....................................................................................................iter 3300 (46.933 epoch): loss train=0.4366, val=1.0288, eval->1.0288\n",
      "....................................................................................................iter 3400 (48.356 epoch): loss train=0.4376, val=1.0514, eval->1.0514\n",
      "....................................................................................................iter 3500 (49.778 epoch): loss train=0.4364, val=1.0345, eval->1.0345\n",
      "19+83=102\n",
      "....................................................................................................iter 3600 (51.200 epoch): loss train=0.4359, val=1.0552, eval->1.0552\n",
      "....................................................................................................iter 3700 (52.622 epoch): loss train=0.4363, val=1.0582, eval->1.0582\n",
      "....................................................................................................iter 3800 (54.044 epoch): loss train=0.4365, val=1.0795, eval->1.0795\n",
      "....................................................................................................iter 3900 (55.467 epoch): loss train=0.4359, val=1.0920, eval->1.0920\n",
      "....................................................................................................iter 4000 (56.889 epoch): loss train=0.4361, val=1.0360, eval->1.0360\n",
      "75+91=166\n",
      "....................................................................................................iter 4100 (58.311 epoch): loss train=0.4364, val=1.0693, eval->1.0693\n",
      "....................................................................................................iter 4200 (59.733 epoch): loss train=0.4361, val=1.0816, eval->1.0816\n",
      "....................................................................................................iter 4300 (61.156 epoch): loss train=0.4361, val=0.9691, eval->0.9691\n",
      "....................................................................................................iter 4400 (62.578 epoch): loss train=0.4357, val=1.0870, eval->1.0870\n",
      "....................................................................................................iter 4500 (64.000 epoch): loss train=0.4352, val=1.0919, eval->1.0919\n",
      "31+77=108\n",
      "....................................................................................................iter 4600 (65.422 epoch): loss train=0.4362, val=1.0811, eval->1.0811\n",
      "....................................................................................................iter 4700 (66.844 epoch): loss train=0.4356, val=1.0867, eval->1.0867\n",
      "....................................................................................................iter 4800 (68.267 epoch): loss train=0.4361, val=1.1182, eval->1.1182\n",
      "....................................................................................................iter 4900 (69.689 epoch): loss train=0.4353, val=1.0877, eval->1.0877\n",
      "....................................................................................................iter 5000 (71.111 epoch): loss train=0.4356, val=1.1226, eval->1.1226\n",
      "17+25=42\n",
      "....................................................................................................iter 5100 (72.533 epoch): loss train=0.4353, val=1.1356, eval->1.1356\n",
      "....................................................................................................iter 5200 (73.956 epoch): loss train=0.4357, val=1.1201, eval->1.1201\n",
      "....................................................................................................iter 5300 (75.378 epoch): loss train=0.4353, val=1.0980, eval->1.0980\n",
      "....................................................................................................iter 5400 (76.800 epoch): loss train=0.4352, val=1.1253, eval->1.1253\n",
      "....................................................................................................iter 5500 (78.222 epoch): loss train=0.4351, val=1.1110, eval->1.1110\n",
      "68+38=106\n",
      "....................................................................................................iter 5600 (79.644 epoch): loss train=0.4347, val=1.1330, eval->1.1330\n",
      "....................................................................................................iter 5700 (81.067 epoch): loss train=0.4353, val=1.1168, eval->1.1168\n",
      "....................................................................................................iter 5800 (82.489 epoch): loss train=0.4353, val=1.1075, eval->1.1075\n",
      "....................................................................................................iter 5900 (83.911 epoch): loss train=0.4347, val=1.1192, eval->1.1192\n",
      "....................................................................................................iter 6000 (85.333 epoch): loss train=0.4347, val=1.1122, eval->1.1122\n",
      "9+5=14\n",
      "....................................................................................................iter 6100 (86.756 epoch): loss train=0.4352, val=1.1262, eval->1.1262\n",
      "....................................................................................................iter 6200 (88.178 epoch): loss train=0.4349, val=1.1196, eval->1.1196\n",
      "....................................................................................................iter 6300 (89.600 epoch): loss train=0.4348, val=1.1686, eval->1.1686\n",
      "....................................................................................................iter 6400 (91.022 epoch): loss train=0.4353, val=1.1243, eval->1.1243\n",
      "....................................................................................................iter 6500 (92.444 epoch): loss train=0.4345, val=1.1520, eval->1.1520\n",
      "40+82=122\n",
      "....................................................................................................iter 6600 (93.867 epoch): loss train=0.4349, val=1.1444, eval->1.1444\n",
      "....................................................................................................iter 6700 (95.289 epoch): loss train=0.4351, val=1.1675, eval->1.1675\n",
      "....................................................................................................iter 6800 (96.711 epoch): loss train=0.4352, val=1.1670, eval->1.1670\n",
      "....................................................................................................iter 6900 (98.133 epoch): loss train=0.4349, val=1.1545, eval->1.1545\n",
      "....................................................................................................iter 7000 (99.556 epoch): loss train=0.4348, val=1.1044, eval->1.1044\n",
      "40+75=115\n",
      "....................................................................................................iter 7100 (100.978 epoch): loss train=0.4347, val=1.1213, eval->1.1213\n",
      "....................................................................................................iter 7200 (102.400 epoch): loss train=0.4346, val=1.1433, eval->1.1433\n",
      "....................................................................................................iter 7300 (103.822 epoch): loss train=0.4347, val=1.2178, eval->1.2178\n",
      "....................................................................................................iter 7400 (105.244 epoch): loss train=0.4352, val=1.1793, eval->1.1793\n",
      "....................................................................................................iter 7500 (106.667 epoch): loss train=0.4349, val=1.1734, eval->1.1734\n",
      "12+59=71\n",
      "....................................................................................................iter 7600 (108.089 epoch): loss train=0.4344, val=1.1383, eval->1.1383\n",
      "....................................................................................................iter 7700 (109.511 epoch): loss train=0.4349, val=1.1898, eval->1.1898\n",
      "....................................................................................................iter 7800 (110.933 epoch): loss train=0.4344, val=1.1742, eval->1.1742\n",
      "....................................................................................................iter 7900 (112.356 epoch): loss train=0.4351, val=1.1529, eval->1.1529\n",
      "....................................................................................................iter 8000 (113.778 epoch): loss train=0.4352, val=1.1620, eval->1.1620\n",
      "=2+1=23\n",
      "....................................................................................................iter 8100 (115.200 epoch): loss train=0.4354, val=1.1622, eval->1.1622\n",
      "....................................................................................................iter 8200 (116.622 epoch): loss train=0.4351, val=1.1967, eval->1.1967\n",
      "....................................................................................................iter 8300 (118.044 epoch): loss train=0.4348, val=1.2098, eval->1.2098\n",
      "....................................................................................................iter 8400 (119.467 epoch): loss train=0.4350, val=1.2084, eval->1.2084\n",
      "....................................................................................................iter 8500 (120.889 epoch): loss train=0.4348, val=1.1923, eval->1.1923\n",
      "34+76=110\n",
      "....................................................................................................iter 8600 (122.311 epoch): loss train=0.4350, val=1.1535, eval->1.1535\n",
      "....................................................................................................iter 8700 (123.733 epoch): loss train=0.4351, val=1.2022, eval->1.2022\n",
      "....................................................................................................iter 8800 (125.156 epoch): loss train=0.4349, val=1.2705, eval->1.2705\n",
      "....................................................................................................iter 8900 (126.578 epoch): loss train=0.4348, val=1.2167, eval->1.2167\n",
      "....................................................................................................iter 9000 (128.000 epoch): loss train=0.4353, val=1.2523, eval->1.2523\n",
      "\u00001+1=82\n",
      "....................................................................................................iter 9100 (129.422 epoch): loss train=0.4346, val=1.2061, eval->1.2061\n",
      "....................................................................................................iter 9200 (130.844 epoch): loss train=0.4347, val=1.2415, eval->1.2415\n",
      "....................................................................................................iter 9300 (132.267 epoch): loss train=0.4348, val=1.1970, eval->1.1970\n",
      "....................................................................................................iter 9400 (133.689 epoch): loss train=0.4348, val=1.2237, eval->1.2237\n",
      "....................................................................................................iter 9500 (135.111 epoch): loss train=0.4349, val=1.2102, eval->1.2102\n",
      "29+9=38\n",
      "....................................................................................................iter 9600 (136.533 epoch): loss train=0.4349, val=1.1749, eval->1.1749\n",
      "....................................................................................................iter 9700 (137.956 epoch): loss train=0.4348, val=1.1985, eval->1.1985\n",
      "....................................................................................................iter 9800 (139.378 epoch): loss train=0.4351, val=1.2666, eval->1.2666\n",
      "....................................................................................................iter 9900 (140.800 epoch): loss train=0.4346, val=1.2149, eval->1.2149\n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "ben = Train('add2paddrop2', log_mask=LogFlag.ALL)\n",
    "\n",
    "# set datasets\n",
    "ben.set_datasets('padlinechar', train_path='../data/add2.train.txt', val_path='../data/add2.val.txt')\n",
    "\n",
    "# set config settings\n",
    "cfg = empty_config()\n",
    "cfg.train.log_period=0\n",
    "cfg.model.set(n_layer=6, n_head=6, n_embd=180, block_size=16, dropout=0.25)\n",
    "cfg.sample.set(top=1, max_batch_size=256) # top_k(1) - always pick the best item\n",
    "cfg.train.set(sample_period=-5)\n",
    "cfg.trainer.set(batch_size=128)\n",
    "\n",
    "# and init a new model with config\n",
    "if ben.can_resume():\n",
    "    ben.init_resume(cfg)\n",
    "else:\n",
    "    ben.init_new(cfg)\n",
    "\n",
    "print(ben.get_config().dump(1))\n",
    "ben.train(iter_count=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
