{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0fbbaf-9565-40a9-b378-9f40175390d7",
   "metadata": {},
   "source": [
    "Can the model learn how to add two 2 digit numbers?\n",
    "\n",
    "How well will it generalize for unseen sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3ce45a-3722-4f8b-ba0b-32ec4a64805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Train, empty_config, LogFlag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de1360-3928-4c16-b436-4a2b69d5d337",
   "metadata": {},
   "source": [
    "To create data from where we'll create train and validation datasets run in the ../dataprep folder:\n",
    "```\n",
    "python prepare_addition.py ../data/add2.txt 2 --sep=\"\\n\"\n",
    "```\n",
    "The script creates add2.txt with entries in the form a+b=cc, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de89611-55a3-4aa3-a3ca-a829932bf16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0=0\n",
      "0+1=1\n",
      "0+2=2\n",
      "0+3=3\n",
      "0+4=4\n",
      "0+5=5\n",
      "0+6=6\n",
      "0+7=7\n",
      "0+8=8\n",
      "0+9=9\n",
      "0+10=10\n",
      "0+11=11\n",
      "0+12=12\n",
      "0+13=13\n",
      "0+14=14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Opening the data samples - the first 100 chars\n",
    "with open('../data/add2.txt', 'r', newline=None) as f:\n",
    "    data = f.read()\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b346d470-29fb-402e-a767-eb9eef3e79dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99+90=189\n",
      "99+91=190\n",
      "99+92=191\n",
      "99+93=192\n",
      "99+94=193\n",
      "99+95=194\n",
      "99+96=195\n",
      "99+97=196\n",
      "99+98=197\n",
      "99+99=198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# and the last 100:\n",
    "print(data[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878bfb32-8a54-4da8-984f-b5b4ecfe4bf2",
   "metadata": {},
   "source": [
    "All entries in the form a+b=c. We'll split these data into train and validation datasets:\n",
    "- Train includes all samples from 0+0=0 till 89+99=188\n",
    "- Validation includes sample from 90+0=90 till 99+99=198\n",
    "\n",
    "Please note that training never sees sums where the first number is 90+, but it does see numbers 90 and above in the second term of additions like 10+95=105. From this, will the model be able to learn 90+10?\n",
    "\n",
    "We'll load samples via the CharLineDataset class: each read sample line is stored in a 16 character block padded at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e9b4f8-5e0c-4ac6-94eb-75cb182c4e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new model add2\n",
      "Dataset train_path: ../data/add2.txt, val_path: None, train_split: 0.9, vocab_size: 13\n",
      "Model params: 0.59M\n"
     ]
    }
   ],
   "source": [
    "# create the GPTBench object - we'll name this model add2\n",
    "ben = Train('add2', seed=0xADD2BEA7)\n",
    "\n",
    "# set training log periods to avoid cluttering the output below\n",
    "ben.set_train_log_periods(sample_period=500, dot_period=1, loss_period=0)\n",
    "\n",
    "# set train and validation datasets\n",
    "ben.set_datasets(class_name='charline', \n",
    "                 train_path='../data/add2.txt', \n",
    "                 train_split=9000/10000) # split at the start of line with 90+..\n",
    "\n",
    "# set config settings that will override the default values\n",
    "cfg = empty_config()\n",
    "cfg.model.set(n_layer=6, n_head=6, n_embd=90, block_size=16) # our model parameters - block_size is big enough for aa+bb=ccc\n",
    "cfg.sample.set(top=1, max_batch_size=256) # note the top_k(1) - always pick the best item\n",
    "cfg.trainer.set(batch_size=128)\n",
    "\n",
    "# and init a new model with config\n",
    "ben.init_new(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d054c307-91cf-4455-9239-51727a91269e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['90+0=90',\n",
       "  '90+1=91',\n",
       "  '90+2=92',\n",
       "  '90+3=93',\n",
       "  '90+4=94',\n",
       "  '90+5=95',\n",
       "  '90+6=96',\n",
       "  '90+7=97',\n",
       "  '90+8=98',\n",
       "  '90+9=99'],\n",
       " ['99+90=189',\n",
       "  '99+91=190',\n",
       "  '99+92=191',\n",
       "  '99+93=192',\n",
       "  '99+94=193',\n",
       "  '99+95=194',\n",
       "  '99+96=195',\n",
       "  '99+97=196',\n",
       "  '99+98=197',\n",
       "  '99+99=198'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm that validation dataset only has entries where the first addition term is 90..99\n",
    "ben.val_dataset.get_data()[:10], ben.val_dataset.get_data()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec487f69-c12a-4f26-85c5-47d6a68eee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      ".Iter 1 (0.014 epoch): loss train=2.2523, val=2.2685, eval->2.2685\n",
      "==> Saving model at iter=1, eval loss->2.2685 \n",
      "...................................................................................................\n",
      "Iter 100 (1.422 epoch): loss train=1.0253, val=1.0688, eval->1.0688\n",
      "==> Saving model at iter=100, eval loss->1.0688 \n",
      "....................................................................................................\n",
      "Iter 200 (2.844 epoch): loss train=0.8392, val=0.9086, eval->0.9086\n",
      "==> Saving model at iter=200, eval loss->0.9086 \n",
      "....................................................................................................\n",
      "Iter 300 (4.267 epoch): loss train=0.7672, val=0.8903, eval->0.8903\n",
      "==> Saving model at iter=300, eval loss->0.8903 \n",
      "....................................................................................................\n",
      "Iter 400 (5.689 epoch): loss train=0.7212, val=0.8606, eval->0.8606\n",
      "==> Saving model at iter=400, eval loss->0.8606 \n",
      "....................................................................................................\n",
      "Iter 500 (7.111 epoch): loss train=0.6909, val=0.9215, eval->0.9215\n",
      "Sampling: 33+50=81\n",
      "....................................................................................................\n",
      "Iter 600 (8.533 epoch): loss train=0.6699, val=0.9033, eval->0.9033\n",
      "....................................................................................................\n",
      "Iter 700 (9.956 epoch): loss train=0.6575, val=0.9837, eval->0.9837\n",
      "....................................................................................................\n",
      "Iter 800 (11.378 epoch): loss train=0.6447, val=1.0028, eval->1.0028\n",
      "....................................................................................................\n",
      "Iter 900 (12.800 epoch): loss train=0.6356, val=0.9602, eval->0.9602\n",
      "....................................................................................................\n",
      "Iter 1000 (14.222 epoch): loss train=0.6271, val=0.9862, eval->0.9862\n",
      "Sampling: 4+71=70\n",
      "....................................................................................................\n",
      "Iter 1100 (15.644 epoch): loss train=0.6164, val=0.9946, eval->0.9946\n",
      "....................................................................................................\n",
      "Iter 1200 (17.067 epoch): loss train=0.5899, val=0.8903, eval->0.8903\n",
      "....................................................................................................\n",
      "Iter 1300 (18.489 epoch): loss train=0.5514, val=0.8691, eval->0.8691\n",
      "....................................................................................................\n",
      "Iter 1400 (19.911 epoch): loss train=0.5323, val=0.7555, eval->0.7555\n",
      "==> Saving model at iter=1400, eval loss->0.7555 \n",
      "....................................................................................................\n",
      "Iter 1500 (21.333 epoch): loss train=0.5126, val=0.7574, eval->0.7574\n",
      "Sampling: 74+22=96\n",
      "....................................................................................................\n",
      "Iter 1600 (22.756 epoch): loss train=0.5098, val=0.7327, eval->0.7327\n",
      "==> Saving model at iter=1600, eval loss->0.7327 \n",
      "....................................................................................................\n",
      "Iter 1700 (24.178 epoch): loss train=0.4911, val=0.8521, eval->0.8521\n",
      "....................................................................................................\n",
      "Iter 1800 (25.600 epoch): loss train=0.4890, val=0.7794, eval->0.7794\n",
      "....................................................................................................\n",
      "Iter 1900 (27.022 epoch): loss train=0.4824, val=0.8622, eval->0.8622\n",
      "....................................................................................................\n",
      "Iter 2000 (28.444 epoch): loss train=0.4774, val=0.8036, eval->0.8036\n",
      "Sampling: 26+98=124\n",
      "....................................................................................................\n",
      "Iter 2100 (29.867 epoch): loss train=0.4704, val=0.8976, eval->0.8976\n",
      "....................................................................................................\n",
      "Iter 2200 (31.289 epoch): loss train=0.4677, val=0.8883, eval->0.8883\n",
      "....................................................................................................\n",
      "Iter 2300 (32.711 epoch): loss train=0.4658, val=0.9215, eval->0.9215\n",
      "....................................................................................................\n",
      "Iter 2400 (34.133 epoch): loss train=0.4599, val=0.7662, eval->0.7662\n",
      "....................................................................................................\n",
      "Iter 2500 (35.556 epoch): loss train=0.4591, val=0.8561, eval->0.8561\n",
      "Sampling: \u0000+99=105\n",
      "....................................................................................................\n",
      "Iter 2600 (36.978 epoch): loss train=0.4561, val=0.8296, eval->0.8296\n",
      "....................................................................................................\n",
      "Iter 2700 (38.400 epoch): loss train=0.4538, val=0.8399, eval->0.8399\n",
      "....................................................................................................\n",
      "Iter 2800 (39.822 epoch): loss train=0.4573, val=0.8078, eval->0.8078\n",
      "....................................................................................................\n",
      "Iter 2900 (41.244 epoch): loss train=0.4499, val=0.8463, eval->0.8463\n",
      "....................................................................................................\n",
      "Iter 3000 (42.667 epoch): loss train=0.4492, val=0.9223, eval->0.9223\n",
      "Sampling: 7+19=26\n"
     ]
    }
   ],
   "source": [
    "# Let's train for 3000 batch iterations. \n",
    "# Each dot means a batch was trained.\n",
    "# Train and validation losses are evaluated each 100 iterations (iters). \n",
    "# Also each 500 iters a random sample is taken.\n",
    "ben.train(iter_count=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05df85bf-d0e9-46b1-a0f7-0ef966c0fe5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 384000,\n",
       " 'train_loss': 0.4491744935512543,\n",
       " 'val_loss': 0.9222894906997681,\n",
       " 'eval_loss': 0.9222894906997681}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No point in training much more because the train loss keeps going down (it's overfitting),\n",
    "# while the validation loss keeps going up, so the model is not generalizing.\n",
    "# Let's compare the current state loss info:\n",
    "ben.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef787745-d60e-49aa-9a62-f3d4e6d3492f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 204800,\n",
       " 'train_loss': 0.5097604990005493,\n",
       " 'val_loss': 0.7327322959899902,\n",
       " 'eval_loss': 0.7327322959899902}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The last saved checkpoint info - the best performing model we got.\n",
    "ben.last_saved_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c12a9f7-d760-4cf2-8f16-343d57532d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ./checkpoints/add2/\n",
      "Checkpoint: iter=1600 (22.756 epoch), loss train=0.5098 val=0.7327 eval->0.7327\n",
      "Dataset train_path: ../data/add2.txt, val_path: None, train_split: 0.9, vocab_size: 13\n",
      "Model params: 0.59M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_samples': 204800,\n",
       " 'train_loss': 0.5097604990005493,\n",
       " 'val_loss': 0.7327322959899902,\n",
       " 'eval_loss': 0.7327322959899902}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last saved checkpoint has lower validation loss, which means more generalization, so let's load it\n",
    "ben.load()\n",
    "ben.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b68280ce-472d-4ad0-9dde-c919c175bede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1=1\n",
      "34+7=30\n",
      "78+99=177\n"
     ]
    }
   ],
   "source": [
    "# take a few samples: are the sums correct?\n",
    "ben.sample('1+1=')\n",
    "ben.sample('34+7=')\n",
    "ben.sample('78+99=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bef4335d-bda5-40ba-b06a-5d0724bfdd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0+0=', '0+1=', '0+2=']\n",
      "['0', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "# Ugh - only the third sum is right!\n",
    "# Let's measure the accuracy of training dataset - \n",
    "# this should be mostly memorization, as the model trained on these data\n",
    "train_ds = ben.train_dataset\n",
    "\n",
    "# split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=train_ds.get_data_split(0, len(train_ds), sep='=', sep_included=-1)\n",
    "\n",
    "print(q[:3])\n",
    "print(a[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14d0f26d-652f-4928-9aac-20a07433d3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6911111111111111"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure the accuracy - how good was the memorization?\n",
    "# This may take a while (and give different results than the number below, if you changed the initial seed)\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0368e0cc-8dbd-498a-bca2-8196ce996960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['90+0=', '90+1=', '90+2=']\n",
      "['90', '91', '92']\n"
     ]
    }
   ],
   "source": [
    "# Not good: about 69%. Further training could improve accuracy, \n",
    "# but the model would be overfitting and memorizing the given samples.\n",
    "# What about the accuracy of the validation dataset, on which the model never trained?\n",
    "val_ds = ben.val_dataset\n",
    "\n",
    "# split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=val_ds.get_data_split(0, len(val_ds), sep='=', sep_included=-1)\n",
    "\n",
    "print(q[:3])\n",
    "print(a[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dded223d-05ba-4771-a85f-ea2a7214c14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.253"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember that validation dataset only has sums starting in 90+..99+..., for example 90+2=92.\n",
    "# The model did however see the reversed addition of 90.100 numbers, for example 2+90=92.\n",
    "# Did it somehow learn the commutative property of addition?\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caa92eda-8433-4013-8713-1c00cbe5148b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('90+48=', '138', '128'),\n",
       " ('90+49=', '139', '129'),\n",
       " ('90+50=', '140', '130'),\n",
       " ('90+51=', '141', '131'),\n",
       " ('90+52=', '142', '132'),\n",
       " ('90+53=', '143', '133'),\n",
       " ('90+54=', '144', '134'),\n",
       " ('90+55=', '145', '135'),\n",
       " ('90+56=', '146', '136'),\n",
       " ('90+57=', '147', '137')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quite bad: about 25%!\n",
    "# How is the model failing - let's see some incorrect answers:\n",
    "\n",
    "wrongs = []\n",
    "ben.measure_accuracy(q,a, log_list=wrongs, log_cond=-0.5)\n",
    "\n",
    "# first column is the start_text, second is the right answer, third is the generated text\n",
    "wrongs[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3db76130-9c3c-4749-b012-731642168c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('92+72=', '164', '154'),\n",
       " ('92+73=', '165', '154'),\n",
       " ('92+74=', '166', '155'),\n",
       " ('92+75=', '167', '166'),\n",
       " ('92+78=', '170', '160'),\n",
       " ('92+83=', '175', '174'),\n",
       " ('92+84=', '176', '175'),\n",
       " ('92+85=', '177', '176'),\n",
       " ('92+93=', '185', '184'),\n",
       " ('92+94=', '186', '185')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In many cases the wrongly generated entries are off by around -10 from the right answer...\n",
    "wrongs[200:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fa19a06-2ff2-48a9-8ce7-f8a36fcb65ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new model add2drop\n",
      "Dataset train_path: ../data/add2.txt, val_path: None, train_split: 0.9, vocab_size: 13\n",
      "Model params: 0.59M\n",
      "seed=-1\n",
      "sample: \n",
      "    max_len=100\n",
      "    count=1\n",
      "    start_text=None\n",
      "    start_text_sep=|\n",
      "    emit_start=True\n",
      "    emit_after=None\n",
      "    emit_before=None\n",
      "    flush=True\n",
      "    eot_stop=0\n",
      "    top=1.0\n",
      "    temp=1.0\n",
      "    max_batch_size=256\n",
      "    multiline_prompt=False\n",
      "train: \n",
      "    eval_period=100\n",
      "    eval_type=1.0\n",
      "    eval_iters=100\n",
      "    eval_save_checkpt=1\n",
      "    eval_save_loss=csv,tensorboard\n",
      "dataset: \n",
      "    class_name=charline\n",
      "    train_path=../data/add2.txt\n",
      "    train_split=0.9\n",
      "    val_path=None\n",
      "    params=None\n",
      "model: \n",
      "    device=auto\n",
      "    dtype=float32\n",
      "    n_layer=6\n",
      "    n_head=6\n",
      "    n_embd=90\n",
      "    vocab_size=13\n",
      "    block_size=16\n",
      "    dropout=0.2\n",
      "trainer: \n",
      "    batch_size=128\n",
      "    accum_size=None\n",
      "    n_workers=0\n",
      "    max_samples=None\n",
      "    grad_norm_clip=1.0\n",
      "    optimizer=adamw\n",
      "    learning_rate=0.0001\n",
      "    adamw_beta1=0.9\n",
      "    adamw_beta2=0.95\n",
      "    adamw_weight_decay=0.1\n"
     ]
    }
   ],
   "source": [
    "# let's try increasing model dropout from its 0.1 default, to improve generalization\n",
    "\n",
    "# set config settings that will override existing values - only dropout changes\n",
    "cfg = empty_config()\n",
    "cfg.model.set(dropout=0.2)\n",
    "\n",
    "# init a new model with config\n",
    "ben.init_new(cfg, name='add2drop')\n",
    "\n",
    "# list total config:\n",
    "print(ben.get_config().dump(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78a74a05-6f66-46ca-884d-afe550d115e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      ".Iter 1 (0.014 epoch): loss train=2.1484, val=2.1668, eval->2.1668\n",
      "==> Saving model at iter=1, eval loss->2.1668 \n",
      "...................................................................................................\n",
      "Iter 100 (1.422 epoch): loss train=1.0695, val=1.0998, eval->1.0998\n",
      "==> Saving model at iter=100, eval loss->1.0998 \n",
      "....................................................................................................\n",
      "Iter 200 (2.844 epoch): loss train=0.8507, val=0.9028, eval->0.9028\n",
      "==> Saving model at iter=200, eval loss->0.9028 \n",
      "....................................................................................................\n",
      "Iter 300 (4.267 epoch): loss train=0.7834, val=0.8850, eval->0.8850\n",
      "==> Saving model at iter=300, eval loss->0.8850 \n",
      "....................................................................................................\n",
      "Iter 400 (5.689 epoch): loss train=0.7443, val=0.8734, eval->0.8734\n",
      "==> Saving model at iter=400, eval loss->0.8734 \n",
      "....................................................................................................\n",
      "Iter 500 (7.111 epoch): loss train=0.7188, val=0.8320, eval->0.8320\n",
      "==> Saving model at iter=500, eval loss->0.8320 \n",
      "Sampling: 77+98=141\n",
      "....................................................................................................\n",
      "Iter 600 (8.533 epoch): loss train=0.6993, val=0.8473, eval->0.8473\n",
      "....................................................................................................\n",
      "Iter 700 (9.956 epoch): loss train=0.6803, val=0.8944, eval->0.8944\n",
      "....................................................................................................\n",
      "Iter 800 (11.378 epoch): loss train=0.6698, val=0.9097, eval->0.9097\n",
      "....................................................................................................\n",
      "Iter 900 (12.800 epoch): loss train=0.6594, val=0.9364, eval->0.9364\n",
      "....................................................................................................\n",
      "Iter 1000 (14.222 epoch): loss train=0.6510, val=0.9050, eval->0.9050\n",
      "Sampling: 0+41=23\n",
      "....................................................................................................\n",
      "Iter 1100 (15.644 epoch): loss train=0.6439, val=0.9189, eval->0.9189\n",
      "....................................................................................................\n",
      "Iter 1200 (17.067 epoch): loss train=0.6336, val=0.9429, eval->0.9429\n",
      "....................................................................................................\n",
      "Iter 1300 (18.489 epoch): loss train=0.6285, val=0.9122, eval->0.9122\n",
      "....................................................................................................\n",
      "Iter 1400 (19.911 epoch): loss train=0.6201, val=0.9814, eval->0.9814\n",
      "....................................................................................................\n",
      "Iter 1500 (21.333 epoch): loss train=0.5721, val=0.8946, eval->0.8946\n",
      "Sampling: 6+90=100\n",
      "....................................................................................................\n",
      "Iter 1600 (22.756 epoch): loss train=0.5467, val=0.8835, eval->0.8835\n",
      "....................................................................................................\n",
      "Iter 1700 (24.178 epoch): loss train=0.5372, val=0.8325, eval->0.8325\n",
      "....................................................................................................\n",
      "Iter 1800 (25.600 epoch): loss train=0.5253, val=0.8755, eval->0.8755\n",
      "....................................................................................................\n",
      "Iter 1900 (27.022 epoch): loss train=0.5171, val=0.8843, eval->0.8843\n",
      "....................................................................................................\n",
      "Iter 2000 (28.444 epoch): loss train=0.5060, val=0.8349, eval->0.8349\n",
      "Sampling: 8+63=71\n",
      "....................................................................................................\n",
      "Iter 2100 (29.867 epoch): loss train=0.5007, val=0.8415, eval->0.8415\n",
      "....................................................................................................\n",
      "Iter 2200 (31.289 epoch): loss train=0.4935, val=0.8111, eval->0.8111\n",
      "==> Saving model at iter=2200, eval loss->0.8111 \n",
      "....................................................................................................\n",
      "Iter 2300 (32.711 epoch): loss train=0.4877, val=0.8553, eval->0.8553\n",
      "....................................................................................................\n",
      "Iter 2400 (34.133 epoch): loss train=0.4848, val=0.8174, eval->0.8174\n",
      "....................................................................................................\n",
      "Iter 2500 (35.556 epoch): loss train=0.4767, val=0.8791, eval->0.8791\n",
      "Sampling: =\n",
      "....................................................................................................\n",
      "Iter 2600 (36.978 epoch): loss train=0.4757, val=0.8409, eval->0.8409\n",
      "....................................................................................................\n",
      "Iter 2700 (38.400 epoch): loss train=0.4725, val=0.8528, eval->0.8528\n",
      "....................................................................................................\n",
      "Iter 2800 (39.822 epoch): loss train=0.4683, val=0.8928, eval->0.8928\n",
      "....................................................................................................\n",
      "Iter 2900 (41.244 epoch): loss train=0.4662, val=0.8798, eval->0.8798\n",
      "....................................................................................................\n",
      "Iter 3000 (42.667 epoch): loss train=0.4618, val=0.9385, eval->0.9385\n",
      "Sampling: 1+2=3\n",
      "....................................................................................................\n",
      "Iter 3100 (44.089 epoch): loss train=0.4632, val=0.9378, eval->0.9378\n",
      "....................................................................................................\n",
      "Iter 3200 (45.511 epoch): loss train=0.4605, val=0.8740, eval->0.8740\n",
      "....................................................................................................\n",
      "Iter 3300 (46.933 epoch): loss train=0.4574, val=0.8862, eval->0.8862\n",
      "....................................................................................................\n",
      "Iter 3400 (48.356 epoch): loss train=0.4557, val=0.9214, eval->0.9214\n",
      "....................................................................................................\n",
      "Iter 3500 (49.778 epoch): loss train=0.4557, val=0.8550, eval->0.8550\n",
      "Sampling: 19+52=71\n",
      "....................................................................................................\n",
      "Iter 3600 (51.200 epoch): loss train=0.4545, val=0.9165, eval->0.9165\n",
      "....................................................................................................\n",
      "Iter 3700 (52.622 epoch): loss train=0.4518, val=0.9184, eval->0.9184\n",
      "....................................................................................................\n",
      "Iter 3800 (54.044 epoch): loss train=0.4506, val=0.8408, eval->0.8408\n",
      "....................................................................................................\n",
      "Iter 3900 (55.467 epoch): loss train=0.4518, val=0.9194, eval->0.9194\n",
      "....................................................................................................\n",
      "Iter 4000 (56.889 epoch): loss train=0.4503, val=0.9120, eval->0.9120\n",
      "Sampling: =1+26=47\n",
      "....................................................................................................\n",
      "Iter 4100 (58.311 epoch): loss train=0.4472, val=0.9398, eval->0.9398\n",
      "....................................................................................................\n",
      "Iter 4200 (59.733 epoch): loss train=0.4467, val=0.9250, eval->0.9250\n",
      "....................................................................................................\n",
      "Iter 4300 (61.156 epoch): loss train=0.4468, val=0.8913, eval->0.8913\n",
      "....................................................................................................\n",
      "Iter 4400 (62.578 epoch): loss train=0.4493, val=0.9378, eval->0.9378\n",
      "....................................................................................................\n",
      "Iter 4500 (64.000 epoch): loss train=0.4447, val=0.8979, eval->0.8979\n",
      "Sampling: +3=66=129\n",
      "....................................................................................................\n",
      "Iter 4600 (65.422 epoch): loss train=0.4449, val=0.9860, eval->0.9860\n",
      "....................................................................................................\n",
      "Iter 4700 (66.844 epoch): loss train=0.4428, val=0.9493, eval->0.9493\n",
      "....................................................................................................\n",
      "Iter 4800 (68.267 epoch): loss train=0.4433, val=0.9957, eval->0.9957\n",
      "....................................................................................................\n",
      "Iter 4900 (69.689 epoch): loss train=0.4424, val=0.9760, eval->0.9760\n",
      "....................................................................................................\n",
      "Iter 5000 (71.111 epoch): loss train=0.4414, val=0.9351, eval->0.9351\n",
      "Sampling: 72+22=94\n"
     ]
    }
   ],
   "source": [
    "# train for a bit more this time - 5000 batch iterations\n",
    "ben.train(iter_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0e06409-800b-4514-88a4-5c63e4588cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 281600,\n",
       " 'train_loss': 0.49347180128097534,\n",
       " 'val_loss': 0.8111000061035156,\n",
       " 'eval_loss': 0.8111000061035156}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the loss of the best saved state?\n",
    "ben.last_saved_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "035bf312-8d67-4a1b-b17d-c60d570a5653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9872222222222222"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's measure accuracy with training data first\n",
    "train_ds = ben.train_dataset\n",
    "\n",
    "# split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=train_ds.get_data_split(0, len(train_ds), sep='=', sep_included=-1)\n",
    "\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18afa8a4-9f08-4f83-a575-b03a3d5423a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not bad, it's now over 98% (from 64% above)\n",
    "# And now with validation data:\n",
    "val_ds = ben.val_dataset\n",
    "\n",
    "# split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=val_ds.get_data_split(0, len(val_ds), sep='=', sep_included=-1)\n",
    "\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cd0ba6b-180a-4c99-9cc7-2a6063695224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('90+1=', '91', '10'),\n",
       " ('90+30=', '120', '110'),\n",
       " ('90+40=', '130', '120'),\n",
       " ('90+50=', '140', '130'),\n",
       " ('90+60=', '150', '140'),\n",
       " ('91+1=', '92', '10'),\n",
       " ('91+2=', '93', '92'),\n",
       " ('91+3=', '94', '93'),\n",
       " ('91+5=', '96', '95'),\n",
       " ('91+6=', '97', '96'),\n",
       " ('91+9=', '100', '90'),\n",
       " ('92+1=', '93', '10'),\n",
       " ('92+2=', '94', '93'),\n",
       " ('92+8=', '100', '90'),\n",
       " ('93+1=', '94', '10'),\n",
       " ('93+2=', '95', '96'),\n",
       " ('93+7=', '100', '90'),\n",
       " ('94+1=', '95', '17'),\n",
       " ('94+2=', '96', '97'),\n",
       " ('94+5=', '99', '90'),\n",
       " ('94+6=', '100', '90'),\n",
       " ('94+7=', '101', '91'),\n",
       " ('94+97=', '191', '111'),\n",
       " ('95+1=', '96', '18'),\n",
       " ('95+2=', '97', '98'),\n",
       " ('95+3=', '98', '99'),\n",
       " ('95+4=', '99', '90'),\n",
       " ('95+5=', '100', '91'),\n",
       " ('95+6=', '101', '92'),\n",
       " ('95+7=', '102', '93'),\n",
       " ('95+8=', '103', '104'),\n",
       " ('95+42=', '137', '127'),\n",
       " ('95+43=', '138', '128'),\n",
       " ('95+96=', '191', '111'),\n",
       " ('96+1=', '97', '18'),\n",
       " ('96+2=', '98', '10'),\n",
       " ('96+3=', '99', '91'),\n",
       " ('96+4=', '100', '91'),\n",
       " ('96+5=', '101', '92'),\n",
       " ('96+6=', '102', '93'),\n",
       " ('96+7=', '103', '10'),\n",
       " ('96+8=', '104', '105'),\n",
       " ('96+9=', '105', '106'),\n",
       " ('96+40=', '136', '126'),\n",
       " ('96+41=', '137', '127'),\n",
       " ('96+42=', '138', '128'),\n",
       " ('96+52=', '148', '138'),\n",
       " ('96+94=', '190', '110'),\n",
       " ('96+95=', '191', '111'),\n",
       " ('96+96=', '192', '112'),\n",
       " ('96+97=', '193', '113'),\n",
       " ('97+1=', '98', '19'),\n",
       " ('97+2=', '99', '10'),\n",
       " ('97+3=', '100', '10'),\n",
       " ('97+4=', '101', '10'),\n",
       " ('97+5=', '102', '10'),\n",
       " ('97+6=', '103', '10'),\n",
       " ('97+7=', '104', '106'),\n",
       " ('97+8=', '105', '106'),\n",
       " ('97+31=', '128', '118'),\n",
       " ('97+40=', '137', '127'),\n",
       " ('97+41=', '138', '128'),\n",
       " ('97+50=', '147', '137'),\n",
       " ('97+51=', '148', '138'),\n",
       " ('97+93=', '190', '110'),\n",
       " ('97+94=', '191', '111'),\n",
       " ('97+95=', '192', '112'),\n",
       " ('98+1=', '99', '10'),\n",
       " ('98+2=', '100', '10'),\n",
       " ('98+3=', '101', '13'),\n",
       " ('98+4=', '102', '10'),\n",
       " ('98+5=', '103', '10'),\n",
       " ('98+6=', '104', '106'),\n",
       " ('98+7=', '105', '106'),\n",
       " ('98+8=', '106', '107'),\n",
       " ('98+30=', '128', '118'),\n",
       " ('98+40=', '138', '128'),\n",
       " ('98+50=', '148', '138'),\n",
       " ('98+91=', '189', '199'),\n",
       " ('98+93=', '191', '111'),\n",
       " ('99+1=', '100', '10'),\n",
       " ('99+2=', '101', '12'),\n",
       " ('99+3=', '102', '13'),\n",
       " ('99+4=', '103', '10'),\n",
       " ('99+5=', '104', '10'),\n",
       " ('99+6=', '105', '106'),\n",
       " ('99+7=', '106', '107')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation accuracy jumped to 91% (from 25% above). You may see a different accuracy \n",
    "# Let's get an idea of which cases are giving the model a hard time in the validation data:\n",
    "wrongs = []\n",
    "ben.measure_accuracy(q,a, log_list=wrongs, log_cond=-0.5)\n",
    "wrongs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b93905e-3980-4bba-aba9-9782e0632f7c",
   "metadata": {},
   "source": [
    "Many errors occcur when the second number is single digit and also between 30 and 50.\n",
    "\n",
    "Single digits could be explained, because the model sees relatively little examples (single digits are 10% of two digit examples).\n",
    "\n",
    "But the 30..60 ranges are weird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7752a67-06bb-4701-8c6c-cd79f4dd7d4b",
   "metadata": {},
   "source": [
    "### More...\n",
    "\n",
    "Even if dropout improves generalization, validation data loss could be better.\n",
    "\n",
    "The training dataset (first adding number between 0 and 89) and validation dataset (90 to 99) are sharply cut around the 90 boundary and represent different distributions of the data. This is likely to increase training data overfit because the model is being trained on a subset of the data with a different distribution than the whole data with the validation set. One could say the training set is not representative of the overall distribution of the data.\n",
    "\n",
    "See add_two_digits_shuffled notebook for how shuffling improves the model by a lot.\n",
    "\n",
    "Perhaps using a zero-padded data format would allow better accuracy, like 82+07=089 ?\n",
    "\n",
    "From other experiences I noted a smaller model.block_size (than the 16 we've used) increases overall loss, which is wird. Should be the opposite, because there are now less characters in immediate memory to handle. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
