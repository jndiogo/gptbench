{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0fbbaf-9565-40a9-b378-9f40175390d7",
   "metadata": {},
   "source": [
    "Can the model learn how to add two 2 digit numbers? How well will it generalize for unseen sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3ce45a-3722-4f8b-ba0b-32ec4a64805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Train, empty_config, LogFlag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de1360-3928-4c16-b436-4a2b69d5d337",
   "metadata": {},
   "source": [
    "To create data from where we'll create train and validation datasets run in the ../dataprep folder:\n",
    "```\n",
    "python prepare_addition.py ../data/add2.txt 2 --sep=\"\\n\"\n",
    "```\n",
    "The script creates add2.txt with entries in the form a+b=cc, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de89611-55a3-4aa3-a3ca-a829932bf16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0=0\n",
      "0+1=1\n",
      "0+2=2\n",
      "0+3=3\n",
      "0+4=4\n",
      "0+5=5\n",
      "0+6=6\n",
      "0+7=7\n",
      "0+8=8\n",
      "0+9=9\n",
      "0+10=10\n",
      "0+11=11\n",
      "0+12=12\n",
      "0+13=13\n",
      "0+14=14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Opening it - the first 100 chars\n",
    "with open('../data/add2.txt', 'r', newline=None) as f:\n",
    "    data = f.read()\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b346d470-29fb-402e-a767-eb9eef3e79dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99+90=189\n",
      "99+91=190\n",
      "99+92=191\n",
      "99+93=192\n",
      "99+94=193\n",
      "99+95=194\n",
      "99+96=195\n",
      "99+97=196\n",
      "99+98=197\n",
      "99+99=198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# and the last 100:\n",
    "print(data[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878bfb32-8a54-4da8-984f-b5b4ecfe4bf2",
   "metadata": {},
   "source": [
    "So, all a+b=c with two digits. We'll split these data into a train and a validation datasets:\n",
    "- Train with all lines from 0+0=0 till 89+99=188\n",
    "- Validation with 90+0=90 till 99+99=198\n",
    "\n",
    "Please note that training never sees sums where the first number is 90+, but it does see numbers 90 and above in the second term of additions, like 10+95=105. Will the model learn 90+10?\n",
    "\n",
    "We'll load sample via the CharLineDataset: each read sample line is stored in a 16 character block padded at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e9b4f8-5e0c-4ac6-94eb-75cb182c4e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================seed 2916269735\n",
      "Initializing new model add2\n",
      "Dataset train_path: ../data/add2.txt, val_path: None, train_split: 0.9, vocab_size: 13\n",
      "Model params: 0.59M\n"
     ]
    }
   ],
   "source": [
    "# create the GPTBench object - we'll name this model add2\n",
    "ben = Train('add2', log_mask=LogFlag.ALL)\n",
    "ben.set_seed(0xADD2BEA7)\n",
    "\n",
    "# set train and validation datasets\n",
    "ben.set_datasets(class_name='charline', \n",
    "                 train_path='../data/add2.txt', \n",
    "                 train_split=9000/10000) # split at the start of line with 90+..\n",
    "\n",
    "# set config settings that will override the default values\n",
    "cfg = empty_config()\n",
    "cfg.train.log_period=0\n",
    "cfg.model.set(n_layer=6, n_head=6, n_embd=90, block_size=16) # our model parameters - block_size is big enough for aa+bb=ccc\n",
    "cfg.sample.set(top=1, max_batch_size=256) # note the top_k(1) - always pick the best item\n",
    "cfg.train.set(sample_period=-5)\n",
    "cfg.trainer.set(batch_size=128)\n",
    "\n",
    "# and init a new model with config\n",
    "ben.init_new(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d054c307-91cf-4455-9239-51727a91269e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['90+0=90',\n",
       "  '90+1=91',\n",
       "  '90+2=92',\n",
       "  '90+3=93',\n",
       "  '90+4=94',\n",
       "  '90+5=95',\n",
       "  '90+6=96',\n",
       "  '90+7=97',\n",
       "  '90+8=98',\n",
       "  '90+9=99'],\n",
       " ['99+90=189',\n",
       "  '99+91=190',\n",
       "  '99+92=191',\n",
       "  '99+93=192',\n",
       "  '99+94=193',\n",
       "  '99+95=194',\n",
       "  '99+96=195',\n",
       "  '99+97=196',\n",
       "  '99+98=197',\n",
       "  '99+99=198'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation dataset only has entries where the first addition term is 90..99\n",
    "ben.val_dataset.get_data()[:10], ben.val_dataset.get_data()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec487f69-c12a-4f26-85c5-47d6a68eee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Batches per epoch: 70\n",
      "iter 0 (0.000 epoch): loss train=2.2523, val=2.2685, eval->2.2685\n",
      "==> Saving model at iter=0, eval loss->2.2685 \n",
      "3\n",
      "CUDA max memory used: 164.88M\n",
      "....................................................................................................iter 100 (1.422 epoch): loss train=1.0242, val=1.0641, eval->1.0641\n",
      "==> Saving model at iter=100, eval loss->1.0641 \n",
      "....................................................................................................iter 200 (2.844 epoch): loss train=0.8386, val=0.9072, eval->0.9072\n",
      "==> Saving model at iter=200, eval loss->0.9072 \n",
      "....................................................................................................iter 300 (4.267 epoch): loss train=0.7672, val=0.8868, eval->0.8868\n",
      "==> Saving model at iter=300, eval loss->0.8868 \n",
      "....................................................................................................iter 400 (5.689 epoch): loss train=0.7230, val=0.8593, eval->0.8593\n",
      "==> Saving model at iter=400, eval loss->0.8593 \n",
      "....................................................................................................iter 500 (7.111 epoch): loss train=0.6973, val=0.8660, eval->0.8660\n",
      "48+85=141\n",
      "....................................................................................................iter 600 (8.533 epoch): loss train=0.6714, val=0.9460, eval->0.9460\n",
      "....................................................................................................iter 700 (9.956 epoch): loss train=0.6544, val=0.9687, eval->0.9687\n",
      "....................................................................................................iter 800 (11.378 epoch): loss train=0.6498, val=0.9405, eval->0.9405\n",
      "....................................................................................................iter 900 (12.800 epoch): loss train=0.6347, val=0.9409, eval->0.9409\n",
      "....................................................................................................iter 1000 (14.222 epoch): loss train=0.6298, val=1.0720, eval->1.0720\n",
      "7+71=81\n",
      "....................................................................................................iter 1100 (15.644 epoch): loss train=0.6200, val=1.0136, eval->1.0136\n",
      "....................................................................................................iter 1200 (17.067 epoch): loss train=0.6188, val=0.9224, eval->0.9224\n",
      "....................................................................................................iter 1300 (18.489 epoch): loss train=0.6091, val=1.0336, eval->1.0336\n",
      "....................................................................................................iter 1400 (19.911 epoch): loss train=0.5890, val=0.9848, eval->0.9848\n",
      "....................................................................................................iter 1500 (21.333 epoch): loss train=0.5422, val=0.8530, eval->0.8530\n",
      "==> Saving model at iter=1500, eval loss->0.8530 \n",
      "22+43=65\n",
      "....................................................................................................iter 1600 (22.756 epoch): loss train=0.5232, val=0.8067, eval->0.8067\n",
      "==> Saving model at iter=1600, eval loss->0.8067 \n",
      "....................................................................................................iter 1700 (24.178 epoch): loss train=0.5073, val=0.8597, eval->0.8597\n",
      "....................................................................................................iter 1800 (25.600 epoch): loss train=0.5001, val=0.8352, eval->0.8352\n",
      "....................................................................................................iter 1900 (27.022 epoch): loss train=0.4878, val=0.8323, eval->0.8323\n",
      "....................................................................................................iter 2000 (28.444 epoch): loss train=0.4815, val=0.8965, eval->0.8965\n",
      "\u0000+22=26\n",
      "....................................................................................................iter 2100 (29.867 epoch): loss train=0.4776, val=0.8891, eval->0.8891\n",
      "....................................................................................................iter 2200 (31.289 epoch): loss train=0.4714, val=0.8663, eval->0.8663\n",
      "....................................................................................................iter 2300 (32.711 epoch): loss train=0.4682, val=0.9565, eval->0.9565\n",
      "....................................................................................................iter 2400 (34.133 epoch): loss train=0.4698, val=0.9101, eval->0.9101\n",
      "....................................................................................................iter 2500 (35.556 epoch): loss train=0.4638, val=0.9438, eval->0.9438\n",
      "73+93=166\n",
      "....................................................................................................iter 2600 (36.978 epoch): loss train=0.4580, val=0.8165, eval->0.8165\n",
      "....................................................................................................iter 2700 (38.400 epoch): loss train=0.4563, val=0.8896, eval->0.8896\n",
      "....................................................................................................iter 2800 (39.822 epoch): loss train=0.4546, val=0.8948, eval->0.8948\n",
      "....................................................................................................iter 2900 (41.244 epoch): loss train=0.4555, val=0.9535, eval->0.9535\n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "# Let's train for 3000 batch iterations. \n",
    "# Each dot means a batch was trained.\n",
    "# Train and validation losses are evaluated each 100 iterations (iters). \n",
    "# Also each 500 iters a random sample is taken.\n",
    "ben.train(iter_count=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05df85bf-d0e9-46b1-a0f7-0ef966c0fe5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 383872,\n",
       " 'train_loss': 0.4554595947265625,\n",
       " 'val_loss': 0.9535066485404968,\n",
       " 'eval_loss': 0.9535066485404968}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No point in training much more because the train loss keeps going down (overfitting) and the validation loss going up (model is not generalizing)\n",
    "# Let's compare the current state loss info:\n",
    "ben.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef787745-d60e-49aa-9a62-f3d4e6d3492f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 204800,\n",
       " 'train_loss': 0.5231900215148926,\n",
       " 'val_loss': 0.8066989779472351,\n",
       " 'eval_loss': 0.8066989779472351}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The last saved checkpoint info - the best performing model we got.\n",
    "ben.last_saved_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c12a9f7-d760-4cf2-8f16-343d57532d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ./models/add2/\n",
      "Checkpoint: iter=1600 (22.756 epoch), loss train=0.5232 val=0.8067 eval->0.8067\n",
      "Dataset train_path: ../data/add2.txt, val_path: None, train_split: 0.9, vocab_size: 13\n",
      "Model params: 0.59M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_samples': 204800,\n",
       " 'train_loss': 0.5231900215148926,\n",
       " 'val_loss': 0.8066989779472351,\n",
       " 'eval_loss': 0.8066989779472351}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last saved checkpoint has lower validation loss, which means more generalization: let's load it\n",
    "ben.load()\n",
    "ben.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b68280ce-472d-4ad0-9dde-c919c175bede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1=1\n",
      "34+7=49\n",
      "78+99=177\n"
     ]
    }
   ],
   "source": [
    "# take a few samples: are the sums correct?\n",
    "ben.sample('1+1=')\n",
    "ben.sample('34+7=')\n",
    "ben.sample('78+99=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bef4335d-bda5-40ba-b06a-5d0724bfdd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0+0=', '0+1=', '0+2=']\n",
      "['0', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "# Ugh! Let's measure the accuracy of training dataset - this should be mostly memorization, as the model trained on these data\n",
    "train_ds = ben.train_dataset\n",
    "\n",
    "# split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=train_ds.get_data_split(0, len(train_ds), sep='=', sep_included=-1)\n",
    "\n",
    "print(q[:3])\n",
    "print(a[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14d0f26d-652f-4928-9aac-20a07433d3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6481111111111111"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure the accuracy - how good was the memorization?\n",
    "# This may take a while (and give different results than the number below, if you changed the initial seed)\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0368e0cc-8dbd-498a-bca2-8196ce996960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['90+0=', '90+1=', '90+2=']\n",
      "['90', '91', '92']\n"
     ]
    }
   ],
   "source": [
    "# Not good: 64%. Further training would improve accuracy, \n",
    "# but the model would be overfitting and memorizing the given samples.\n",
    "# What about the accuracy of the validation dataset, on which the model never trained?\n",
    "val_ds = ben.val_dataset\n",
    "\n",
    "#split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=val_ds.get_data_split(0, len(val_ds), sep='=', sep_included=-1)\n",
    "\n",
    "print(q[:3])\n",
    "print(a[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dded223d-05ba-4771-a85f-ea2a7214c14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.151"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation dataset has sums starting in 90+..99+..., for example 90+2=92.\n",
    "# The model did however see the reversed addition of 90.100 numbers, for example 2+90=92.\n",
    "# Did it somehow learn the commutative property of addition?\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caa92eda-8433-4013-8713-1c00cbe5148b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['90+41=131 != 111',\n",
       " '90+42=132 != 122',\n",
       " '90+43=133 != 123',\n",
       " '90+44=134 != 124',\n",
       " '90+45=135 != 126',\n",
       " '90+46=136 != 127',\n",
       " '90+47=137 != 127',\n",
       " '90+48=138 != 128',\n",
       " '90+49=139 != 128',\n",
       " '90+50=140 != 120']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How is the model failing - let's see some incorrect answers:\n",
    "\n",
    "wrongs = []\n",
    "def test(q,a,g):\n",
    "    global wrongs\n",
    "    res = float(a == g)\n",
    "    if not res: wrongs += [f\"{q}{a} != {g}\"]\n",
    "    return res\n",
    "\n",
    "ben.measure_accuracy(q,a, test_fn=test)\n",
    "\n",
    "wrongs[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3db76130-9c3c-4749-b012-731642168c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['92+43=135 != 125',\n",
       " '92+44=136 != 126',\n",
       " '92+45=137 != 127',\n",
       " '92+46=138 != 128',\n",
       " '92+47=139 != 129',\n",
       " '92+48=140 != 129',\n",
       " '92+49=141 != 120',\n",
       " '92+50=142 != 123',\n",
       " '92+51=143 != 133',\n",
       " '92+52=144 != 134']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongs[200:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a83e0d-defe-43f5-b2b6-b0c1bc4f448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In many cases it's off by around -10..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fa19a06-2ff2-48a9-8ce7-f8a36fcb65ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new model add2drop\n",
      "Dataset train_path: ../data/add2.txt, val_path: None, train_split: 0.9, vocab_size: 13\n",
      "Model params: 0.59M\n",
      "seed: -1\n",
      "sample: \n",
      "    max_len: 100\n",
      "    count: 1\n",
      "    start_text: None\n",
      "    start_text_sep: |\n",
      "    emit_start: True\n",
      "    emit_after: None\n",
      "    emit_before: None\n",
      "    flush: True\n",
      "    eot_stop: 0\n",
      "    top: 1.0\n",
      "    temp: 1.0\n",
      "    max_batch_size: 256\n",
      "    multiline_prompt: False\n",
      "train: \n",
      "    eval_period: 100\n",
      "    eval_type: 1.0\n",
      "    eval_iters: 100\n",
      "    eval_save_checkpt: 1\n",
      "    eval_save_loss: csv,tensorboard\n",
      "    sample_period: -5.0\n",
      "    log_period: 0.0\n",
      "dataset: \n",
      "    class_name: charline\n",
      "    train_path: ../data/add2.txt\n",
      "    train_split: 0.9\n",
      "    val_path: None\n",
      "    params: None\n",
      "model: \n",
      "    device: auto\n",
      "    dtype: float32\n",
      "    n_layer: 6\n",
      "    n_head: 6\n",
      "    n_embd: 90\n",
      "    vocab_size: 13\n",
      "    block_size: 16\n",
      "    dropout: 0.2\n",
      "trainer: \n",
      "    n_workers: 0\n",
      "    batch_size: 128\n",
      "    max_samples: None\n",
      "    grad_norm_clip: 1.0\n",
      "    optimizer: adamw\n",
      "    learning_rate: 0.0001\n",
      "    adamw_beta1: 0.9\n",
      "    adamw_beta2: 0.95\n",
      "    adamw_weight_decay: 0.1\n"
     ]
    }
   ],
   "source": [
    "# let's try increaisng dropout from its 0.1 default to improve generalization\n",
    "\n",
    "# set config settings that will override existing values - only dropout\n",
    "cfg = empty_config()\n",
    "cfg.model.set(dropout=0.2)\n",
    "\n",
    "# init a new model with config\n",
    "ben.init_new(cfg, name='add2drop')\n",
    "\n",
    "# see total config:\n",
    "print(ben.get_config().dump(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78a74a05-6f66-46ca-884d-afe550d115e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Batches per epoch: 70\n",
      "iter 0 (0.000 epoch): loss train=2.2124, val=2.2532, eval->2.2532\n",
      "==> Saving model at iter=0, eval loss->2.2532 \n",
      "7\n",
      "CUDA max memory used: 164.88M\n",
      "....................................................................................................iter 100 (1.422 epoch): loss train=1.0394, val=1.0746, eval->1.0746\n",
      "==> Saving model at iter=100, eval loss->1.0746 \n",
      "....................................................................................................iter 200 (2.844 epoch): loss train=0.8433, val=0.9148, eval->0.9148\n",
      "==> Saving model at iter=200, eval loss->0.9148 \n",
      "....................................................................................................iter 300 (4.267 epoch): loss train=0.7767, val=0.8772, eval->0.8772\n",
      "==> Saving model at iter=300, eval loss->0.8772 \n",
      "....................................................................................................iter 400 (5.689 epoch): loss train=0.7380, val=0.8253, eval->0.8253\n",
      "==> Saving model at iter=400, eval loss->0.8253 \n",
      "....................................................................................................iter 500 (7.111 epoch): loss train=0.7087, val=0.8484, eval->0.8484\n",
      "0+22=22\n",
      "....................................................................................................iter 600 (8.533 epoch): loss train=0.6929, val=0.8727, eval->0.8727\n",
      "....................................................................................................iter 700 (9.956 epoch): loss train=0.6793, val=0.9570, eval->0.9570\n",
      "....................................................................................................iter 800 (11.378 epoch): loss train=0.6668, val=0.9530, eval->0.9530\n",
      "....................................................................................................iter 900 (12.800 epoch): loss train=0.6567, val=0.9220, eval->0.9220\n",
      "....................................................................................................iter 1000 (14.222 epoch): loss train=0.6482, val=0.9305, eval->0.9305\n",
      "68+87=150\n",
      "....................................................................................................iter 1100 (15.644 epoch): loss train=0.6396, val=0.9698, eval->0.9698\n",
      "....................................................................................................iter 1200 (17.067 epoch): loss train=0.6325, val=0.9177, eval->0.9177\n",
      "....................................................................................................iter 1300 (18.489 epoch): loss train=0.6258, val=0.9705, eval->0.9705\n",
      "....................................................................................................iter 1400 (19.911 epoch): loss train=0.6113, val=0.9034, eval->0.9034\n",
      "....................................................................................................iter 1500 (21.333 epoch): loss train=0.5660, val=0.8216, eval->0.8216\n",
      "==> Saving model at iter=1500, eval loss->0.8216 \n",
      "8+87=95\n",
      "....................................................................................................iter 1600 (22.756 epoch): loss train=0.5417, val=0.8177, eval->0.8177\n",
      "==> Saving model at iter=1600, eval loss->0.8177 \n",
      "....................................................................................................iter 1700 (24.178 epoch): loss train=0.5285, val=0.8251, eval->0.8251\n",
      "....................................................................................................iter 1800 (25.600 epoch): loss train=0.5190, val=0.8402, eval->0.8402\n",
      "....................................................................................................iter 1900 (27.022 epoch): loss train=0.5110, val=0.8143, eval->0.8143\n",
      "==> Saving model at iter=1900, eval loss->0.8143 \n",
      "....................................................................................................iter 2000 (28.444 epoch): loss train=0.5030, val=0.8537, eval->0.8537\n",
      "=\n",
      "....................................................................................................iter 2100 (29.867 epoch): loss train=0.4947, val=0.8561, eval->0.8561\n",
      "....................................................................................................iter 2200 (31.289 epoch): loss train=0.4870, val=0.8495, eval->0.8495\n",
      "....................................................................................................iter 2300 (32.711 epoch): loss train=0.4859, val=0.8572, eval->0.8572\n",
      "....................................................................................................iter 2400 (34.133 epoch): loss train=0.4802, val=0.8309, eval->0.8309\n",
      "....................................................................................................iter 2500 (35.556 epoch): loss train=0.4743, val=0.8233, eval->0.8233\n",
      "16+19=35\n",
      "....................................................................................................iter 2600 (36.978 epoch): loss train=0.4712, val=0.7790, eval->0.7790\n",
      "==> Saving model at iter=2600, eval loss->0.7790 \n",
      "....................................................................................................iter 2700 (38.400 epoch): loss train=0.4690, val=0.8183, eval->0.8183\n",
      "....................................................................................................iter 2800 (39.822 epoch): loss train=0.4650, val=0.9410, eval->0.9410\n",
      "....................................................................................................iter 2900 (41.244 epoch): loss train=0.4612, val=0.8486, eval->0.8486\n",
      "....................................................................................................iter 3000 (42.667 epoch): loss train=0.4591, val=0.7969, eval->0.7969\n",
      "1+2=2\n",
      "....................................................................................................iter 3100 (44.089 epoch): loss train=0.4579, val=0.9267, eval->0.9267\n",
      "....................................................................................................iter 3200 (45.511 epoch): loss train=0.4573, val=0.9288, eval->0.9288\n",
      "....................................................................................................iter 3300 (46.933 epoch): loss train=0.4538, val=0.9077, eval->0.9077\n",
      "....................................................................................................iter 3400 (48.356 epoch): loss train=0.4552, val=0.9148, eval->0.9148\n",
      "....................................................................................................iter 3500 (49.778 epoch): loss train=0.4519, val=0.9682, eval->0.9682\n",
      "=\n",
      "....................................................................................................iter 3600 (51.200 epoch): loss train=0.4496, val=0.9431, eval->0.9431\n",
      "....................................................................................................iter 3700 (52.622 epoch): loss train=0.4486, val=0.9314, eval->0.9314\n",
      "....................................................................................................iter 3800 (54.044 epoch): loss train=0.4507, val=0.9052, eval->0.9052\n",
      "....................................................................................................iter 3900 (55.467 epoch): loss train=0.4488, val=1.0087, eval->1.0087\n",
      "....................................................................................................iter 4000 (56.889 epoch): loss train=0.4464, val=0.9957, eval->0.9957\n",
      "+5+1=28\n",
      "....................................................................................................iter 4100 (58.311 epoch): loss train=0.4459, val=1.0087, eval->1.0087\n",
      "....................................................................................................iter 4200 (59.733 epoch): loss train=0.4450, val=0.9785, eval->0.9785\n",
      "....................................................................................................iter 4300 (61.156 epoch): loss train=0.4448, val=0.9917, eval->0.9917\n",
      "....................................................................................................iter 4400 (62.578 epoch): loss train=0.4437, val=0.8869, eval->0.8869\n",
      "....................................................................................................iter 4500 (64.000 epoch): loss train=0.4436, val=0.9457, eval->0.9457\n",
      "75+51=126\n",
      "....................................................................................................iter 4600 (65.422 epoch): loss train=0.4446, val=1.0118, eval->1.0118\n",
      "....................................................................................................iter 4700 (66.844 epoch): loss train=0.4427, val=1.0227, eval->1.0227\n",
      "....................................................................................................iter 4800 (68.267 epoch): loss train=0.4423, val=1.0258, eval->1.0258\n",
      "....................................................................................................iter 4900 (69.689 epoch): loss train=0.4438, val=0.9902, eval->0.9902\n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "# train for a bit more - 5000 batch iterations\n",
    "ben.train(iter_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0e06409-800b-4514-88a4-5c63e4588cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 332800,\n",
       " 'train_loss': 0.4711717665195465,\n",
       " 'val_loss': 0.7789766192436218,\n",
       " 'eval_loss': 0.7789766192436218}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the loss of the best saved state?\n",
    "ben.last_saved_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "035bf312-8d67-4a1b-b17d-c60d570a5653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.985"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's measure accuracy with training data\n",
    "train_ds = ben.train_dataset\n",
    "\n",
    "#split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=train_ds.get_data_split(0, len(train_ds), sep='=', sep_included=-1)\n",
    "\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18afa8a4-9f08-4f83-a575-b03a3d5423a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.886"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not bad, it's now over 98%\n",
    "# And now with validation data\n",
    "val_ds = ben.val_dataset\n",
    "\n",
    "#split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=val_ds.get_data_split(0, len(val_ds), sep='=', sep_included=-1)\n",
    "\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cd0ba6b-180a-4c99-9cc7-2a6063695224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['90+2=92 != 91',\n",
       " '90+3=93 != 92',\n",
       " '91+0=91 != 90',\n",
       " '91+1=92 != 91',\n",
       " '91+2=93 != 92',\n",
       " '91+3=94 != 93',\n",
       " '91+4=95 != 94',\n",
       " '91+6=97 != 96',\n",
       " '91+9=100 != 90',\n",
       " '91+39=130 != 120',\n",
       " '91+49=140 != 130',\n",
       " '91+59=150 != 140',\n",
       " '91+69=160 != 150',\n",
       " '92+0=92 != 91',\n",
       " '92+1=93 != 92',\n",
       " '92+2=94 != 93',\n",
       " '92+3=95 != 94',\n",
       " '92+4=96 != 95',\n",
       " '92+5=97 != 96',\n",
       " '92+6=98 != 97',\n",
       " '92+8=100 != 90',\n",
       " '92+9=101 != 90',\n",
       " '92+48=140 != 130',\n",
       " '92+58=150 != 140',\n",
       " '92+68=160 != 150',\n",
       " '93+0=93 != 92',\n",
       " '93+1=94 != 93',\n",
       " '93+2=95 != 94',\n",
       " '93+4=97 != 96',\n",
       " '93+7=100 != 90',\n",
       " '93+8=101 != 90',\n",
       " '93+9=102 != 91',\n",
       " '94+1=95 != 96',\n",
       " '94+2=96 != 97',\n",
       " '94+3=97 != 98',\n",
       " '94+6=100 != 90',\n",
       " '94+7=101 != 91',\n",
       " '94+8=102 != 91',\n",
       " '94+9=103 != 92',\n",
       " '94+36=130 != 120',\n",
       " '94+39=133 != 123',\n",
       " '95+1=96 != 98',\n",
       " '95+2=97 != 99',\n",
       " '95+3=98 != 90',\n",
       " '95+4=99 != 90',\n",
       " '95+5=100 != 90',\n",
       " '95+6=101 != 92',\n",
       " '95+7=102 != 92',\n",
       " '95+8=103 != 93',\n",
       " '95+9=104 != 95',\n",
       " '95+38=133 != 123',\n",
       " '95+39=134 != 124',\n",
       " '96+1=97 != 98',\n",
       " '96+2=98 != 90',\n",
       " '96+3=99 != 112',\n",
       " '96+4=100 != 113',\n",
       " '96+5=101 != 92',\n",
       " '96+6=102 != 93',\n",
       " '96+7=103 != 94',\n",
       " '96+8=104 != 95',\n",
       " '96+9=105 != 96',\n",
       " '96+34=130 != 120',\n",
       " '96+37=133 != 123',\n",
       " '96+38=134 != 124',\n",
       " '96+39=135 != 125',\n",
       " '96+44=140 != 130',\n",
       " '96+49=145 != 135',\n",
       " '96+54=150 != 140',\n",
       " '96+64=160 != 150',\n",
       " '97+0=97 != 98',\n",
       " '97+1=98 != 99',\n",
       " '97+2=99 != 11',\n",
       " '97+3=100 != 12',\n",
       " '97+4=101 != 12',\n",
       " '97+5=102 != 93',\n",
       " '97+6=103 != 94',\n",
       " '97+7=104 != 95',\n",
       " '97+8=105 != 96',\n",
       " '97+9=106 != 97',\n",
       " '97+38=135 != 125',\n",
       " '97+39=136 != 126',\n",
       " '97+43=140 != 130',\n",
       " '97+49=146 != 136',\n",
       " '97+53=150 != 140',\n",
       " '97+60=157 != 147',\n",
       " '97+63=160 != 150',\n",
       " '98+1=99 != 90',\n",
       " '98+2=100 != 111',\n",
       " '98+3=101 != 12',\n",
       " '98+4=102 != 13',\n",
       " '98+5=103 != 94',\n",
       " '98+6=104 != 95',\n",
       " '98+7=105 != 96',\n",
       " '98+8=106 != 97',\n",
       " '98+9=107 != 97',\n",
       " '98+39=137 != 127',\n",
       " '98+52=150 != 140',\n",
       " '98+55=153 != 154',\n",
       " '98+62=160 != 150',\n",
       " '99+1=100 != 90',\n",
       " '99+2=101 != 111',\n",
       " '99+3=102 != 12',\n",
       " '99+4=103 != 13',\n",
       " '99+5=104 != 14',\n",
       " '99+6=105 != 95',\n",
       " '99+7=106 != 96',\n",
       " '99+8=107 != 98',\n",
       " '99+9=108 != 98',\n",
       " '99+31=130 != 120',\n",
       " '99+38=137 != 127',\n",
       " '99+39=138 != 128',\n",
       " '99+41=140 != 130',\n",
       " '99+51=150 != 140',\n",
       " '99+61=160 != 150']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation accuracy jumped to 88% (from 15%). \n",
    "# Let's get an idea of which cases are giving the model a hard time in the validation data:\n",
    "wrongs = []\n",
    "ben.measure_accuracy(q,a, test_fn=test)\n",
    "wrongs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b93905e-3980-4bba-aba9-9782e0632f7c",
   "metadata": {},
   "source": [
    "A pattern is that most errors occur when the second number is single digit - the model is having problems with this, perhaps because it sees relatively little examples (single digits are 10% of two digit examples)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7752a67-06bb-4701-8c6c-cd79f4dd7d4b",
   "metadata": {},
   "source": [
    "### More...\n",
    "\n",
    "Even if dropout reduces overfit, validation data loss is still \n",
    "\n",
    "The training dataset (first adding number 0 to 89) and validation dataset (90 to 99) are sharply cut around the 90 boundary and represent different distributions of the data. This is likely to increase training data overfit because the model is being trained on a subset of the data with a different distribution than the whole data (and the validation set).One could say the training set is not representative of the overall distribution of the data. See add_two_digits_shuffled for how shuffling improves the model by a lot.\n",
    "\n",
    "Perhaps using a zero-padded data format would allow better accuracy, like 82+07=089 ?\n",
    "\n",
    "Why would a smaller block_size (than the 16 we've used) increase overall loss? Should be the opposite because there are now less characters in immediate memory. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
