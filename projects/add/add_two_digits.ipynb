{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0fbbaf-9565-40a9-b378-9f40175390d7",
   "metadata": {},
   "source": [
    "Can the model learn how to add two 2 digit numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b3ce45a-3722-4f8b-ba0b-32ec4a64805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Train, empty_config, LogFlag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de1360-3928-4c16-b436-4a2b69d5d337",
   "metadata": {},
   "source": [
    "To create train and validation datasets run in ../dataprep/:\n",
    "```\n",
    "python prepare_addition.py ../data/add2.txt 2 --sep=\"\\n\" --split=0.9\n",
    "```\n",
    "The creates add2.train.txt and add2.val.txt with entries in the form a+b=cc, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0de89611-55a3-4aa3-a3ca-a829932bf16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90+0=90\n",
      "90+1=91\n",
      "90+2=92\n",
      "90+3=93\n",
      "90+4=94\n",
      "90+5=95\n",
      "90+6=96\n",
      "90+7=97\n",
      "90+8=98\n",
      "90+9=99\n",
      "90+10=100\n",
      "90+11=101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../data/add2.val.txt', 'r', newline=None) as f:\n",
    "    val_data = f.read()\n",
    "print(val_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9046b8b-a8db-486e-9067-746447745b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll load sample via the PaddedLineCharDataset: each read sample line is stored in a 16 character block padded at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e9b4f8-5e0c-4ac6-94eb-75cb182c4e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New random seed 1648103466\n",
      "Initializing new model add2\n",
      "Dataset train_path: ../data/add2.train.txt, val_path: ../data/add2.val.txt, train_split: None, vocab_size: 13\n",
      "Model params: 0.59M\n"
     ]
    }
   ],
   "source": [
    "# create the GPTBench object - we'll name this model add2\n",
    "ben = Train('add2', log_mask=LogFlag.ALL)\n",
    "\n",
    "# set train and validation datasets\n",
    "ben.set_datasets('padlinechar', \n",
    "                 train_path='../data/add2.train.txt', \n",
    "                 val_path='../data/add2.val.txt')\n",
    "\n",
    "# set config settings that will override the default values\n",
    "cfg = empty_config()\n",
    "cfg.train.log_period=0\n",
    "cfg.model.set(n_layer=6, n_head=6, n_embd=90, block_size=16) # our model parameters - block_size is big enough for aa+bb=ccc\n",
    "cfg.sample.set(top=1, max_batch_size=256) # note the top_k(1) - always pick the best item\n",
    "cfg.train.set(sample_period=-5)\n",
    "cfg.trainer.set(batch_size=128)\n",
    "\n",
    "# and init a new model with config\n",
    "ben.init_new(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec487f69-c12a-4f26-85c5-47d6a68eee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Batches per epoch: 70\n",
      "iter 0 (0.000 epoch): loss train=2.1801, val=2.2212, eval->2.2212\n",
      "==> Saving model at iter=0, eval loss->2.2212 \n",
      "9\n",
      ".CUDA max memory used: 174.47M\n",
      "...................................................................................................iter 100 (1.422 epoch): loss train=1.0779, val=1.1047, eval->1.1047\n",
      "==> Saving model at iter=100, eval loss->1.1047 \n",
      "....................................................................................................iter 200 (2.844 epoch): loss train=0.8542, val=0.9584, eval->0.9584\n",
      "==> Saving model at iter=200, eval loss->0.9584 \n",
      "....................................................................................................iter 300 (4.267 epoch): loss train=0.7791, val=0.9301, eval->0.9301\n",
      "==> Saving model at iter=300, eval loss->0.9301 \n",
      "....................................................................................................iter 400 (5.689 epoch): loss train=0.7270, val=0.8859, eval->0.8859\n",
      "==> Saving model at iter=400, eval loss->0.8859 \n",
      "....................................................................................................iter 500 (7.111 epoch): loss train=0.6939, val=0.8865, eval->0.8865\n",
      "20+2=10\n",
      "....................................................................................................iter 600 (8.533 epoch): loss train=0.6738, val=0.8963, eval->0.8963\n",
      "....................................................................................................iter 700 (9.956 epoch): loss train=0.6587, val=0.9454, eval->0.9454\n",
      "....................................................................................................iter 800 (11.378 epoch): loss train=0.6428, val=0.9256, eval->0.9256\n",
      "....................................................................................................iter 900 (12.800 epoch): loss train=0.6334, val=0.9421, eval->0.9421\n",
      "....................................................................................................iter 1000 (14.222 epoch): loss train=0.6190, val=0.9658, eval->0.9658\n",
      "8+68=75\n",
      "....................................................................................................iter 1100 (15.644 epoch): loss train=0.6036, val=0.8896, eval->0.8896\n",
      "....................................................................................................iter 1200 (17.067 epoch): loss train=0.5773, val=0.9167, eval->0.9167\n",
      "....................................................................................................iter 1300 (18.489 epoch): loss train=0.5422, val=0.9462, eval->0.9462\n",
      "....................................................................................................iter 1400 (19.911 epoch): loss train=0.5209, val=0.9629, eval->0.9629\n",
      "....................................................................................................iter 1500 (21.333 epoch): loss train=0.5063, val=0.9492, eval->0.9492\n",
      "51+90=141\n",
      "....................................................................................................iter 1600 (22.756 epoch): loss train=0.4994, val=0.9364, eval->0.9364\n",
      "....................................................................................................iter 1700 (24.178 epoch): loss train=0.4916, val=0.9727, eval->0.9727\n",
      "....................................................................................................iter 1800 (25.600 epoch): loss train=0.4849, val=0.9297, eval->0.9297\n",
      "....................................................................................................iter 1900 (27.022 epoch): loss train=0.4771, val=0.8681, eval->0.8681\n",
      "==> Saving model at iter=1900, eval loss->0.8681 \n",
      "....................................................................................................iter 2000 (28.444 epoch): loss train=0.4747, val=0.8504, eval->0.8504\n",
      "==> Saving model at iter=2000, eval loss->0.8504 \n",
      "71+80=151\n",
      "....................................................................................................iter 2100 (29.867 epoch): loss train=0.4721, val=0.9162, eval->0.9162\n",
      "....................................................................................................iter 2200 (31.289 epoch): loss train=0.4638, val=0.8813, eval->0.8813\n",
      "....................................................................................................iter 2300 (32.711 epoch): loss train=0.4608, val=0.9328, eval->0.9328\n",
      "....................................................................................................iter 2400 (34.133 epoch): loss train=0.4570, val=0.9228, eval->0.9228\n",
      "....................................................................................................iter 2500 (35.556 epoch): loss train=0.4598, val=0.9162, eval->0.9162\n",
      "47+91=138\n",
      "....................................................................................................iter 2600 (36.978 epoch): loss train=0.4524, val=1.0001, eval->1.0001\n",
      "....................................................................................................iter 2700 (38.400 epoch): loss train=0.4521, val=0.9749, eval->0.9749\n",
      "....................................................................................................iter 2800 (39.822 epoch): loss train=0.4491, val=0.9645, eval->0.9645\n",
      "....................................................................................................iter 2900 (41.244 epoch): loss train=0.4479, val=0.9760, eval->0.9760\n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "# Let's train for 3000 batch iterations. \n",
    "# Each dot means a batch was trained.\n",
    "# Train and validation losses are evaluated wach 100 iterations (iters). \n",
    "# Also each 500 iters a random sample is taken.\n",
    "ben.train(iter_count=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05df85bf-d0e9-46b1-a0f7-0ef966c0fe5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 383872,\n",
       " 'train_loss': 0.44791367650032043,\n",
       " 'val_loss': 0.9759516716003418,\n",
       " 'eval_loss': 0.9759516716003418}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The current state loss info:\n",
    "ben.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef787745-d60e-49aa-9a62-f3d4e6d3492f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 256000,\n",
       " 'train_loss': 0.4747387766838074,\n",
       " 'val_loss': 0.8504394292831421,\n",
       " 'eval_loss': 0.8504394292831421}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The last saved checkpoint info - the best performing model we got. Both train and val losses are thus lower than above.\n",
    "ben.last_saved_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c12a9f7-d760-4cf2-8f16-343d57532d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New random seed 542600063\n",
      "Loading checkpoint from ./models/add2/\n",
      "Checkpoint: iter=2000 (28.444 epoch), loss train=0.4747 val=0.8504 eval->0.8504\n",
      "Dataset train_path: ../data/add2.train.txt, val_path: ../data/add2.val.txt, train_split: None, vocab_size: 13\n",
      "Model params: 0.59M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_samples': 256000,\n",
       " 'train_loss': 0.4747387766838074,\n",
       " 'val_loss': 0.8504394292831421,\n",
       " 'eval_loss': 0.8504394292831421}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last saved checkpoint has lower validation loss: let's load it\n",
    "ben.load()\n",
    "ben.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b68280ce-472d-4ad0-9dde-c919c175bede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1=1\n",
      "34+7=41\n",
      "78+99=177\n"
     ]
    }
   ],
   "source": [
    "# take a few samples:\n",
    "ben.sample('1+1=')\n",
    "ben.sample('34+7=')\n",
    "ben.sample('78+99=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bef4335d-bda5-40ba-b06a-5d0724bfdd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0+0=', '0+1=', '0+2=']\n",
      "['0', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "# Let's measure the accuracy of training dataset - this should be mostly memorization, as the model trained on these data\n",
    "train_ds = ben.train_dataset\n",
    "\n",
    "#split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=train_ds.sample_split(0, len(train_ds), sep='=', sep_included=-1)\n",
    "\n",
    "print(q[:3])\n",
    "print(a[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14d0f26d-652f-4928-9aac-20a07433d3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9073333333333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure the accuracy - how good was the memorization? This may take a while and give different results than the number below\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0368e0cc-8dbd-498a-bca2-8196ce996960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['90+0=', '90+1=', '90+2=']\n",
      "['90', '91', '92']\n"
     ]
    }
   ],
   "source": [
    "# We should get a number above 90% for evaluating on train dataset. Further training would improve accuracy, \n",
    "# but the model would be overfitting - memorizing the given samples.\n",
    "# What about the accuracy of the validation dataset, on which the model never trained?\n",
    "val_ds = ben.val_dataset\n",
    "\n",
    "#split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=val_ds.sample_split(0, len(val_ds), sep='=', sep_included=-1)\n",
    "\n",
    "print(q[:3])\n",
    "print(a[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dded223d-05ba-4771-a85f-ea2a7214c14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.592"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation dataset has sums starting in 90+..99+..., for example 90+2=92.\n",
    "# The model did however see the reversed addition of 90.100 numbers, for example 2+90=92.\n",
    "# Did it somehow learn the commutative property of addition?\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "caa92eda-8433-4013-8713-1c00cbe5148b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.592"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How is the model failing - let's see some incorrect answers:\n",
    "\n",
    "wrongs = []\n",
    "def test(q,a,g):\n",
    "    global wrongs\n",
    "    res = float(a == g)\n",
    "    if not res: wrongs += [f\"{q}{a} != {g}\"]\n",
    "    return res\n",
    "\n",
    "ben.measure_accuracy(q,a, test_fn=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "982d9e7d-1e5a-4736-9665-8bc0c6f91056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['91+11=102 != 92',\n",
       " '91+12=103 != 93',\n",
       " '91+13=104 != 94',\n",
       " '91+14=105 != 94',\n",
       " '91+15=106 != 96',\n",
       " '91+16=107 != 97',\n",
       " '91+17=108 != 98',\n",
       " '91+18=109 != 99',\n",
       " '91+19=110 != 100',\n",
       " '91+29=120 != 110']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see some examples:\n",
    "wrongs[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3db76130-9c3c-4749-b012-731642168c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['95+55=150 != 141',\n",
       " '95+56=151 != 141',\n",
       " '95+65=160 != 150',\n",
       " '95+66=161 != 151',\n",
       " '95+75=170 != 160',\n",
       " '95+85=180 != 170',\n",
       " '95+86=181 != 171',\n",
       " '95+95=190 != 180',\n",
       " '95+96=191 != 181',\n",
       " '95+97=192 != 182']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongs[200:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a83e0d-defe-43f5-b2b6-b0c1bc4f448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In many cases it's off by -10..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fa19a06-2ff2-48a9-8ce7-f8a36fcb65ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New random seed 2474415964\n",
      "Initializing new model add2drop\n",
      "Dataset train_path: ../data/add2.train.txt, val_path: ../data/add2.val.txt, train_split: None, vocab_size: 13\n",
      "Model params: 0.59M\n",
      "seed: 0 (int) \n",
      "sample: \n",
      "    max_len: 100 (int) \n",
      "    count: 1 (int) \n",
      "    start_text: None (NoneType) \n",
      "    start_text_sep: | (str) \n",
      "    emit_start: True (bool) \n",
      "    emit_after: None (NoneType) \n",
      "    emit_before: None (NoneType) \n",
      "    flush: True (bool) \n",
      "    eot_stop: 0 (int) \n",
      "    top: 1.0 (float) \n",
      "    temp: 1.0 (float) \n",
      "    max_batch_size: 256 (int) \n",
      "    multiline_prompt: False (bool) \n",
      "train: \n",
      "    eval_period: 100 (int) \n",
      "    eval_type: 1.0 (float) \n",
      "    eval_iters: 100 (int) \n",
      "    eval_save_checkpt: 1 (int) \n",
      "    eval_save_loss: csv,tensorboard (str) \n",
      "    sample_period: -5.0 (float) \n",
      "    log_period: 0.0 (float) \n",
      "dataset: \n",
      "    class_name: padlinechar (str) \n",
      "    train_path: ../data/add2.train.txt (str) \n",
      "    train_split: None (NoneType) \n",
      "    val_path: ../data/add2.val.txt (str) \n",
      "    params: None (NoneType) \n",
      "model: \n",
      "    device: auto (str) \n",
      "    dtype: float32 (str) \n",
      "    n_layer: 6 (int) \n",
      "    n_head: 6 (int) \n",
      "    n_embd: 90 (int) \n",
      "    vocab_size: 13 (int) \n",
      "    block_size: 16 (int) \n",
      "    dropout: 0.2 (float) \n",
      "trainer: \n",
      "    n_workers: 0 (int) \n",
      "    batch_size: 128 (int) \n",
      "    max_samples: None (NoneType) \n",
      "    grad_norm_clip: 1.0 (float) \n",
      "    optimizer: adamw (str) \n",
      "    learning_rate: 0.0001 (float) \n",
      "    adamw_beta1: 0.9 (float) \n",
      "    adamw_beta2: 0.95 (float) \n",
      "    adamw_weight_decay: 0.1 (float) \n"
     ]
    }
   ],
   "source": [
    "# let's try increaisng dropout from its 0.1 default to improve generalization\n",
    "\n",
    "# set config settings that will override existing values - only dropout\n",
    "cfg = empty_config()\n",
    "cfg.model.set(dropout=0.2)\n",
    "\n",
    "# init a new model with config\n",
    "ben.init_new(cfg, name='add2drop')\n",
    "\n",
    "# see total config:\n",
    "print(ben.get_config().dump(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78a74a05-6f66-46ca-884d-afe550d115e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Batches per epoch: 70\n",
      "iter 0 (0.000 epoch): loss train=2.2807, val=2.2979, eval->2.2979\n",
      "==> Saving model at iter=0, eval loss->2.2979 \n",
      "4+\n",
      ".CUDA max memory used: 164.88M\n",
      "...................................................................................................iter 100 (1.422 epoch): loss train=1.0749, val=1.1097, eval->1.1097\n",
      "==> Saving model at iter=100, eval loss->1.1097 \n",
      "....................................................................................................iter 200 (2.844 epoch): loss train=0.8587, val=0.9122, eval->0.9122\n",
      "==> Saving model at iter=200, eval loss->0.9122 \n",
      "....................................................................................................iter 300 (4.267 epoch): loss train=0.7904, val=0.8829, eval->0.8829\n",
      "==> Saving model at iter=300, eval loss->0.8829 \n",
      "....................................................................................................iter 400 (5.689 epoch): loss train=0.7530, val=0.8817, eval->0.8817\n",
      "==> Saving model at iter=400, eval loss->0.8817 \n",
      "....................................................................................................iter 500 (7.111 epoch): loss train=0.7219, val=0.8681, eval->0.8681\n",
      "==> Saving model at iter=500, eval loss->0.8681 \n",
      "32+40=77\n",
      "....................................................................................................iter 600 (8.533 epoch): loss train=0.7046, val=0.8765, eval->0.8765\n",
      "....................................................................................................iter 700 (9.956 epoch): loss train=0.6860, val=0.8694, eval->0.8694\n",
      "....................................................................................................iter 800 (11.378 epoch): loss train=0.6724, val=0.8956, eval->0.8956\n",
      "....................................................................................................iter 900 (12.800 epoch): loss train=0.6599, val=0.8763, eval->0.8763\n",
      "....................................................................................................iter 1000 (14.222 epoch): loss train=0.6520, val=0.9786, eval->0.9786\n",
      "19+20=40\n",
      "....................................................................................................iter 1100 (15.644 epoch): loss train=0.6448, val=0.9633, eval->0.9633\n",
      "....................................................................................................iter 1200 (17.067 epoch): loss train=0.6410, val=0.8936, eval->0.8936\n",
      "....................................................................................................iter 1300 (18.489 epoch): loss train=0.6375, val=1.0082, eval->1.0082\n",
      "....................................................................................................iter 1400 (19.911 epoch): loss train=0.6298, val=1.0000, eval->1.0000\n",
      "....................................................................................................iter 1500 (21.333 epoch): loss train=0.6243, val=0.9704, eval->0.9704\n",
      "6+66=72\n",
      "....................................................................................................iter 1600 (22.756 epoch): loss train=0.6194, val=1.0735, eval->1.0735\n",
      "....................................................................................................iter 1700 (24.178 epoch): loss train=0.6158, val=0.9096, eval->0.9096\n",
      "....................................................................................................iter 1800 (25.600 epoch): loss train=0.6120, val=1.0432, eval->1.0432\n",
      "....................................................................................................iter 1900 (27.022 epoch): loss train=0.5693, val=1.0345, eval->1.0345\n",
      "....................................................................................................iter 2000 (28.444 epoch): loss train=0.5315, val=0.9967, eval->0.9967\n",
      "9+85=94\n",
      "....................................................................................................iter 2100 (29.867 epoch): loss train=0.5176, val=0.8562, eval->0.8562\n",
      "==> Saving model at iter=2100, eval loss->0.8562 \n",
      "....................................................................................................iter 2200 (31.289 epoch): loss train=0.5101, val=0.8103, eval->0.8103\n",
      "==> Saving model at iter=2200, eval loss->0.8103 \n",
      "....................................................................................................iter 2300 (32.711 epoch): loss train=0.4994, val=0.9322, eval->0.9322\n",
      "....................................................................................................iter 2400 (34.133 epoch): loss train=0.4949, val=0.9217, eval->0.9217\n",
      "....................................................................................................iter 2500 (35.556 epoch): loss train=0.4889, val=0.8164, eval->0.8164\n",
      "9+99=108\n",
      "....................................................................................................iter 2600 (36.978 epoch): loss train=0.4846, val=0.9235, eval->0.9235\n",
      "....................................................................................................iter 2700 (38.400 epoch): loss train=0.4781, val=0.9059, eval->0.9059\n",
      "....................................................................................................iter 2800 (39.822 epoch): loss train=0.4733, val=0.9582, eval->0.9582\n",
      "....................................................................................................iter 2900 (41.244 epoch): loss train=0.4728, val=0.9580, eval->0.9580\n",
      "....................................................................................................iter 3000 (42.667 epoch): loss train=0.4711, val=0.9438, eval->0.9438\n",
      "70+20=90\n",
      "....................................................................................................iter 3100 (44.089 epoch): loss train=0.4649, val=0.9535, eval->0.9535\n",
      "....................................................................................................iter 3200 (45.511 epoch): loss train=0.4646, val=0.9504, eval->0.9504\n",
      "....................................................................................................iter 3300 (46.933 epoch): loss train=0.4638, val=0.9245, eval->0.9245\n",
      "....................................................................................................iter 3400 (48.356 epoch): loss train=0.4621, val=0.9773, eval->0.9773\n",
      "....................................................................................................iter 3500 (49.778 epoch): loss train=0.4625, val=0.9126, eval->0.9126\n",
      "\u00001+14=75\n",
      "....................................................................................................iter 3600 (51.200 epoch): loss train=0.4599, val=0.9953, eval->0.9953\n",
      "....................................................................................................iter 3700 (52.622 epoch): loss train=0.4614, val=0.9464, eval->0.9464\n",
      "....................................................................................................iter 3800 (54.044 epoch): loss train=0.4574, val=0.8966, eval->0.8966\n",
      "....................................................................................................iter 3900 (55.467 epoch): loss train=0.4571, val=1.0041, eval->1.0041\n",
      "....................................................................................................iter 4000 (56.889 epoch): loss train=0.4553, val=1.0139, eval->1.0139\n",
      "+8+87=175\n",
      "....................................................................................................iter 4100 (58.311 epoch): loss train=0.4549, val=1.0190, eval->1.0190\n",
      "....................................................................................................iter 4200 (59.733 epoch): loss train=0.4538, val=1.0177, eval->1.0177\n",
      "....................................................................................................iter 4300 (61.156 epoch): loss train=0.4529, val=1.0173, eval->1.0173\n",
      "....................................................................................................iter 4400 (62.578 epoch): loss train=0.4524, val=0.9970, eval->0.9970\n",
      "....................................................................................................iter 4500 (64.000 epoch): loss train=0.4535, val=0.9420, eval->0.9420\n",
      "4+60=64\n",
      "....................................................................................................iter 4600 (65.422 epoch): loss train=0.4519, val=1.0382, eval->1.0382\n",
      "....................................................................................................iter 4700 (66.844 epoch): loss train=0.4515, val=0.9209, eval->0.9209\n",
      "....................................................................................................iter 4800 (68.267 epoch): loss train=0.4516, val=1.0450, eval->1.0450\n",
      "....................................................................................................iter 4900 (69.689 epoch): loss train=0.4509, val=1.0181, eval->1.0181\n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "# train for a bit more - 5000 batch iterations to give it time to converge\n",
    "ben.train(iter_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0e06409-800b-4514-88a4-5c63e4588cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 281600,\n",
       " 'train_loss': 0.5101192593574524,\n",
       " 'val_loss': 0.8103063106536865,\n",
       " 'eval_loss': 0.8103063106536865}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the loss of the best saved state?\n",
    "ben.last_saved_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18afa8a4-9f08-4f83-a575-b03a3d5423a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previous model has train_loss=0.47 and val_loss=0.87 - we got an improvement in validation loss.\n",
    "val_ds = ben.val_dataset\n",
    "\n",
    "#split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=val_ds.sample_split(0, len(val_ds), sep='=', sep_included=-1)\n",
    "\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9cd0ba6b-180a-4c99-9cc7-2a6063695224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wow! Accuracy jumped to 91%. Let's get an idea of which cases are giving the model a hard time:\n",
    "wrongs = []\n",
    "ben.measure_accuracy(q,a, test_fn=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d23f105-9ef9-44eb-9142-556733debd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['90+1=91 != 90',\n",
       " '90+2=92 != 90',\n",
       " '90+3=93 != 90',\n",
       " '90+4=94 != 90',\n",
       " '90+5=95 != 90',\n",
       " '90+6=96 != 95',\n",
       " '91+1=92 != 90',\n",
       " '91+2=93 != 90',\n",
       " '91+3=94 != 90',\n",
       " '91+4=95 != 90',\n",
       " '91+5=96 != 94',\n",
       " '91+6=97 != 95',\n",
       " '91+7=98 != 96',\n",
       " '91+8=99 != 98',\n",
       " '92+1=93 != 90',\n",
       " '92+2=94 != 90',\n",
       " '92+3=95 != 90',\n",
       " '92+4=96 != 90',\n",
       " '92+5=97 != 94',\n",
       " '92+6=98 != 95',\n",
       " '92+7=99 != 96',\n",
       " '92+8=100 != 97',\n",
       " '92+9=101 != 109',\n",
       " '93+1=94 != 90',\n",
       " '93+2=95 != 90',\n",
       " '93+3=96 != 90',\n",
       " '93+4=97 != 90',\n",
       " '93+5=98 != 90',\n",
       " '93+6=99 != 95',\n",
       " '93+7=100 != 97',\n",
       " '93+8=101 != 97',\n",
       " '93+9=102 != 108',\n",
       " '94+1=95 != 90',\n",
       " '94+2=96 != 90',\n",
       " '94+3=97 != 90',\n",
       " '94+4=98 != 90',\n",
       " '94+5=99 != 90',\n",
       " '94+6=100 != 95',\n",
       " '94+7=101 != 106',\n",
       " '94+8=102 != 107',\n",
       " '94+9=103 != 108',\n",
       " '95+1=96 != 90',\n",
       " '95+2=97 != 10',\n",
       " '95+3=98 != 112',\n",
       " '95+4=99 != 12',\n",
       " '95+5=100 != 90',\n",
       " '95+6=101 != 105',\n",
       " '95+7=102 != 107',\n",
       " '95+8=103 != 107',\n",
       " '95+9=104 != 108',\n",
       " '96+1=97 != 10',\n",
       " '96+2=98 != 111',\n",
       " '96+3=99 != 12',\n",
       " '96+4=100 != 13',\n",
       " '96+5=101 != 104',\n",
       " '96+6=102 != 105',\n",
       " '96+7=103 != 106',\n",
       " '96+8=104 != 107',\n",
       " '96+9=105 != 108',\n",
       " '97+1=98 != 10',\n",
       " '97+2=99 != 10',\n",
       " '97+3=100 != 12',\n",
       " '97+4=101 != 13',\n",
       " '97+5=102 != 104',\n",
       " '97+6=103 != 105',\n",
       " '97+7=104 != 106',\n",
       " '97+8=105 != 107',\n",
       " '97+9=106 != 108',\n",
       " '98+1=99 != 10',\n",
       " '98+2=100 != 109',\n",
       " '98+4=102 != 103',\n",
       " '98+5=103 != 105',\n",
       " '98+6=104 != 16',\n",
       " '98+7=105 != 107',\n",
       " '98+8=106 != 107',\n",
       " '98+9=107 != 108',\n",
       " '99+1=100 != 10',\n",
       " '99+2=101 != 109',\n",
       " '99+3=102 != 100',\n",
       " '99+4=103 != 101',\n",
       " '99+5=104 != 105',\n",
       " '99+6=105 != 106',\n",
       " '99+7=106 != 107',\n",
       " '99+8=107 != 108']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Great, we jumped form 59% accuracy to over 91%! Is there any pattern on wrong additions?\n",
    "wrongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4d90b-6dd2-4f7d-8c8e-df95d50fca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouble happens when the second number is single digit...\n",
    "# Perhaps using a zero-padded data format would allow better accuracy, like 99+07=107 ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
