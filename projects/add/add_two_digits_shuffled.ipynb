{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0fbbaf-9565-40a9-b378-9f40175390d7",
   "metadata": {},
   "source": [
    "Can the model learn how to add two 2 digit numbers, from a shuffled dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3ce45a-3722-4f8b-ba0b-32ec4a64805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Train, empty_config, LogFlag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de1360-3928-4c16-b436-4a2b69d5d337",
   "metadata": {},
   "source": [
    "To create train and validation datasets run in ../dataprep/:\n",
    "```\n",
    "python prepare_addition.py ../data/add2_shuffled.txt 2 --sep=\"\\n\" --shuffle\n",
    "```\n",
    "The creates add2_shuffled.txt which will be split at 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de89611-55a3-4aa3-a3ca-a829932bf16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24+83=107\n",
      "54+82=136\n",
      "51+65=116\n",
      "67+5=72\n",
      "12+67=79\n",
      "34+38=72\n",
      "53+10=63\n",
      "47+53=100\n",
      "79+67=146\n",
      "18+40=58\n",
      "65+18=\n"
     ]
    }
   ],
   "source": [
    "with open('../data/add2_shuffled.txt', 'r', newline=None) as f:\n",
    "    val_data = f.read()\n",
    "print(val_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9046b8b-a8db-486e-9067-746447745b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll load sample via the PaddedLineCharDataset: each read sample line is stored in a 16 character block padded at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e9b4f8-5e0c-4ac6-94eb-75cb182c4e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New random seed 1258198076\n",
      "Initializing new model add2_shuffled\n",
      "Dataset train_path: ../data/add2_shuffled.txt, val_path: None, train_split: 0.9, vocab_size: 13\n",
      "Model params: 0.59M\n"
     ]
    }
   ],
   "source": [
    "# create the GPTBench object - we'll name this model add2\n",
    "ben = Train('add2_shuffled', log_mask=LogFlag.ALL)\n",
    "\n",
    "# set train and validation datasets\n",
    "ben.set_datasets('padlinechar', # id for the PaddedLineCharDataset class\n",
    "                 train_path='../data/add2_shuffled.txt', \n",
    "                 train_split=0.9)\n",
    "\n",
    "# set config settings that will override the default values\n",
    "cfg = empty_config()\n",
    "cfg.train.log_period=0\n",
    "cfg.model.set(n_layer=6, n_head=6, n_embd=90, block_size=16) # our model parameters - block_size is big enough for aa+bb=ccc\n",
    "cfg.sample.set(top=1, max_batch_size=256) # note the top_k(1) - always pick the best item\n",
    "cfg.train.set(sample_period=-5)\n",
    "cfg.trainer.set(batch_size=128)\n",
    "\n",
    "# and init a new model with config\n",
    "ben.init_new(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec487f69-c12a-4f26-85c5-47d6a68eee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Batches per epoch: 70\n",
      "iter 3000 (42.667 epoch): loss train=0.4480, val=0.4493, eval->0.4493\n",
      "==> Saving model at iter=3000, eval loss->0.4493 \n",
      "60+44=104\n",
      "....................................................................................................iter 3100 (44.089 epoch): loss train=0.4472, val=0.4479, eval->0.4479\n",
      "==> Saving model at iter=3100, eval loss->0.4479 \n",
      "....................................................................................................iter 3200 (45.511 epoch): loss train=0.4461, val=0.4482, eval->0.4482\n",
      "....................................................................................................iter 3300 (46.933 epoch): loss train=0.4466, val=0.4475, eval->0.4475\n",
      "==> Saving model at iter=3300, eval loss->0.4475 \n",
      "....................................................................................................iter 3400 (48.356 epoch): loss train=0.4440, val=0.4455, eval->0.4455\n",
      "==> Saving model at iter=3400, eval loss->0.4455 \n",
      "....................................................................................................iter 3500 (49.778 epoch): loss train=0.4440, val=0.4457, eval->0.4457\n",
      "32+47=79\n",
      "....................................................................................................iter 3600 (51.200 epoch): loss train=0.4436, val=0.4449, eval->0.4449\n",
      "==> Saving model at iter=3600, eval loss->0.4449 \n",
      "....................................................................................................iter 3700 (52.622 epoch): loss train=0.4428, val=0.4442, eval->0.4442\n",
      "==> Saving model at iter=3700, eval loss->0.4442 \n",
      "....................................................................................................iter 3800 (54.044 epoch): loss train=0.4427, val=0.4457, eval->0.4457\n",
      "....................................................................................................iter 3900 (55.467 epoch): loss train=0.4424, val=0.4442, eval->0.4442\n",
      "....................................................................................................iter 4000 (56.889 epoch): loss train=0.4405, val=0.4426, eval->0.4426\n",
      "==> Saving model at iter=4000, eval loss->0.4426 \n",
      "+1=\n",
      "....................................................................................................iter 4100 (58.311 epoch): loss train=0.4403, val=0.4421, eval->0.4421\n",
      "==> Saving model at iter=4100, eval loss->0.4421 \n",
      "....................................................................................................iter 4200 (59.733 epoch): loss train=0.4403, val=0.4424, eval->0.4424\n",
      "....................................................................................................iter 4300 (61.156 epoch): loss train=0.4402, val=0.4412, eval->0.4412\n",
      "==> Saving model at iter=4300, eval loss->0.4412 \n",
      "....................................................................................................iter 4400 (62.578 epoch): loss train=0.4397, val=0.4406, eval->0.4406\n",
      "==> Saving model at iter=4400, eval loss->0.4406 \n",
      "....................................................................................................iter 4500 (64.000 epoch): loss train=0.4397, val=0.4410, eval->0.4410\n",
      "+1=\n",
      "....................................................................................................iter 4600 (65.422 epoch): loss train=0.4386, val=0.4403, eval->0.4403\n",
      "==> Saving model at iter=4600, eval loss->0.4403 \n",
      "....................................................................................................iter 4700 (66.844 epoch): loss train=0.4388, val=0.4409, eval->0.4409\n",
      "....................................................................................................iter 4800 (68.267 epoch): loss train=0.4389, val=0.4406, eval->0.4406\n",
      "....................................................................................................iter 4900 (69.689 epoch): loss train=0.4394, val=0.4407, eval->0.4407\n",
      "....................................................................................................iter 5000 (71.111 epoch): loss train=0.4381, val=0.4399, eval->0.4399\n",
      "==> Saving model at iter=5000, eval loss->0.4399 \n",
      "0+89=89\n",
      "....................................................................................................iter 5100 (72.533 epoch): loss train=0.4398, val=0.4409, eval->0.4409\n",
      "....................................................................................................iter 5200 (73.956 epoch): loss train=0.4383, val=0.4404, eval->0.4404\n",
      "....................................................................................................iter 5300 (75.378 epoch): loss train=0.4380, val=0.4392, eval->0.4392\n",
      "==> Saving model at iter=5300, eval loss->0.4392 \n",
      "....................................................................................................iter 5400 (76.800 epoch): loss train=0.4380, val=0.4398, eval->0.4398\n",
      "....................................................................................................iter 5500 (78.222 epoch): loss train=0.4380, val=0.4398, eval->0.4398\n",
      "\u00009+2=31\n",
      "....................................................................................................iter 5600 (79.644 epoch): loss train=0.4382, val=0.4394, eval->0.4394\n",
      "....................................................................................................iter 5700 (81.067 epoch): loss train=0.4378, val=0.4402, eval->0.4402\n",
      "....................................................................................................iter 5800 (82.489 epoch): loss train=0.4378, val=0.4390, eval->0.4390\n",
      "==> Saving model at iter=5800, eval loss->0.4390 \n",
      "....................................................................................................iter 5900 (83.911 epoch): loss train=0.4383, val=0.4401, eval->0.4401\n",
      "....................................................................................................iter 6000 (85.333 epoch): loss train=0.4376, val=0.4392, eval->0.4392\n",
      "98+3=101\n",
      "....................................................................................................iter 6100 (86.756 epoch): loss train=0.4374, val=0.4391, eval->0.4391\n",
      "....................................................................................................iter 6200 (88.178 epoch): loss train=0.4373, val=0.4393, eval->0.4393\n",
      "....................................................................................................iter 6300 (89.600 epoch): loss train=0.4375, val=0.4391, eval->0.4391\n",
      "....................................................................................................iter 6400 (91.022 epoch): loss train=0.4373, val=0.4390, eval->0.4390\n",
      "==> Saving model at iter=6400, eval loss->0.4390 \n",
      "....................................................................................................iter 6500 (92.444 epoch): loss train=0.4374, val=0.4393, eval->0.4393\n",
      "62+66=128\n",
      "....................................................................................................iter 6600 (93.867 epoch): loss train=0.4370, val=0.4382, eval->0.4382\n",
      "==> Saving model at iter=6600, eval loss->0.4382 \n",
      "....................................................................................................iter 6700 (95.289 epoch): loss train=0.4373, val=0.4392, eval->0.4392\n",
      "....................................................................................................iter 6800 (96.711 epoch): loss train=0.4375, val=0.4394, eval->0.4394\n",
      "....................................................................................................iter 6900 (98.133 epoch): loss train=0.4372, val=0.4390, eval->0.4390\n",
      "....................................................................................................iter 7000 (99.556 epoch): loss train=0.4375, val=0.4393, eval->0.4393\n",
      "+1=10=21\n",
      "....................................................................................................iter 7100 (100.978 epoch): loss train=0.4368, val=0.4389, eval->0.4389\n",
      "....................................................................................................iter 7200 (102.400 epoch): loss train=0.4371, val=0.4391, eval->0.4391\n",
      "....................................................................................................iter 7300 (103.822 epoch): loss train=0.4371, val=0.4382, eval->0.4382\n",
      "==> Saving model at iter=7300, eval loss->0.4382 \n",
      "....................................................................................................iter 7400 (105.244 epoch): loss train=0.4374, val=0.4395, eval->0.4395\n",
      "....................................................................................................iter 7500 (106.667 epoch): loss train=0.4373, val=0.4388, eval->0.4388\n",
      "97+3=100\n",
      "....................................................................................................iter 7600 (108.089 epoch): loss train=0.4374, val=0.4389, eval->0.4389\n",
      "....................................................................................................iter 7700 (109.511 epoch): loss train=0.4371, val=0.4384, eval->0.4384\n",
      "....................................................................................................iter 7800 (110.933 epoch): loss train=0.4372, val=0.4383, eval->0.4383\n",
      "....................................................................................................iter 7900 (112.356 epoch): loss train=0.4371, val=0.4385, eval->0.4385\n",
      "....................................................................................................iter 8000 (113.778 epoch): loss train=0.4369, val=0.4386, eval->0.4386\n",
      "9+29=38\n",
      "....................................................................................................iter 8100 (115.200 epoch): loss train=0.4374, val=0.4386, eval->0.4386\n",
      "....................................................................................................iter 8200 (116.622 epoch): loss train=0.4369, val=0.4389, eval->0.4389\n",
      "....................................................................................................iter 8300 (118.044 epoch): loss train=0.4370, val=0.4384, eval->0.4384\n",
      "....................................................................................................iter 8400 (119.467 epoch): loss train=0.4375, val=0.4386, eval->0.4386\n",
      "....................................................................................................iter 8500 (120.889 epoch): loss train=0.4373, val=0.4387, eval->0.4387\n",
      "22+17=39\n",
      "....................................................................................................iter 8600 (122.311 epoch): loss train=0.4368, val=0.4388, eval->0.4388\n",
      "....................................................................................................iter 8700 (123.733 epoch): loss train=0.4367, val=0.4386, eval->0.4386\n",
      "....................................................................................................iter 8800 (125.156 epoch): loss train=0.4367, val=0.4386, eval->0.4386\n",
      "....................................................................................................iter 8900 (126.578 epoch): loss train=0.4370, val=0.4385, eval->0.4385\n",
      "....................................................................................................iter 9000 (128.000 epoch): loss train=0.4367, val=0.4392, eval->0.4392\n",
      "64+6=70\n",
      "....................................................................................................iter 9100 (129.422 epoch): loss train=0.4367, val=0.4387, eval->0.4387\n",
      "....................................................................................................iter 9200 (130.844 epoch): loss train=0.4370, val=0.4387, eval->0.4387\n",
      "....................................................................................................iter 9300 (132.267 epoch): loss train=0.4366, val=0.4386, eval->0.4386\n",
      "....................................................................................................iter 9400 (133.689 epoch): loss train=0.4370, val=0.4386, eval->0.4386\n",
      "....................................................................................................iter 9500 (135.111 epoch): loss train=0.4369, val=0.4383, eval->0.4383\n",
      "4+52=56\n",
      "....................................................................................................iter 9600 (136.533 epoch): loss train=0.4369, val=0.4387, eval->0.4387\n",
      "....................................................................................................iter 9700 (137.956 epoch): loss train=0.4370, val=0.4387, eval->0.4387\n",
      "....................................................................................................iter 9800 (139.378 epoch): loss train=0.4370, val=0.4391, eval->0.4391\n",
      "....................................................................................................iter 9900 (140.800 epoch): loss train=0.4371, val=0.4384, eval->0.4384\n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "# Let's train for 3000 batch iterations. \n",
    "# Each dot means a batch was trained.\n",
    "# Train and validation losses are evaluated each 100 iterations (iters). \n",
    "# Also each 500 iters a random sample is taken.\n",
    "ben.train(iter_count=7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05df85bf-d0e9-46b1-a0f7-0ef966c0fe5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 1279872,\n",
       " 'train_loss': 0.4370552897453308,\n",
       " 'val_loss': 0.4384059011936188,\n",
       " 'eval_loss': 0.4384059011936188}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The current state loss info:\n",
    "ben.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef787745-d60e-49aa-9a62-f3d4e6d3492f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 934400,\n",
       " 'train_loss': 0.43707430362701416,\n",
       " 'val_loss': 0.4381597936153412,\n",
       " 'eval_loss': 0.4381597936153412}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The last saved checkpoint info - the best performing model we got. Both train and val losses are thus lower than above.\n",
    "ben.last_saved_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c12a9f7-d760-4cf2-8f16-343d57532d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New random seed 1866725781\n",
      "Loading checkpoint from ./models/add2_shuffled/\n",
      "Checkpoint: iter=7300 (103.822 epoch), loss train=0.4371 val=0.4382 eval->0.4382\n",
      "Dataset train_path: ../data/add2_shuffled.txt, val_path: None, train_split: 0.9, vocab_size: 13\n",
      "Model params: 0.59M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_samples': 934400,\n",
       " 'train_loss': 0.43707430362701416,\n",
       " 'val_loss': 0.4381597936153412,\n",
       " 'eval_loss': 0.4381597936153412}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last saved checkpoint has lower validation loss: let's load it\n",
    "ben.load()\n",
    "ben.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b68280ce-472d-4ad0-9dde-c919c175bede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1=1\n",
      "34+7=41\n",
      "78+99=177\n"
     ]
    }
   ],
   "source": [
    "# take a few samples:\n",
    "ben.sample('1+1=')\n",
    "ben.sample('34+7=')\n",
    "ben.sample('78+99=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bef4335d-bda5-40ba-b06a-5d0724bfdd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['24+83=', '54+82=', '51+65=']\n",
      "['107', '136', '116']\n"
     ]
    }
   ],
   "source": [
    "# Let's measure the accuracy of training dataset - this should be mostly memorization, as the model trained on these data\n",
    "train_ds = ben.train_dataset\n",
    "\n",
    "#split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=train_ds.sample_split(0, len(train_ds), sep='=', sep_included=-1)\n",
    "\n",
    "print(q[:3])\n",
    "print(a[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14d0f26d-652f-4928-9aac-20a07433d3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997777777777778"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure the accuracy - how good was the memorization? This may take a while and give different results than the number below\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0368e0cc-8dbd-498a-bca2-8196ce996960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['84+21=', '90+56=', '65+70=']\n",
      "['105', '146', '135']\n"
     ]
    }
   ],
   "source": [
    "# We should get a number above 90% for evaluating on train dataset. Further training would improve accuracy, \n",
    "# but the model would be overfitting - memorizing the given samples.\n",
    "# What about the accuracy of the validation dataset, on which the model never trained?\n",
    "val_ds = ben.val_dataset\n",
    "\n",
    "#split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=val_ds.sample_split(0, len(val_ds), sep='=', sep_included=-1)\n",
    "\n",
    "print(q[:3])\n",
    "print(a[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dded223d-05ba-4771-a85f-ea2a7214c14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation dataset has sums starting in 90+..99+..., for example 90+2=92.\n",
    "# The model did however see the reversed addition of 90.100 numbers, for example 2+90=92.\n",
    "# Did it somehow learn the commutative property of addition?\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caa92eda-8433-4013-8713-1c00cbe5148b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.984"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How is the model failing - let's see some incorrect answers:\n",
    "\n",
    "wrongs = []\n",
    "def test(q,a,g):\n",
    "    global wrongs\n",
    "    res = float(a == g)\n",
    "    if not res: wrongs += [f\"{q}{a} != {g}\"]\n",
    "    return res\n",
    "\n",
    "ben.measure_accuracy(q,a, test_fn=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "982d9e7d-1e5a-4736-9665-8bc0c6f91056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['86+3=89 != 99',\n",
       " '8+62=70 != 60',\n",
       " '0+0=0 != 9',\n",
       " '2+3=5 != 4',\n",
       " '30+59=89 != 99',\n",
       " '58+2=60 != 50',\n",
       " '15+3=18 != 17',\n",
       " '4+3=7 != 6',\n",
       " '7+93=100 != 90',\n",
       " '1+2=3 != 2',\n",
       " '3+15=18 != 19',\n",
       " '4+26=30 != 20',\n",
       " '6+48=54 != 55',\n",
       " '69+1=70 != 60',\n",
       " '6+49=55 != 56',\n",
       " '96+4=100 != 101']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see some examples:\n",
    "wrongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd2a2bd9-b808-4a9e-acb8-18f347958e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4+6=10 != 11', '16+6=22 != 21']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = ben.train_dataset\n",
    "\n",
    "#split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=train_ds.sample_split(0, len(train_ds), sep='=', sep_included=-1)\n",
    "\n",
    "wrongs=[]\n",
    "ben.measure_accuracy(q,a, test_fn=test)\n",
    "wrongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3db76130-9c3c-4749-b012-731642168c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0=0\n"
     ]
    }
   ],
   "source": [
    "ben.sample('0+0=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a83e0d-defe-43f5-b2b6-b0c1bc4f448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In many cases it's off by -10..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fa19a06-2ff2-48a9-8ce7-f8a36fcb65ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New random seed 2474415964\n",
      "Initializing new model add2drop\n",
      "Dataset train_path: ../data/add2.train.txt, val_path: ../data/add2.val.txt, train_split: None, vocab_size: 13\n",
      "Model params: 0.59M\n",
      "seed: 0 (int) \n",
      "sample: \n",
      "    max_len: 100 (int) \n",
      "    count: 1 (int) \n",
      "    start_text: None (NoneType) \n",
      "    start_text_sep: | (str) \n",
      "    emit_start: True (bool) \n",
      "    emit_after: None (NoneType) \n",
      "    emit_before: None (NoneType) \n",
      "    flush: True (bool) \n",
      "    eot_stop: 0 (int) \n",
      "    top: 1.0 (float) \n",
      "    temp: 1.0 (float) \n",
      "    max_batch_size: 256 (int) \n",
      "    multiline_prompt: False (bool) \n",
      "train: \n",
      "    eval_period: 100 (int) \n",
      "    eval_type: 1.0 (float) \n",
      "    eval_iters: 100 (int) \n",
      "    eval_save_checkpt: 1 (int) \n",
      "    eval_save_loss: csv,tensorboard (str) \n",
      "    sample_period: -5.0 (float) \n",
      "    log_period: 0.0 (float) \n",
      "dataset: \n",
      "    class_name: padlinechar (str) \n",
      "    train_path: ../data/add2.train.txt (str) \n",
      "    train_split: None (NoneType) \n",
      "    val_path: ../data/add2.val.txt (str) \n",
      "    params: None (NoneType) \n",
      "model: \n",
      "    device: auto (str) \n",
      "    dtype: float32 (str) \n",
      "    n_layer: 6 (int) \n",
      "    n_head: 6 (int) \n",
      "    n_embd: 90 (int) \n",
      "    vocab_size: 13 (int) \n",
      "    block_size: 16 (int) \n",
      "    dropout: 0.2 (float) \n",
      "trainer: \n",
      "    n_workers: 0 (int) \n",
      "    batch_size: 128 (int) \n",
      "    max_samples: None (NoneType) \n",
      "    grad_norm_clip: 1.0 (float) \n",
      "    optimizer: adamw (str) \n",
      "    learning_rate: 0.0001 (float) \n",
      "    adamw_beta1: 0.9 (float) \n",
      "    adamw_beta2: 0.95 (float) \n",
      "    adamw_weight_decay: 0.1 (float) \n"
     ]
    }
   ],
   "source": [
    "# let's try increaisng dropout from its 0.1 default to improve generalization\n",
    "\n",
    "# set config settings that will override existing values - only dropout\n",
    "cfg = empty_config()\n",
    "cfg.model.set(dropout=0.2)\n",
    "\n",
    "# init a new model with config\n",
    "ben.init_new(cfg, name='add2drop')\n",
    "\n",
    "# see total config:\n",
    "print(ben.get_config().dump(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78a74a05-6f66-46ca-884d-afe550d115e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Batches per epoch: 70\n",
      "iter 0 (0.000 epoch): loss train=2.2807, val=2.2979, eval->2.2979\n",
      "==> Saving model at iter=0, eval loss->2.2979 \n",
      "4+\n",
      ".CUDA max memory used: 164.88M\n",
      "...................................................................................................iter 100 (1.422 epoch): loss train=1.0749, val=1.1097, eval->1.1097\n",
      "==> Saving model at iter=100, eval loss->1.1097 \n",
      "....................................................................................................iter 200 (2.844 epoch): loss train=0.8587, val=0.9122, eval->0.9122\n",
      "==> Saving model at iter=200, eval loss->0.9122 \n",
      "....................................................................................................iter 300 (4.267 epoch): loss train=0.7904, val=0.8829, eval->0.8829\n",
      "==> Saving model at iter=300, eval loss->0.8829 \n",
      "....................................................................................................iter 400 (5.689 epoch): loss train=0.7530, val=0.8817, eval->0.8817\n",
      "==> Saving model at iter=400, eval loss->0.8817 \n",
      "....................................................................................................iter 500 (7.111 epoch): loss train=0.7219, val=0.8681, eval->0.8681\n",
      "==> Saving model at iter=500, eval loss->0.8681 \n",
      "32+40=77\n",
      "....................................................................................................iter 600 (8.533 epoch): loss train=0.7046, val=0.8765, eval->0.8765\n",
      "....................................................................................................iter 700 (9.956 epoch): loss train=0.6860, val=0.8694, eval->0.8694\n",
      "....................................................................................................iter 800 (11.378 epoch): loss train=0.6724, val=0.8956, eval->0.8956\n",
      "....................................................................................................iter 900 (12.800 epoch): loss train=0.6599, val=0.8763, eval->0.8763\n",
      "....................................................................................................iter 1000 (14.222 epoch): loss train=0.6520, val=0.9786, eval->0.9786\n",
      "19+20=40\n",
      "....................................................................................................iter 1100 (15.644 epoch): loss train=0.6448, val=0.9633, eval->0.9633\n",
      "....................................................................................................iter 1200 (17.067 epoch): loss train=0.6410, val=0.8936, eval->0.8936\n",
      "....................................................................................................iter 1300 (18.489 epoch): loss train=0.6375, val=1.0082, eval->1.0082\n",
      "....................................................................................................iter 1400 (19.911 epoch): loss train=0.6298, val=1.0000, eval->1.0000\n",
      "....................................................................................................iter 1500 (21.333 epoch): loss train=0.6243, val=0.9704, eval->0.9704\n",
      "6+66=72\n",
      "....................................................................................................iter 1600 (22.756 epoch): loss train=0.6194, val=1.0735, eval->1.0735\n",
      "....................................................................................................iter 1700 (24.178 epoch): loss train=0.6158, val=0.9096, eval->0.9096\n",
      "....................................................................................................iter 1800 (25.600 epoch): loss train=0.6120, val=1.0432, eval->1.0432\n",
      "....................................................................................................iter 1900 (27.022 epoch): loss train=0.5693, val=1.0345, eval->1.0345\n",
      "....................................................................................................iter 2000 (28.444 epoch): loss train=0.5315, val=0.9967, eval->0.9967\n",
      "9+85=94\n",
      "....................................................................................................iter 2100 (29.867 epoch): loss train=0.5176, val=0.8562, eval->0.8562\n",
      "==> Saving model at iter=2100, eval loss->0.8562 \n",
      "....................................................................................................iter 2200 (31.289 epoch): loss train=0.5101, val=0.8103, eval->0.8103\n",
      "==> Saving model at iter=2200, eval loss->0.8103 \n",
      "....................................................................................................iter 2300 (32.711 epoch): loss train=0.4994, val=0.9322, eval->0.9322\n",
      "....................................................................................................iter 2400 (34.133 epoch): loss train=0.4949, val=0.9217, eval->0.9217\n",
      "....................................................................................................iter 2500 (35.556 epoch): loss train=0.4889, val=0.8164, eval->0.8164\n",
      "9+99=108\n",
      "....................................................................................................iter 2600 (36.978 epoch): loss train=0.4846, val=0.9235, eval->0.9235\n",
      "....................................................................................................iter 2700 (38.400 epoch): loss train=0.4781, val=0.9059, eval->0.9059\n",
      "....................................................................................................iter 2800 (39.822 epoch): loss train=0.4733, val=0.9582, eval->0.9582\n",
      "....................................................................................................iter 2900 (41.244 epoch): loss train=0.4728, val=0.9580, eval->0.9580\n",
      "....................................................................................................iter 3000 (42.667 epoch): loss train=0.4711, val=0.9438, eval->0.9438\n",
      "70+20=90\n",
      "....................................................................................................iter 3100 (44.089 epoch): loss train=0.4649, val=0.9535, eval->0.9535\n",
      "....................................................................................................iter 3200 (45.511 epoch): loss train=0.4646, val=0.9504, eval->0.9504\n",
      "....................................................................................................iter 3300 (46.933 epoch): loss train=0.4638, val=0.9245, eval->0.9245\n",
      "....................................................................................................iter 3400 (48.356 epoch): loss train=0.4621, val=0.9773, eval->0.9773\n",
      "....................................................................................................iter 3500 (49.778 epoch): loss train=0.4625, val=0.9126, eval->0.9126\n",
      "\u00001+14=75\n",
      "....................................................................................................iter 3600 (51.200 epoch): loss train=0.4599, val=0.9953, eval->0.9953\n",
      "....................................................................................................iter 3700 (52.622 epoch): loss train=0.4614, val=0.9464, eval->0.9464\n",
      "....................................................................................................iter 3800 (54.044 epoch): loss train=0.4574, val=0.8966, eval->0.8966\n",
      "....................................................................................................iter 3900 (55.467 epoch): loss train=0.4571, val=1.0041, eval->1.0041\n",
      "....................................................................................................iter 4000 (56.889 epoch): loss train=0.4553, val=1.0139, eval->1.0139\n",
      "+8+87=175\n",
      "....................................................................................................iter 4100 (58.311 epoch): loss train=0.4549, val=1.0190, eval->1.0190\n",
      "....................................................................................................iter 4200 (59.733 epoch): loss train=0.4538, val=1.0177, eval->1.0177\n",
      "....................................................................................................iter 4300 (61.156 epoch): loss train=0.4529, val=1.0173, eval->1.0173\n",
      "....................................................................................................iter 4400 (62.578 epoch): loss train=0.4524, val=0.9970, eval->0.9970\n",
      "....................................................................................................iter 4500 (64.000 epoch): loss train=0.4535, val=0.9420, eval->0.9420\n",
      "4+60=64\n",
      "....................................................................................................iter 4600 (65.422 epoch): loss train=0.4519, val=1.0382, eval->1.0382\n",
      "....................................................................................................iter 4700 (66.844 epoch): loss train=0.4515, val=0.9209, eval->0.9209\n",
      "....................................................................................................iter 4800 (68.267 epoch): loss train=0.4516, val=1.0450, eval->1.0450\n",
      "....................................................................................................iter 4900 (69.689 epoch): loss train=0.4509, val=1.0181, eval->1.0181\n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "# train for a bit more - 5000 batch iterations to give it time to converge\n",
    "ben.train(iter_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0e06409-800b-4514-88a4-5c63e4588cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 281600,\n",
       " 'train_loss': 0.5101192593574524,\n",
       " 'val_loss': 0.8103063106536865,\n",
       " 'eval_loss': 0.8103063106536865}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the loss of the best saved state?\n",
    "ben.last_saved_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18afa8a4-9f08-4f83-a575-b03a3d5423a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previous model has train_loss=0.47 and val_loss=0.87 - we got an improvement in validation loss.\n",
    "val_ds = ben.val_dataset\n",
    "\n",
    "#split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=val_ds.sample_split(0, len(val_ds), sep='=', sep_included=-1)\n",
    "\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9cd0ba6b-180a-4c99-9cc7-2a6063695224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wow! Accuracy jumped to 91%. Let's get an idea of which cases are giving the model a hard time:\n",
    "wrongs = []\n",
    "ben.measure_accuracy(q,a, test_fn=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d23f105-9ef9-44eb-9142-556733debd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['90+1=91 != 90',\n",
       " '90+2=92 != 90',\n",
       " '90+3=93 != 90',\n",
       " '90+4=94 != 90',\n",
       " '90+5=95 != 90',\n",
       " '90+6=96 != 95',\n",
       " '91+1=92 != 90',\n",
       " '91+2=93 != 90',\n",
       " '91+3=94 != 90',\n",
       " '91+4=95 != 90',\n",
       " '91+5=96 != 94',\n",
       " '91+6=97 != 95',\n",
       " '91+7=98 != 96',\n",
       " '91+8=99 != 98',\n",
       " '92+1=93 != 90',\n",
       " '92+2=94 != 90',\n",
       " '92+3=95 != 90',\n",
       " '92+4=96 != 90',\n",
       " '92+5=97 != 94',\n",
       " '92+6=98 != 95',\n",
       " '92+7=99 != 96',\n",
       " '92+8=100 != 97',\n",
       " '92+9=101 != 109',\n",
       " '93+1=94 != 90',\n",
       " '93+2=95 != 90',\n",
       " '93+3=96 != 90',\n",
       " '93+4=97 != 90',\n",
       " '93+5=98 != 90',\n",
       " '93+6=99 != 95',\n",
       " '93+7=100 != 97',\n",
       " '93+8=101 != 97',\n",
       " '93+9=102 != 108',\n",
       " '94+1=95 != 90',\n",
       " '94+2=96 != 90',\n",
       " '94+3=97 != 90',\n",
       " '94+4=98 != 90',\n",
       " '94+5=99 != 90',\n",
       " '94+6=100 != 95',\n",
       " '94+7=101 != 106',\n",
       " '94+8=102 != 107',\n",
       " '94+9=103 != 108',\n",
       " '95+1=96 != 90',\n",
       " '95+2=97 != 10',\n",
       " '95+3=98 != 112',\n",
       " '95+4=99 != 12',\n",
       " '95+5=100 != 90',\n",
       " '95+6=101 != 105',\n",
       " '95+7=102 != 107',\n",
       " '95+8=103 != 107',\n",
       " '95+9=104 != 108',\n",
       " '96+1=97 != 10',\n",
       " '96+2=98 != 111',\n",
       " '96+3=99 != 12',\n",
       " '96+4=100 != 13',\n",
       " '96+5=101 != 104',\n",
       " '96+6=102 != 105',\n",
       " '96+7=103 != 106',\n",
       " '96+8=104 != 107',\n",
       " '96+9=105 != 108',\n",
       " '97+1=98 != 10',\n",
       " '97+2=99 != 10',\n",
       " '97+3=100 != 12',\n",
       " '97+4=101 != 13',\n",
       " '97+5=102 != 104',\n",
       " '97+6=103 != 105',\n",
       " '97+7=104 != 106',\n",
       " '97+8=105 != 107',\n",
       " '97+9=106 != 108',\n",
       " '98+1=99 != 10',\n",
       " '98+2=100 != 109',\n",
       " '98+4=102 != 103',\n",
       " '98+5=103 != 105',\n",
       " '98+6=104 != 16',\n",
       " '98+7=105 != 107',\n",
       " '98+8=106 != 107',\n",
       " '98+9=107 != 108',\n",
       " '99+1=100 != 10',\n",
       " '99+2=101 != 109',\n",
       " '99+3=102 != 100',\n",
       " '99+4=103 != 101',\n",
       " '99+5=104 != 105',\n",
       " '99+6=105 != 106',\n",
       " '99+7=106 != 107',\n",
       " '99+8=107 != 108']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Great, we jumped form 59% accuracy to over 91%! Is there any pattern on wrong additions?\n",
    "wrongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4d90b-6dd2-4f7d-8c8e-df95d50fca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouble happens when the second number is single digit...\n",
    "# Perhaps using a zero-padded data format would allow better accuracy, like 99+07=107 ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
