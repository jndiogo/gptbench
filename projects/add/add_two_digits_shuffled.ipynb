{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0fbbaf-9565-40a9-b378-9f40175390d7",
   "metadata": {},
   "source": [
    "Can the model learn how to add two 2 digit numbers, from a shuffled dataset?\n",
    "\n",
    "The \"add_two_digits\" notebook sharply divided the test dataset (0..89 for first adding term) and validation dataset (90..99). This hurts generalization, because the distribution of the training data is not representative of the whole data - it misses 90..99 additions.\n",
    "\n",
    "So here we shuffle before splitting, hoping that the model is now trained on a more representative distribution of the entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3ce45a-3722-4f8b-ba0b-32ec4a64805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Train, empty_config, LogFlag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de1360-3928-4c16-b436-4a2b69d5d337",
   "metadata": {},
   "source": [
    "We'll load ../data/add2.txt from the add_two_digits notebook, which can be created by running in the dataprep folder:\n",
    "```\n",
    "python prepare_addition.py ../data/add2.txt 2 --sep=\"\\n\"\n",
    "```\n",
    "The creates add2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4742c678-5e39-41f2-9589-a9f16fa7751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first: 0+0=0\n",
      "0+1=1\n",
      "0+2=2\n",
      "0+3=3\n",
      "0+4=4\n",
      "0+5=5\n",
      "0+6=6\n",
      "0+7=7\n",
      "0+8=8\n",
      "0+9=9\n",
      "0+10=10\n",
      "0+11=11\n",
      "0+12=12\n",
      "0+13=13\n",
      "0+14=14\n",
      "\n",
      "last: 99+90=189\n",
      "99+91=190\n",
      "99+92=191\n",
      "99+93=192\n",
      "99+94=193\n",
      "99+95=194\n",
      "99+96=195\n",
      "99+97=196\n",
      "99+98=197\n",
      "99+99=198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Opening it - the first 100 chars\n",
    "with open('../data/add2.txt', 'r', newline=None) as f:\n",
    "    data = f.read()\n",
    "print(\"first:\", data[:100])\n",
    "print(\"last:\", data[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9046b8b-a8db-486e-9067-746447745b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll load these data samples into two CharLineDatasets, taking care to shuffle the data before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e9b4f8-5e0c-4ac6-94eb-75cb182c4e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================seed 2916266069\n",
      "Initializing new model add2_shuffled\n",
      "Dataset train_path: ../data/add2.txt, val_path: None, train_split: 0.9, vocab_size: 13\n",
      "Model params: 0.59M\n"
     ]
    }
   ],
   "source": [
    "# create the GPTBench object - we'll name this model add2\n",
    "ben = Train('add2_shuffled', log_mask=LogFlag.ALL)\n",
    "ben.set_seed(0xADD2B055)\n",
    "\n",
    "# set train and validation datasets\n",
    "ben.set_datasets(class_name='charline', # id for the PaddedLineCharDataset class\n",
    "                 train_path='../data/add2.txt', \n",
    "                 train_split=0.9,\n",
    "                 pre_shuffle=True)\n",
    "\n",
    "# set config settings that will override the default values\n",
    "cfg = empty_config()\n",
    "cfg.train.log_period=0\n",
    "cfg.model.set(n_layer=6, n_head=6, n_embd=90, block_size=16) # our model parameters - block_size is big enough for aa+bb=ccc\n",
    "cfg.sample.set(top=1, max_batch_size=256) # note the top_k(1) - always pick the best item\n",
    "cfg.train.set(sample_period=-5)\n",
    "cfg.trainer.set(batch_size=128)\n",
    "\n",
    "# and init a new model with config\n",
    "ben.init_new(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "322a19aa-7b8c-49ab-890b-6b43431d1a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['25+47=72',\n",
       "  '57+16=73',\n",
       "  '3+59=62',\n",
       "  '24+18=42',\n",
       "  '53+3=56',\n",
       "  '2+3=5',\n",
       "  '28+67=95',\n",
       "  '72+13=85',\n",
       "  '54+52=106',\n",
       "  '26+21=47'],\n",
       " ['18+25=43',\n",
       "  '9+72=81',\n",
       "  '64+75=139',\n",
       "  '74+21=95',\n",
       "  '54+37=91',\n",
       "  '18+74=92',\n",
       "  '42+11=53',\n",
       "  '48+57=105',\n",
       "  '31+41=72',\n",
       "  '5+38=43'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# both train and validation datasets use shuffled data from the add2.txt source file\n",
    "ben.train_dataset.get_data()[:10], ben.val_dataset.get_data()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec487f69-c12a-4f26-85c5-47d6a68eee80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Batches per epoch: 70\n",
      "iter 0 (0.000 epoch): loss train=2.1426, val=2.1427, eval->2.1427\n",
      "==> Saving model at iter=0, eval loss->2.1427 \n",
      "0=\n",
      "CUDA max memory used: 164.88M\n",
      "....................................................................................................iter 100 (1.422 epoch): loss train=1.0543, val=1.0541, eval->1.0541\n",
      "==> Saving model at iter=100, eval loss->1.0541 \n",
      "....................................................................................................iter 200 (2.845 epoch): loss train=0.8524, val=0.8539, eval->0.8539\n",
      "==> Saving model at iter=200, eval loss->0.8539 \n",
      "....................................................................................................iter 300 (4.267 epoch): loss train=0.7820, val=0.7822, eval->0.7822\n",
      "==> Saving model at iter=300, eval loss->0.7822 \n",
      "....................................................................................................iter 400 (5.690 epoch): loss train=0.7295, val=0.7294, eval->0.7294\n",
      "==> Saving model at iter=400, eval loss->0.7294 \n",
      "....................................................................................................iter 500 (7.112 epoch): loss train=0.6990, val=0.7001, eval->0.7001\n",
      "==> Saving model at iter=500, eval loss->0.7001 \n",
      "43+42=86\n",
      "....................................................................................................iter 600 (8.534 epoch): loss train=0.6776, val=0.6802, eval->0.6802\n",
      "==> Saving model at iter=600, eval loss->0.6802 \n",
      "....................................................................................................iter 700 (9.957 epoch): loss train=0.6617, val=0.6636, eval->0.6636\n",
      "==> Saving model at iter=700, eval loss->0.6636 \n",
      "....................................................................................................iter 800 (11.379 epoch): loss train=0.6538, val=0.6555, eval->0.6555\n",
      "==> Saving model at iter=800, eval loss->0.6555 \n",
      "....................................................................................................iter 900 (12.801 epoch): loss train=0.6410, val=0.6434, eval->0.6434\n",
      "==> Saving model at iter=900, eval loss->0.6434 \n",
      "....................................................................................................iter 1000 (14.224 epoch): loss train=0.6297, val=0.6330, eval->0.6330\n",
      "==> Saving model at iter=1000, eval loss->0.6330 \n",
      "60+74=131\n",
      "....................................................................................................iter 1100 (15.646 epoch): loss train=0.6244, val=0.6273, eval->0.6273\n",
      "==> Saving model at iter=1100, eval loss->0.6273 \n",
      "....................................................................................................iter 1200 (17.069 epoch): loss train=0.6179, val=0.6217, eval->0.6217\n",
      "==> Saving model at iter=1200, eval loss->0.6217 \n",
      "....................................................................................................iter 1300 (18.491 epoch): loss train=0.5877, val=0.5927, eval->0.5927\n",
      "==> Saving model at iter=1300, eval loss->0.5927 \n",
      "....................................................................................................iter 1400 (19.913 epoch): loss train=0.5451, val=0.5471, eval->0.5471\n",
      "==> Saving model at iter=1400, eval loss->0.5471 \n",
      "....................................................................................................iter 1500 (21.336 epoch): loss train=0.5243, val=0.5268, eval->0.5268\n",
      "==> Saving model at iter=1500, eval loss->0.5268 \n",
      "5+42=46\n",
      "....................................................................................................iter 1600 (22.758 epoch): loss train=0.5115, val=0.5149, eval->0.5149\n",
      "==> Saving model at iter=1600, eval loss->0.5149 \n",
      "....................................................................................................iter 1700 (24.180 epoch): loss train=0.5004, val=0.5044, eval->0.5044\n",
      "==> Saving model at iter=1700, eval loss->0.5044 \n",
      "....................................................................................................iter 1800 (25.603 epoch): loss train=0.4941, val=0.4968, eval->0.4968\n",
      "==> Saving model at iter=1800, eval loss->0.4968 \n",
      "....................................................................................................iter 1900 (27.025 epoch): loss train=0.4881, val=0.4918, eval->0.4918\n",
      "==> Saving model at iter=1900, eval loss->0.4918 \n",
      "....................................................................................................iter 2000 (28.448 epoch): loss train=0.4810, val=0.4846, eval->0.4846\n",
      "==> Saving model at iter=2000, eval loss->0.4846 \n",
      "28+34=62\n",
      "....................................................................................................iter 2100 (29.870 epoch): loss train=0.4734, val=0.4760, eval->0.4760\n",
      "==> Saving model at iter=2100, eval loss->0.4760 \n",
      "....................................................................................................iter 2200 (31.292 epoch): loss train=0.4698, val=0.4721, eval->0.4721\n",
      "==> Saving model at iter=2200, eval loss->0.4721 \n",
      "....................................................................................................iter 2300 (32.715 epoch): loss train=0.4664, val=0.4697, eval->0.4697\n",
      "==> Saving model at iter=2300, eval loss->0.4697 \n",
      "....................................................................................................iter 2400 (34.137 epoch): loss train=0.4646, val=0.4655, eval->0.4655\n",
      "==> Saving model at iter=2400, eval loss->0.4655 \n",
      "....................................................................................................iter 2500 (35.560 epoch): loss train=0.4599, val=0.4630, eval->0.4630\n",
      "==> Saving model at iter=2500, eval loss->0.4630 \n",
      "+1+11=32\n",
      "....................................................................................................iter 2600 (36.982 epoch): loss train=0.4559, val=0.4588, eval->0.4588\n",
      "==> Saving model at iter=2600, eval loss->0.4588 \n",
      "....................................................................................................iter 2700 (38.404 epoch): loss train=0.4560, val=0.4587, eval->0.4587\n",
      "==> Saving model at iter=2700, eval loss->0.4587 \n",
      "....................................................................................................iter 2800 (39.827 epoch): loss train=0.4524, val=0.4550, eval->0.4550\n",
      "==> Saving model at iter=2800, eval loss->0.4550 \n",
      "....................................................................................................iter 2900 (41.249 epoch): loss train=0.4500, val=0.4513, eval->0.4513\n",
      "==> Saving model at iter=2900, eval loss->0.4513 \n",
      "....................................................................................................iter 3000 (42.671 epoch): loss train=0.4488, val=0.4513, eval->0.4513\n",
      "2+3=5\n",
      "....................................................................................................iter 3100 (44.094 epoch): loss train=0.4482, val=0.4497, eval->0.4497\n",
      "==> Saving model at iter=3100, eval loss->0.4497 \n",
      "....................................................................................................iter 3200 (45.516 epoch): loss train=0.4484, val=0.4494, eval->0.4494\n",
      "==> Saving model at iter=3200, eval loss->0.4494 \n",
      "....................................................................................................iter 3300 (46.939 epoch): loss train=0.4463, val=0.4469, eval->0.4469\n",
      "==> Saving model at iter=3300, eval loss->0.4469 \n",
      "....................................................................................................iter 3400 (48.361 epoch): loss train=0.4448, val=0.4458, eval->0.4458\n",
      "==> Saving model at iter=3400, eval loss->0.4458 \n",
      "....................................................................................................iter 3500 (49.783 epoch): loss train=0.4455, val=0.4471, eval->0.4471\n",
      "0+80=80\n",
      "....................................................................................................iter 3600 (51.206 epoch): loss train=0.4446, val=0.4461, eval->0.4461\n",
      "....................................................................................................iter 3700 (52.628 epoch): loss train=0.4438, val=0.4449, eval->0.4449\n",
      "==> Saving model at iter=3700, eval loss->0.4449 \n",
      "....................................................................................................iter 3800 (54.050 epoch): loss train=0.4425, val=0.4446, eval->0.4446\n",
      "==> Saving model at iter=3800, eval loss->0.4446 \n",
      "....................................................................................................iter 3900 (55.473 epoch): loss train=0.4421, val=0.4438, eval->0.4438\n",
      "==> Saving model at iter=3900, eval loss->0.4438 \n",
      "....................................................................................................iter 4000 (56.895 epoch): loss train=0.4420, val=0.4442, eval->0.4442\n",
      "87+9=96\n",
      "....................................................................................................iter 4100 (58.318 epoch): loss train=0.4408, val=0.4432, eval->0.4432\n",
      "==> Saving model at iter=4100, eval loss->0.4432 \n",
      "....................................................................................................iter 4200 (59.740 epoch): loss train=0.4411, val=0.4425, eval->0.4425\n",
      "==> Saving model at iter=4200, eval loss->0.4425 \n",
      "....................................................................................................iter 4300 (61.162 epoch): loss train=0.4414, val=0.4425, eval->0.4425\n",
      "....................................................................................................iter 4400 (62.585 epoch): loss train=0.4408, val=0.4418, eval->0.4418\n",
      "==> Saving model at iter=4400, eval loss->0.4418 \n",
      "....................................................................................................iter 4500 (64.007 epoch): loss train=0.4397, val=0.4417, eval->0.4417\n",
      "==> Saving model at iter=4500, eval loss->0.4417 \n",
      "36+7=43\n",
      "....................................................................................................iter 4600 (65.429 epoch): loss train=0.4402, val=0.4424, eval->0.4424\n",
      "....................................................................................................iter 4700 (66.852 epoch): loss train=0.4400, val=0.4412, eval->0.4412\n",
      "==> Saving model at iter=4700, eval loss->0.4412 \n",
      "....................................................................................................iter 4800 (68.274 epoch): loss train=0.4410, val=0.4422, eval->0.4422\n",
      "....................................................................................................iter 4900 (69.697 epoch): loss train=0.4391, val=0.4410, eval->0.4410\n",
      "==> Saving model at iter=4900, eval loss->0.4410 \n",
      "....................................................................................................iter 5000 (71.119 epoch): loss train=0.4397, val=0.4409, eval->0.4409\n",
      "==> Saving model at iter=5000, eval loss->0.4409 \n",
      "82+55=137\n",
      "....................................................................................................iter 5100 (72.541 epoch): loss train=0.4388, val=0.4401, eval->0.4401\n",
      "==> Saving model at iter=5100, eval loss->0.4401 \n",
      "....................................................................................................iter 5200 (73.964 epoch): loss train=0.4395, val=0.4410, eval->0.4410\n",
      "....................................................................................................iter 5300 (75.386 epoch): loss train=0.4393, val=0.4403, eval->0.4403\n",
      "....................................................................................................iter 5400 (76.809 epoch): loss train=0.4395, val=0.4412, eval->0.4412\n",
      "....................................................................................................iter 5500 (78.231 epoch): loss train=0.4390, val=0.4409, eval->0.4409\n",
      "2+1=3\n",
      "....................................................................................................iter 5600 (79.653 epoch): loss train=0.4383, val=0.4398, eval->0.4398\n",
      "==> Saving model at iter=5600, eval loss->0.4398 \n",
      "....................................................................................................iter 5700 (81.076 epoch): loss train=0.4400, val=0.4422, eval->0.4422\n",
      "....................................................................................................iter 5800 (82.498 epoch): loss train=0.4390, val=0.4399, eval->0.4399\n",
      "....................................................................................................iter 5900 (83.920 epoch): loss train=0.4378, val=0.4393, eval->0.4393\n",
      "==> Saving model at iter=5900, eval loss->0.4393 \n",
      "....................................................................................................iter 6000 (85.343 epoch): loss train=0.4381, val=0.4401, eval->0.4401\n",
      "\u00003+3=76\n",
      "....................................................................................................iter 6100 (86.765 epoch): loss train=0.4383, val=0.4403, eval->0.4403\n",
      "....................................................................................................iter 6200 (88.188 epoch): loss train=0.4384, val=0.4401, eval->0.4401\n",
      "....................................................................................................iter 6300 (89.610 epoch): loss train=0.4376, val=0.4393, eval->0.4393\n",
      "....................................................................................................iter 6400 (91.032 epoch): loss train=0.4378, val=0.4399, eval->0.4399\n",
      "....................................................................................................iter 6500 (92.455 epoch): loss train=0.4376, val=0.4390, eval->0.4390\n",
      "==> Saving model at iter=6500, eval loss->0.4390 \n",
      "66+9=75\n",
      "....................................................................................................iter 6600 (93.877 epoch): loss train=0.4380, val=0.4395, eval->0.4395\n",
      "....................................................................................................iter 6700 (95.299 epoch): loss train=0.4377, val=0.4398, eval->0.4398\n",
      "....................................................................................................iter 6800 (96.722 epoch): loss train=0.4380, val=0.4400, eval->0.4400\n",
      "....................................................................................................iter 6900 (98.144 epoch): loss train=0.4380, val=0.4400, eval->0.4400\n",
      "....................................................................................................iter 7000 (99.567 epoch): loss train=0.4382, val=0.4396, eval->0.4396\n",
      "96+15=111\n",
      "....................................................................................................iter 7100 (100.989 epoch): loss train=0.4382, val=0.4399, eval->0.4399\n",
      "....................................................................................................iter 7200 (102.411 epoch): loss train=0.4378, val=0.4391, eval->0.4391\n",
      "....................................................................................................iter 7300 (103.834 epoch): loss train=0.4380, val=0.4396, eval->0.4396\n",
      "....................................................................................................iter 7400 (105.256 epoch): loss train=0.4378, val=0.4396, eval->0.4396\n",
      "....................................................................................................iter 7500 (106.679 epoch): loss train=0.4378, val=0.4390, eval->0.4390\n",
      "15+43=58\n",
      "....................................................................................................iter 7600 (108.101 epoch): loss train=0.4378, val=0.4391, eval->0.4391\n",
      "....................................................................................................iter 7700 (109.523 epoch): loss train=0.4378, val=0.4393, eval->0.4393\n",
      "....................................................................................................iter 7800 (110.946 epoch): loss train=0.4372, val=0.4385, eval->0.4385\n",
      "==> Saving model at iter=7800, eval loss->0.4385 \n",
      "....................................................................................................iter 7900 (112.368 epoch): loss train=0.4377, val=0.4393, eval->0.4393\n",
      "....................................................................................................iter 8000 (113.790 epoch): loss train=0.4376, val=0.4389, eval->0.4389\n",
      "+4+4=48\n",
      "....................................................................................................iter 8100 (115.213 epoch): loss train=0.4378, val=0.4399, eval->0.4399\n",
      "....................................................................................................iter 8200 (116.635 epoch): loss train=0.4372, val=0.4389, eval->0.4389\n",
      "....................................................................................................iter 8300 (118.058 epoch): loss train=0.4368, val=0.4383, eval->0.4383\n",
      "==> Saving model at iter=8300, eval loss->0.4383 \n",
      "....................................................................................................iter 8400 (119.480 epoch): loss train=0.4372, val=0.4386, eval->0.4386\n",
      "....................................................................................................iter 8500 (120.902 epoch): loss train=0.4375, val=0.4390, eval->0.4390\n",
      "63+67=130\n",
      "....................................................................................................iter 8600 (122.325 epoch): loss train=0.4377, val=0.4385, eval->0.4385\n",
      "....................................................................................................iter 8700 (123.747 epoch): loss train=0.4375, val=0.4394, eval->0.4394\n",
      "....................................................................................................iter 8800 (125.169 epoch): loss train=0.4374, val=0.4395, eval->0.4395\n",
      "....................................................................................................iter 8900 (126.592 epoch): loss train=0.4371, val=0.4386, eval->0.4386\n",
      "....................................................................................................iter 9000 (128.014 epoch): loss train=0.4371, val=0.4390, eval->0.4390\n",
      "3+4=7\n",
      "....................................................................................................iter 9100 (129.437 epoch): loss train=0.4372, val=0.4387, eval->0.4387\n",
      "....................................................................................................iter 9200 (130.859 epoch): loss train=0.4374, val=0.4396, eval->0.4396\n",
      "....................................................................................................iter 9300 (132.281 epoch): loss train=0.4369, val=0.4381, eval->0.4381\n",
      "==> Saving model at iter=9300, eval loss->0.4381 \n",
      "....................................................................................................iter 9400 (133.704 epoch): loss train=0.4369, val=0.4387, eval->0.4387\n",
      "....................................................................................................iter 9500 (135.126 epoch): loss train=0.4369, val=0.4389, eval->0.4389\n",
      "92+51=143\n",
      "....................................................................................................iter 9600 (136.549 epoch): loss train=0.4372, val=0.4390, eval->0.4390\n",
      "....................................................................................................iter 9700 (137.971 epoch): loss train=0.4374, val=0.4387, eval->0.4387\n",
      "....................................................................................................iter 9800 (139.393 epoch): loss train=0.4368, val=0.4392, eval->0.4392\n",
      "....................................................................................................iter 9900 (140.816 epoch): loss train=0.4373, val=0.4391, eval->0.4391\n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "# Let's train for 10000 batch iterations. \n",
    "# Each dot means a batch was trained.\n",
    "# Train and validation losses are evaluated each 100 iterations (iters). \n",
    "# Also each 500 iters a random sample is taken.\n",
    "ben.train(iter_count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05df85bf-d0e9-46b1-a0f7-0ef966c0fe5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 1279872,\n",
       " 'train_loss': 0.43727046251296997,\n",
       " 'val_loss': 0.43912574648857117,\n",
       " 'eval_loss': 0.43912574648857117}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The current state loss info:\n",
    "ben.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef787745-d60e-49aa-9a62-f3d4e6d3492f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_samples': 1190400,\n",
       " 'train_loss': 0.4368865191936493,\n",
       " 'val_loss': 0.43814781308174133,\n",
       " 'eval_loss': 0.43814781308174133}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The last saved checkpoint info - the best performing model we got. Both train and val losses are thus lower than above.\n",
    "ben.last_saved_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c12a9f7-d760-4cf2-8f16-343d57532d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ./models/add2_shuffled/\n",
      "Checkpoint: iter=9300 (132.267 epoch), loss train=0.4369 val=0.4381 eval->0.4381\n",
      "Dataset train_path: ../data/add2.txt, val_path: None, train_split: 0.9, vocab_size: 13\n",
      "Model params: 0.59M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_samples': 1190400,\n",
       " 'train_loss': 0.4368865191936493,\n",
       " 'val_loss': 0.43814781308174133,\n",
       " 'eval_loss': 0.43814781308174133}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last saved checkpoint has a bit lower validation loss: let's load it\n",
    "ben.load()\n",
    "ben.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b68280ce-472d-4ad0-9dde-c919c175bede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1=2\n",
      "34+7=41\n",
      "78+99=177\n"
     ]
    }
   ],
   "source": [
    "# take a few samples:\n",
    "ben.sample('1+1=')\n",
    "ben.sample('34+7=')\n",
    "ben.sample('78+99=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bef4335d-bda5-40ba-b06a-5d0724bfdd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['96+30=', '91+85=', '75+11=']\n",
      "['126', '176', '86']\n"
     ]
    }
   ],
   "source": [
    "# Much better now - all are correct\n",
    "# Let's measure the accuracy of training dataset - this should be mostly memorization, as the model trained on these data\n",
    "train_ds = ben.train_dataset\n",
    "\n",
    "#split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=train_ds.sample_split(0, len(train_ds), sep='=', sep_included=-1)\n",
    "\n",
    "print(q[:3])\n",
    "print(a[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14d0f26d-652f-4928-9aac-20a07433d3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure the accuracy - how good was the memorization? This may take a while and give different results than the number below\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0368e0cc-8dbd-498a-bca2-8196ce996960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31+19=', '80+54=', '96+68=']\n",
      "['50', '134', '164']\n"
     ]
    }
   ],
   "source": [
    "# Perfect accuracy!\n",
    "# What about the accuracy of the validation dataset, on which the model never trained?\n",
    "val_ds = ben.val_dataset\n",
    "\n",
    "#split each aa+bb=cc into a prompt: 'aa+bb=' and an answer 'cc'\n",
    "q,a=val_ds.sample_split(0, len(val_ds), sep='=', sep_included=-1)\n",
    "\n",
    "print(q[:3])\n",
    "print(a[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dded223d-05ba-4771-a85f-ea2a7214c14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation dataset has sums starting in 90+..99+..., for example 90+2=92.\n",
    "# The model did however see the reversed addition of 90.100 numbers, for example 2+90=92.\n",
    "# Did it somehow learn the commutative property of addition?\n",
    "ben.measure_accuracy(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caa92eda-8433-4013-8713-1c00cbe5148b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101+120=\n"
     ]
    }
   ],
   "source": [
    "# Also perfect acuracy - it's generalizing!\n",
    "# What about three digit sums?\n",
    "ben.sample('101+120=')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbb6db8-6dee-4300-8a97-8667ee402368",
   "metadata": {},
   "source": [
    "Nothing come sout of it for three digits. Perhaps a new project: three digits addition?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
