{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94132259-86dc-464d-9ffc-15b9b688d271",
   "metadata": {},
   "source": [
    "[Winogrand](https://leaderboard.allenai.org/winogrande/submissions/get-started) is a test to measure a model's common sense reasoning capabilities. It's a series of prompts like this one:\n",
    "\n",
    "\"She remembered how annoying it is to dust her wood chair so she bought a plastic table instead.  Cleaning the _ is time consuming.\"\n",
    "\n",
    "Where two options are given to substitude the _ in the text:\n",
    "\n",
    "- 1: \"chair\" <- correct\n",
    "- 2: \"table\"\n",
    "\n",
    "The [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) uses a similar dataset (Winograd) for measuring performance, and they do it by looking into the token probabilities of the phrase in the two alternatives (the options), picking the higher one. (Also see the ../misc/next_token_probs notebook).\n",
    "\n",
    "Let's investigate how this could be done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa25cc07-fb63-4b06-bb21-e1f4d755a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gptbench import Sample, empty_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a27a922e-e4de-4464-b36d-97b64dfa6baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from gpt2-xl\n",
      "Dataset: dummy 0 tokens\n",
      "Dataset: loading uint16 tokens\n",
      "Expanding initial dataset size of 1 (less than block_size+1) by 1025 times to size of 1025\n",
      "Dataset train_path: dummy empty dataset, val_path: None, train_split: 0.9, vocab_size: 50257\n",
      "Model params: 1557.61M\n"
     ]
    }
   ],
   "source": [
    "ben = Sample(seed=0xDECADEF00D)\n",
    "\n",
    "cfg = empty_config()\n",
    "cfg.model.set(dtype='bfloat16') # halve the memory requirements\n",
    "\n",
    "ben.init_pretrained('gpt2-xl', cfg) # 'gpt2' or 'gpt-xl' if your GPU can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d378698e-af61-49bb-9307-f9939c07dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's ingest the extra-small test set \n",
    "\n",
    "with open('../data/winogrande/train_xs.jsonl', 'r', encoding='utf-8') as f:\n",
    "    json_lines = list(f)\n",
    "\n",
    "entries = []\n",
    "\n",
    "for json_str in json_lines:\n",
    "    entry = json.loads(json_str)\n",
    "    \"\"\" Each entry is in the form:\n",
    "{\"qID\": \"3D5G8J4N5CI2K40F4RZLF9OG2CKVTH-2\", \n",
    "\"sentence\": \"Kyle doesn't wear leg warmers to bed, while Logan almost always does. _ is more likely to live in a colder climate.\", \n",
    "\"option1\": \"Kyle\", \"option2\": \"Logan\", \"answer\": \"2\"}\n",
    "    \"\"\"\n",
    "    entries.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ad9241-27de-4550-b3dd-83447b8ac607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'qID': '3QHITW7OYO7Q6B6ISU2UMJB84ZLAQE-2',\n",
       "  'sentence': \"Ian volunteered to eat Dennis's menudo after already having a bowl because _ despised eating intestine.\",\n",
       "  'option1': 'Ian',\n",
       "  'option2': 'Dennis',\n",
       "  'answer': '2'},\n",
       " {'qID': '3QHITW7OYO7Q6B6ISU2UMJB84ZLAQE-1',\n",
       "  'sentence': \"Ian volunteered to eat Dennis's menudo after already having a bowl because _ enjoyed eating intestine.\",\n",
       "  'option1': 'Ian',\n",
       "  'option2': 'Dennis',\n",
       "  'answer': '1'},\n",
       " {'qID': '3XWUWJ18TLO2DDRXF83QWLKRJ29UU4-1',\n",
       "  'sentence': 'He never comes to my home, but I always go to his house because the _ is smaller.',\n",
       "  'option1': 'home',\n",
       "  'option2': 'house',\n",
       "  'answer': '1'},\n",
       " {'qID': '3XWUWJ18TLO2DDRXF83QWLKRJ29UU4-2',\n",
       "  'sentence': 'He never comes to my home, but I always go to his house because the _ is bigger.',\n",
       "  'option1': 'home',\n",
       "  'option2': 'house',\n",
       "  'answer': '2'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e43a9b4-2184-4154-ba5f-f951c05fa922",
   "metadata": {},
   "source": [
    "Our first try will just use the probability of the option token used to replace _ in the sentence. It won't use the text following the _ character. To keep thinks simple for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b156fc5-69ef-4556-8b0d-2ebeb475b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_next1(sentence, op1, op2, answer, prob_calc, log=False):\n",
    "    \"\"\"\n",
    "    Decide by comparing probabilities of generating the two options, when generating right before the _ character in the sentence\n",
    "    We'll prepend a space to the options and remove the space before _ in the sentence if any.\n",
    "    When an option is encoded to several tokens, we'll use the 'mean' or 'mult' the probabilities. (prob_calc_param)\n",
    "    Returns True if it found the correct answer.\n",
    "    \"\"\"\n",
    "\n",
    "    index = sentence.index(' _')\n",
    "    if index < 0:\n",
    "        index = sentence.index('_')\n",
    "    sent = sentence[:index]\n",
    "    sent_tok = ben.train_dataset.encode(sent)\n",
    "\n",
    "    op1 = ' ' + op1\n",
    "    op2 = ' ' + op2\n",
    "    op1_tok = ben.train_dataset.encode(op1)\n",
    "    op2_tok = ben.train_dataset.encode(op2)\n",
    "    if log: print(f\"op1/op2 tokens: '{op1}'={op1_tok}, '{op2}'={op2_tok}\")\n",
    "\n",
    "    p_op1 = ben.model_next_probs(text_tokens=sent_tok, next_text_tokens=op1_tok)\n",
    "    p_op2 = ben.model_next_probs(text_tokens=sent_tok, next_text_tokens=op2_tok)\n",
    "\n",
    "    if log: print(\"op1/op2 probs:\", p_op1, p_op2)\n",
    "\n",
    "    # calc probabilities\n",
    "    if prob_calc == 'mean':\n",
    "        p_op1 = sum(p_op1) / len(p_op1)\n",
    "        p_op2 = sum(p_op2) / len(p_op2)\n",
    "        \n",
    "    elif prob_calc == 'mult':\n",
    "        p1 = 1.\n",
    "        for p in p_op1:\n",
    "            p1*=p\n",
    "        p_op1=p1\n",
    "        p2 = 1.\n",
    "        for p in p_op2:\n",
    "            p2*=p\n",
    "        p_op2=p2\n",
    "    else:\n",
    "        assert False, \"Unknown prob_calc\"\n",
    "\n",
    "    if log: print(\"op1/op2 choose p:\", p_op1, p_op2)\n",
    "   \n",
    "    if answer == 1:\n",
    "        return p_op1 > p_op2\n",
    "    else:\n",
    "        return p_op2 > p_op1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c650e132-234a-4ed2-a449-3fc4616bdeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op1/op2 tokens: ' blue'=[4171], ' yellow-pink'=[7872, 12, 79, 676]\n",
      "op1/op2 probs: [0.09228515625] [0.00066375732421875, 0.003448486328125, 0.00115966796875, 0.96484375]\n",
      "op1/op2 choose p: 0.09228515625 0.24252891540527344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_next1('The sky is _', 'blue', 'yellow-pink', 1, 'mean', log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee3fc0b-bde3-48a0-8d28-ca8c611c88d9",
   "metadata": {},
   "source": [
    "Averaging falls prey to outliers: the 'ink' final token of option 2 ('yellow-pink') has 96% probability, which (wrongly) kills the 9% probability of option 1.\n",
    "\n",
    "A solution could be to use an outlier-resistant metric like the median.\n",
    "\n",
    "But since the chain rule discrete variables states that:\n",
    "\n",
    "P(C B A) = P(C | B A)  * P(B | A)\n",
    "\n",
    "We can multiply the probabilities for all tokens in the option, however this will penalize options with more tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce5a7007-8b65-4087-9141-a209436b3c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op1/op2 tokens: ' blue'=[4171], ' yellow-pink'=[7872, 12, 79, 676]\n",
      "op1/op2 probs: [0.09228515625] [0.00066375732421875, 0.003448486328125, 0.00115966796875, 0.96484375]\n",
      "op1/op2 choose p: 0.09228515625 2.5611114895518483e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_next1('The sky is _', 'blue', 'yellow-pink', 1, 'mult', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9484d4ad-3e8f-4317-9ea7-696339ae8f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op1/op2 tokens: ' chair'=[5118], ' table'=[3084]\n",
      "op1/op2 probs: [0.04150390625] [0.326171875]\n",
      "op1/op2 choose p: 0.04150390625 0.326171875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_next1(\"She remembered how annoying it is to dust her wood chair so she bought a plastic table instead. Cleaning the _ is time consuming.\",\n",
    "             \"chair\", \"table\", 1, 'mult', log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35365a6d-9731-4c10-b246-4c51eea726f8",
   "metadata": {},
   "source": [
    "Another problem is that we're just looking at the probabilities of generating the option tokens, any following text is ignored. In the previous \"time consuming\" is ignored, and that's where the meaning is.\n",
    "\n",
    "The choose_next1() method of just looking into the probabilities of the options doesn't look very reliable.\n",
    "\n",
    "Let's write a function to calc accuracy along all the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4510bb7-6ab9-42ea-866a-af4e88807738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to test accuracy\n",
    "def calc_accuracy(entries, choose_fn):\n",
    "    \"\"\"\n",
    "    choose_fn(sentence, option1, option2, answer)\n",
    "    \"\"\"\n",
    "\n",
    "    correct_count = 0\n",
    "    for e in entries:\n",
    "        res = choose_fn(e['sentence'], e['option1'], e['option2'], int(e['answer']))\n",
    "        correct_count += int(res)\n",
    "        print('.', end='')\n",
    "\n",
    "    return correct_count / len(entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc30329c-3506-4a0e-b4b6-9b202082827e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................................................................"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49375"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_next1_fn = lambda sentence, option1, option2, answer: choose_next1(sentence, option1, option2, answer, 'mean')\n",
    "\n",
    "calc_accuracy(entries, choose_next1_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47086b90-71a5-4554-b13f-51c949091895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................................................................"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4875"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_next1_fn = lambda sentence, option1, option2, answer: choose_next1(sentence, option1, option2, answer, 'mult')\n",
    "\n",
    "calc_accuracy(entries, choose_next1_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1533f157-5328-4efe-abee-5e32b942a5dd",
   "metadata": {},
   "source": [
    "Accuracy with the the mean and mult ways of counting multiple token probabilities is near 50%: the same as flipping a coin.\n",
    "\n",
    "Let's look into other ways.\n",
    "\n",
    "The paper \"[A Simple Method for Commonsense Reasoning](https://arxiv.org/pdf/1806.02847.pdf)\" from Trieu H. Trinh and Quoc V. Le introduces the idea of calculating the partial probability after the option tokens are generated.\n",
    "\n",
    "In the above example, we'd calculate for each option - say for option 1, 'chair':\n",
    "```\n",
    "P(\" chair is time consuming.\" | \"She remembered how annoying it is to dust her wood chair so she bought a plastic table instead. Cleaning the\")\n",
    "```\n",
    "\n",
    "The conditional probability of ending the phrase with option 1, given the text generated before option 1.\n",
    "\n",
    "And then also calc the probabilities for 'table' and pick the higher one as true.\n",
    "\n",
    "Let's write a function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "234095a9-f51f-431a-9a84-c1ddde29a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_partial(sentence, op1, op2, answer, prob_calc, log=False):\n",
    "    \"\"\"\n",
    "    Returns True if it found the correct answer.\n",
    "    \"\"\"\n",
    "\n",
    "    index = sentence.index('_')\n",
    "    sent_pre = sentence[:index]\n",
    "    \n",
    "    sent_post = sentence[index+1:]\n",
    "    sent_post_tok = ben.train_dataset.encode(sent_post)\n",
    "\n",
    "    op1 = sent_pre + op1\n",
    "    op2 = sent_pre + op2\n",
    "    op1_tok = ben.train_dataset.encode(op1)\n",
    "    op2_tok = ben.train_dataset.encode(op2)\n",
    "    if log: print(f\"op1/op2 tokens: '{op1}'={op1_tok}, '{op2}'={op2_tok}\\npost tokens='{sent_post}'={sent_post_tok}\")\n",
    "\n",
    "    p_op1 = ben.model_next_probs(text_tokens=op1_tok, next_text_tokens=sent_post_tok)\n",
    "    p_op2 = ben.model_next_probs(text_tokens=op2_tok, next_text_tokens=sent_post_tok)\n",
    "\n",
    "    if log: print(\"op1/op2 probs:\", p_op1, p_op2)\n",
    "\n",
    "    # calc probabilities\n",
    "    if prob_calc == 'mean':\n",
    "        p_op1 = sum(p_op1) / len(p_op1)\n",
    "        p_op2 = sum(p_op2) / len(p_op2)\n",
    "        \n",
    "    elif prob_calc == 'mult':\n",
    "        p1 = 1.\n",
    "        for p in p_op1:\n",
    "            p1*=p\n",
    "        p_op1=p1\n",
    "        p2 = 1.\n",
    "        for p in p_op2:\n",
    "            p2*=p\n",
    "        p_op2=p2\n",
    "    else:\n",
    "        assert False, \"Unknown prob_calc\"\n",
    "\n",
    "    if log: print(\"op1/op2 choose p:\", p_op1, p_op2)\n",
    "        \n",
    "    if answer == 1:\n",
    "        return p_op1 > p_op2\n",
    "    else:\n",
    "        return p_op2 > p_op1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9123366-45ad-4cfc-9ce8-ab67c8b822c6",
   "metadata": {},
   "source": [
    "How does it work?\n",
    "\n",
    "Let's just use the 'mult' probabilities aggregation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2c5ca33-3090-4f89-abf7-b93a11880f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op1/op2 tokens: 'She remembered how annoying it is to dust her wood chair so she bought a plastic table instead. Cleaning the chair'=[3347, 12086, 703, 15774, 340, 318, 284, 8977, 607, 4898, 5118, 523, 673, 5839, 257, 7309, 3084, 2427, 13, 5985, 278, 262, 5118], 'She remembered how annoying it is to dust her wood chair so she bought a plastic table instead. Cleaning the table'=[3347, 12086, 703, 15774, 340, 318, 284, 8977, 607, 4898, 5118, 523, 673, 5839, 257, 7309, 3084, 2427, 13, 5985, 278, 262, 3084]\n",
      "post=' is time consuming.'=[318, 640, 18587, 13]\n",
      "op1/op2 probs: [0.1923828125, 0.005615234375, 0.625, 0.099609375] [0.146484375, 0.005279541015625, 0.640625, 0.091796875]\n",
      "op1/op2 choose p: 6.725342245772481e-05 4.547987373371143e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_partial(\"She remembered how annoying it is to dust her wood chair so she bought a plastic table instead. Cleaning the _ is time consuming.\",\n",
    "             \"chair\", \"table\", 1, 'mult', log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05cf4180-31ab-43c6-92af-0ff040f79e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op1/op2 tokens: 'The sky is blue'=[464, 6766, 318, 4171], 'The sky is yellow-pink'=[464, 6766, 318, 7872, 12, 79, 676]\n",
      "post tokens='.'=[13]\n",
      "op1/op2 probs: [0.171875] [0.1484375]\n",
      "op1/op2 choose p: 0.171875 0.1484375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_partial('The sky is _.', 'blue', 'yellow-pink', 1, 'mult', log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656976e5-2ade-410e-9ab5-d1d45069c71c",
   "metadata": {},
   "source": [
    "Calculate accuracy for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d21d222-f323-4e2c-bac3-610789061f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................................................................................................"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_fn = lambda sentence, option1, option2, answer: choose_partial(sentence, option1, option2, answer, 'mult')\n",
    "\n",
    "calc_accuracy(entries, choose_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4314b9-b95a-4c57-b88c-dffd87efa140",
   "metadata": {},
   "source": [
    "About 62% - that's an improvement over coin flipping! : )\n",
    "\n",
    "The [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) mentions 70%, on the gpt2-xl model with the partial scoring method we use, but for the Winograd Schema test (we're using the Winogrande, a later test).\n",
    "\n",
    "From here we could test with the larger Winogrande tests (we're using the extra-small test with 160 entries). Try the small test (../data/winogrande/train_s.jsonl) which has 640 entries or larger ones.\n",
    "\n",
    "This could also work well with multiple choice questions: do a partial generate (as above) of the n choices and choose the one with higher probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
