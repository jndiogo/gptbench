{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02eb88ca-f065-4ac0-88f9-5a9a2e354457",
   "metadata": {},
   "source": [
    "This notebook studies the largest batch size that can be used to sample and train from a GPT-2 model, by using the estimate_max_batch_size() function.\n",
    "\n",
    "For an NVIDIA RTX 3060 with 12GB, using model.dtype='bfloat16' (2 bytes per parameter), these batch sizes were estimated:\n",
    "|Model (Params)|Sample|Train|\n",
    "|------|------|------|\n",
    "|gpt2 (124M)|70|16-10|\n",
    "|gpt2-medium (350M)|70|6-4|\n",
    "|gpt2-large (774M)|50|3-1|\n",
    "|gpt2-xl (1558M)|50|0| |\n",
    "\n",
    "Train column values are separated by a dash: shared GPU memory - dedicated GPU memory (faster)\n",
    "\n",
    "GPTBench includes gradient accumulation, so the values obtained here can be used in the model.accum_size config setting, while the actual model.batch_size can be greater.\n",
    "Also, since build 0.2 the GPT model class supports Flash Attention which decreases memory needs.\n",
    "\n",
    "The memory-hungry AdamW optimizer was used for the numbers above, instead using an SGD optimizer would allow larger batch sizes.\n",
    "\n",
    "This process is quite messy, as sometimes the Jupyter notebook will lock the allocated memory and only restarting the kernel will clean it.\n",
    "\n",
    "It also hapenned that a batch size value that used to work before, sometimes gives an out of memory error. One needs to remember that the GPU is shared with other software, so values near full memory occupation might not work at all times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a03bb40-3f56-4a04-877f-5d3269db82dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from gptbench import Train, GPT, Conf, empty_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587cc8f4-4b3c-431c-b27e-f4a98ff381f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=auto\n",
      "dtype=bfloat16\n",
      "n_layer=12\n",
      "n_head=12\n",
      "n_embd=768\n",
      "vocab_size=50257\n",
      "block_size=1024\n",
      "dropout=0.1\n",
      "flash_attn=True\n"
     ]
    }
   ],
   "source": [
    "model_type = 'gpt2' # 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'\n",
    "\n",
    "dataset_path='../data/shakespeare.txt'\n",
    "\n",
    "# set model config settings\n",
    "model_config = GPT.get_config_from_type(model_type)\n",
    "model_config.dtype='bfloat16' # not 'float32'\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb850a1e-4349-40a0-8046-062b039ab41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model\n",
      "Trying batch_size 80... Out of memory\n",
      "Trying batch_size 75... Out of memory\n",
      "Trying batch_size 70... Fits\n",
      "Enough memory for batch_size 70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimate max batch_size for sampling\n",
    "batch_size = Train.estimate_max_batch_size(model_config, None, starting_size=80, delta_size=-5, times=2)\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdfca118-0f99-49d0-90b7-f7c43252e3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from gpt2\n",
      "Dataset: encoding utf-8 to tokens\n",
      "Dataset: loading uint16 tokens\n",
      "Dataset: loading uint16 tokens\n",
      "Dataset train_path: ../data/shakespeare.txt, val_path: None, train_split: 0.9, vocab_size: 50257\n",
      "Model params: 124.44M\n",
      "Sampling with batch_size=70\n",
      "Measured perplexity=69.01852867010916\n"
     ]
    }
   ],
   "source": [
    "# confirm that we can sample at this batch_size by calling the measure_perplexity()\n",
    "ben = Train(seed=0xb0ccacc10)\n",
    "# set config settings\n",
    "cfg = empty_config()\n",
    "\n",
    "cfg.dataset.set(class_name='gpt2', \n",
    "                train_path=dataset_path, \n",
    "                train_split=0.9)\n",
    "cfg.model = model_config\n",
    "\n",
    "ben.init_pretrained(model_type, cfg)\n",
    "\n",
    "print(f\"Sampling with batch_size={batch_size}\")\n",
    "ppl = ben.measure_perplexity(ben.val_dataset, stride=-1, max_batch_size=batch_size)\n",
    "print(f\"Measured perplexity={ppl}\")\n",
    "\n",
    "# clean up\n",
    "del ben\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcf58b8d-6ca9-4ab3-a89f-7488debb082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model\n",
      "Creating optimizer adamw\n",
      "Trying batch_size 10... Fits\n",
      "Enough memory for batch_size 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now estimate for training using AdamW optimizer\n",
    "batch_size = Train.estimate_max_batch_size(model_config, 'adamw', starting_size=10, delta_size=-2, times=2)\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cda7faa2-a281-4ffd-9978-171648f1fc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from gpt2\n",
      "Dataset: encoding utf-8 to tokens\n",
      "Dataset: loading uint16 tokens\n",
      "Dataset: loading uint16 tokens\n",
      "Dataset train_path: ../data/shakespeare.txt, val_path: None, train_split: 0.9, vocab_size: 50257\n",
      "Model params: 124.44M\n"
     ]
    }
   ],
   "source": [
    "# try testing with \n",
    "ben = Train(seed=0xb0ccacc10)\n",
    "\n",
    "# set config settings\n",
    "cfg = empty_config()\n",
    "\n",
    "cfg.dataset.set(class_name='gpt2', \n",
    "                train_path=dataset_path, \n",
    "                train_split=0.9)\n",
    "cfg.model = model_config\n",
    "cfg.trainer.batch_size=batch_size\n",
    "cfg.trainer.optimizer='adamw'\n",
    "\n",
    "ben.init_pretrained(model_type, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a827734-644d-4e3b-994a-15896210511e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      ".........\n",
      "Iter 10 loss=4.0000, iter_dt=6828.22ms\n",
      "..........\n",
      "Iter 20 loss=3.6875, iter_dt=5771.74ms\n",
      ".Marcus.\n",
      "\n",
      "RENEWAL: It's been quite long, Mrs. Holland.\n",
      "\n",
      "REID: Glad to hear. There are many compliments upon you all, though.\n",
      "\n",
      "REID: I have learnt a lot from Consome Gable, who was once perhaps your best horticulturist; yet he said to me that the principal causes of the dyspeptic are exercised mainly by laziness itself, and sometimes weakness, that is fumes, and put the fumes into\n"
     ]
    }
   ],
   "source": [
    "ben.train(iter_count=20)\n",
    "ben.sample(\"Marcus\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
