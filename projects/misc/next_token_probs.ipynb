{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6c85dc-7122-4c52-aa32-eab8edbd4301",
   "metadata": {},
   "source": [
    "Given a start text, let's list the next n most probable tokens (with their probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a515187-a9dc-4cc3-bf4d-857b59408c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from gptbench import Sample, empty_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46c0697-0122-4d1f-92e9-b6f2ac29cbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from gpt2\n",
      "Dataset: dummy 0 tokens\n",
      "Dataset: loading uint16 tokens\n",
      "Expanding initial dataset size of 1 (less than block_size+1) by 1025 times to size of 1025\n",
      "Dataset train_path: dummy empty dataset, val_path: None, train_split: 0.9, vocab_size: 50257\n",
      "Model params: 124.44M\n"
     ]
    }
   ],
   "source": [
    "ben = Sample(seed=0xcabc0ffee)\n",
    "\n",
    "# set config settings\n",
    "cfg = empty_config()\n",
    "cfg.model.set(dtype='bfloat16') # halve the memory requirements\n",
    "\n",
    "ben.init_pretrained('gpt2', cfg) # 'gpt2' or 'gpt-xl' if your GPU can handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa4d453-b229-49ba-a868-b596176864f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1865234375, ' the', 262),\n",
       " (0.11328125, ' blue', 4171),\n",
       " (0.06884765625, ' falling', 7463),\n",
       " (0.041748046875, ' full', 1336),\n",
       " (0.041748046875, ' a', 257),\n",
       " (0.0252685546875, ' not', 407),\n",
       " (0.0252685546875, ' clear', 1598),\n",
       " (0.0252685546875, ' dark', 3223),\n",
       " (0.01531982421875, ' black', 2042),\n",
       " (0.00927734375, ' so', 523)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the most probable tokens after 'The sky is'\n",
    "ben.model_next(10, text='The sky is')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cabda12-b371-4771-9223-1e06fad67227",
   "metadata": {},
   "source": [
    "Note that most words were tokenized to start with a space character, hence ' blue' in the returned values.\n",
    "\n",
    "Returned tuple means: probability, token_text, token_id\n",
    "\n",
    "Let's build a function to follow and list the top 3 probabilities along n steps/tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95f1e06b-6b42-438d-b8c5-7dbfe63a87d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Step 1\n",
      "'The sky is' -> ' the' (18.65%), ' blue' (11.33%), ' falling' (6.88%), \n",
      "============================== Step 2\n",
      "'The sky is the' -> ' limit' (96.48%), ' Limit' (0.39%), ' only' (0.39%), \n",
      "============================== Step 3\n",
      "'The sky is the limit' -> '.' (20.80%), ',' (12.60%), '!' (7.67%), \n",
      "============================== Step 4\n",
      "'The sky is the limit.' -> '\n",
      "' (22.56%), ' The' (5.03%), ' I' (3.05%), \n",
      "============================== Step 5\n",
      "'The sky is the limit.\n",
      "' -> '\n",
      "' (98.83%), 'The' (0.15%), 'I' (0.05%), \n"
     ]
    }
   ],
   "source": [
    "def follow(text, steps):\n",
    "    for s in range(steps):\n",
    "        gen = ben.model_next(3, text=text)\n",
    "        print(f\"============================== Step {s+1}\")\n",
    "        print(f\"'{text}' -> \", end='')\n",
    "        for o in (gen):\n",
    "            print(f\"'{o[1]}' ({o[0]*100:.2f}%), \", end='')\n",
    "        print()\n",
    "\n",
    "        text += gen[0][1]\n",
    "\n",
    "follow('The sky is', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605464dc-003d-4bbc-9d7e-afe808e8bec7",
   "metadata": {},
   "source": [
    "Using the probability of the next token we can score which one from a number of options has the highest probability. For example:\n",
    "\n",
    "'This sky is' ->\n",
    "\n",
    "- ' blue'\n",
    "\n",
    "- ' yellow'\n",
    "\n",
    "By checking which token as the highest probability, we can choose the winner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3158a12-864a-4d99-8147-39bae4a9a178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The sky is'\n",
      "- ' blue': prob=11.3%\n",
      "- ' yellow': prob=0.1%\n",
      "Best choice: ' blue'\n",
      "--> The sky is blue\n"
     ]
    }
   ],
   "source": [
    "def choose(text, options):\n",
    "    print(f\"Prompt: '{text}'\")\n",
    "    tokens = []\n",
    "    for o in options:\n",
    "        enc = ben.train_dataset.encode(o)\n",
    "        assert len(enc) == 1, f\"Only single token options: '{o}' has {len(enc)} tokens\"\n",
    "        tokens.append(enc[0])\n",
    "        \n",
    "    probs = ben.model_probs(text=text)\n",
    "\n",
    "    best_prob = 0\n",
    "    for i,t in enumerate(tokens):\n",
    "        prob = probs[t].item()\n",
    "        option_text = options[i]\n",
    "        print(f\"- '{option_text}': prob={prob*100:.1f}%\")\n",
    "        \n",
    "        if(prob > best_prob):\n",
    "            best_prob = prob\n",
    "            best_text = option_text\n",
    "\n",
    "    print(f\"Best choice: '{best_text}'\")\n",
    "    print('-->', text + best_text)\n",
    "\n",
    "text = 'The sky is'\n",
    "# note that we're prefixing with a space character:\n",
    "options = [' blue', ' yellow']\n",
    "\n",
    "choose(text, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f35f1cb-65e8-42eb-8062-298710403cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'The capital of Portugal is'\n",
      "- ' Lisbon': prob=9.0%\n",
      "- ' Cuba': prob=0.0%\n",
      "- ' Paris': prob=0.1%\n",
      "- ' white': prob=0.0%\n",
      "Best choice: ' Lisbon'\n",
      "--> The capital of Portugal is Lisbon\n"
     ]
    }
   ],
   "source": [
    "text = 'The capital of Portugal is'\n",
    "# note that we're prefixing with a space character:\n",
    "options = [' Lisbon', ' Cuba', ' Paris', ' white']\n",
    "\n",
    "choose(text, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df9e9c4d-9714-4aed-b467-54115c1c1dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '1+1='\n",
      "- '2': prob=3.4%\n",
      "- '3': prob=2.1%\n",
      "- '7': prob=0.5%\n",
      "Best choice: '2'\n",
      "--> 1+1=2\n"
     ]
    }
   ],
   "source": [
    "text = '1+1='\n",
    "# note that we're prefixing with a space character:\n",
    "options = ['2', '3', '7']\n",
    "\n",
    "choose(text, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e3e84-8951-4ace-92c7-eda3eb83bd77",
   "metadata": {},
   "source": [
    "What if options encode into multiple tokens? In this case we could either use the mean of all tokens for the option or multiply sucessive generated token probabilities.\n",
    "\n",
    "See the ../prompting/winogrande notebook for an example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
