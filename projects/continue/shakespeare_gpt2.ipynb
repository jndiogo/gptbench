{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4637d54f-2278-492c-bb2d-7277fc1aee63",
   "metadata": {},
   "source": [
    "Let's train the gpt2 model on Shakespeare's plays.\n",
    "\n",
    "Because the model is already large (124M params), we'll set the batch_size to 1 and change the datatype to bfloat16.\n",
    "\n",
    "Trained on 167k lines of Shakespeare plays. Only contains plays, no sonnets or other forms. I tried but could not locate the source of this file and believe it's not copywrited.\n",
    "\n",
    "We won't have a validation dataset here, to maximize training samples, to catch the overall \"style\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98692b76-fe53-4b5a-a27f-71e8436b4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Train, empty_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b7ef1f3-de29-4e45-a589-e06075a8fae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from gpt2\n",
      "Dataset: dummy 0 tokens\n",
      "Dataset: loading uint16 tokens\n",
      "Expanding initial dataset size of 1 (less than block_size+1) by 1025 times to size of 1025\n",
      "Dataset train_path: dummy empty dataset, val_path: None, train_split: 0.9, vocab_size: 50257\n",
      "Model params: 124.44M\n"
     ]
    }
   ],
   "source": [
    "ben = Train('shakespeare-gpt2')\n",
    "\n",
    "cfg = empty_config()\n",
    "cfg.model.dtype='bfloat16'\n",
    "cfg.trainer.batch_size=1\n",
    "\n",
    "ben.init_pretrained('gpt2', cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d93421a-3b54-4433-b197-87fcd5f0498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: encoding utf-8 to tokens\n",
      "Dataset: loading uint16 tokens\n",
      "Training\n",
      ".Iter 1 (0.000 epoch): loss train=3.7352, val=inf, eval->3.7352\n",
      "==> Saving model at iter=1, eval loss->3.7352 \n",
      "...................................................................................................\n",
      "Iter 100 (0.000 epoch): loss train=3.5475, val=inf, eval->3.5475\n",
      "==> Saving model at iter=100, eval loss->3.5475 \n",
      "....................................................................................................\n",
      "Iter 200 (0.000 epoch): loss train=3.5611, val=inf, eval->3.5611\n",
      "....................................................................................................\n",
      "Iter 300 (0.000 epoch): loss train=3.4545, val=inf, eval->3.4545\n",
      "==> Saving model at iter=300, eval loss->3.4545 \n",
      "....................................................................................................\n",
      "Iter 400 (0.000 epoch): loss train=3.4828, val=inf, eval->3.4828\n",
      "....................................................................................................\n",
      "Iter 500 (0.000 epoch): loss train=3.5144, val=inf, eval->3.5144\n",
      "Sampling:  California science might have done for by the precedent established in our minds: in concluding, which of our own will it will, this is an excellent point toward a new deciding letter. For you may therefore observe, that my chief of the bill will require us, though not present, obtained a good chief.\n",
      "\n",
      "Question:\n",
      "52.\n",
      "\n",
      "DEAR GERRY:\n",
      "They will find me in a position soon expired, and be deprived of two commissions.\n",
      "\n",
      "Conspiracy:\n",
      "In the\n",
      "....................................................................................................\n",
      "Iter 600 (0.000 epoch): loss train=3.4823, val=inf, eval->3.4823\n",
      "....................................................................................................\n",
      "Iter 700 (0.001 epoch): loss train=3.4436, val=inf, eval->3.4436\n",
      "==> Saving model at iter=700, eval loss->3.4436 \n",
      "....................................................................................................\n",
      "Iter 800 (0.001 epoch): loss train=3.4269, val=inf, eval->3.4269\n",
      "==> Saving model at iter=800, eval loss->3.4269 \n",
      "....................................................................................................\n",
      "Iter 900 (0.001 epoch): loss train=3.4453, val=inf, eval->3.4453\n",
      "....................................................................................................\n",
      "Iter 1000 (0.001 epoch): loss train=3.3692, val=inf, eval->3.3692\n",
      "==> Saving model at iter=1000, eval loss->3.3692 \n",
      "Sampling:  premises:\n",
      "'Why Englishmen colour your breeches?'\n",
      "Twixt the enemy's spiced brow and firm lips: And, bridle of the painter's terms;\n",
      "'By gods hath his lords been chosen Few swans;\n",
      "And foreign from accustomed joys may see of him. Pellio,\n",
      "Three chiefs come up and hold him, tormenting him ever:\n",
      "His unswallowed wish is for me cut:\n",
      "Is it not equally good for him to be set on\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# it loaded using a dummy dataset, because we didn't provide one  - let's set the shakespeare.txt data:\n",
    "# train_split=1 means no validation dataset, to maximize training data\n",
    "ben.set_datasets(class_name='gpt2', # GPT2TokensDataset class\n",
    "                 train_path='../data/shakespeare.txt', \n",
    "                 train_split=1.) \n",
    "\n",
    "# set training log periods to avoid cluttering the training output\n",
    "ben.set_train_log_periods(sample_period=500, dot_period=1, loss_period=0)\n",
    "\n",
    "# and train for 1000 iters\n",
    "ben.train(iter_count=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f509b5f1-cee1-445b-ab2a-20d32ad78a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So it goes then; I have spoken to my mother you know better. And so though it be lost in the sky, it is lost in the clouds, and in the water;\n",
      "And where is it? You must know me as well me as I do see thee. I think, see myself, know the man; for God willing,\n",
      "You friend what man hath sent thy head abroad. Why enough! there I am concerned,\n",
      "Or, wasted at any one's wanting, hath nothing passed in\n"
     ]
    }
   ],
   "source": [
    "ben.sample(\"So it goes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87a1dc9-a4e9-4a3f-9cbb-45104e21874d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bermuda (14d)\n",
      "\n",
      "From Hannover\n",
      "\n",
      "I love thee;\n",
      "These are the letters of love yours and mine.\n",
      "\n",
      "FN:\n",
      "Lord waterfront\n",
      "My lord, you wish me three garlands too close\n",
      "to Poles\n",
      "between yourselves?\n",
      "\n",
      "OMACADAN:\n",
      "Under a glasse\n",
      "To suffer careful Welsh seamen to draw hither,\n",
      "Give princes to bear their quarrels duly;\n",
      "And for every cunning Welshman to scout our king's areas,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ben.sample(\"Bermuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
