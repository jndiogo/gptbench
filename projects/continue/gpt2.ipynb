{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4637d54f-2278-492c-bb2d-7277fc1aee63",
   "metadata": {},
   "source": [
    "Let's load a pretrained GPT2 model and sample from it. \n",
    "\n",
    "We'll use the smallest model named 'gpt2', with 124M parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98692b76-fe53-4b5a-a27f-71e8436b4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Train, empty_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b7ef1f3-de29-4e45-a589-e06075a8fae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from gpt2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: dummy 0 tokens\n",
      "Dataset: loading uint16 tokens\n",
      "Expanding initial dataset size of 1 (less than block_size+1) by 1025 times to size of 1025\n",
      "Dataset train_path: dummy empty dataset, val_path: None, train_split: 0.9, vocab_size: 50257\n",
      "Model params: 124.44M\n"
     ]
    }
   ],
   "source": [
    "ben = Train('gpt2', seed=0xb0ccacc10)\n",
    "\n",
    "# set config settings\n",
    "cfg = empty_config()\n",
    "cfg.dataset.class_name='gpt2' # set a dummy GPT2 tokens train dataset\n",
    "cfg.sample.set(top=40) # top=40 means top_k(40)\n",
    "\n",
    "ben.init_pretrained('gpt2', cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46514c75-d08d-4d00-979c-93749c8b4a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: -1\n",
      "sample: \n",
      "    max_len: 100\n",
      "    count: 1\n",
      "    start_text: None\n",
      "    start_text_sep: |\n",
      "    emit_start: True\n",
      "    emit_after: None\n",
      "    emit_before: None\n",
      "    flush: True\n",
      "    eot_stop: 0\n",
      "    top: 40.0\n",
      "    temp: 1.0\n",
      "    max_batch_size: None\n",
      "    multiline_prompt: False\n",
      "train: \n",
      "    eval_period: 100\n",
      "    eval_type: 1.0\n",
      "    eval_iters: 100\n",
      "    eval_save_checkpt: 1\n",
      "    eval_save_loss: csv,tensorboard\n",
      "dataset: \n",
      "    class_name: gpt2\n",
      "    train_path: None\n",
      "    train_split: 0.9\n",
      "    val_path: None\n",
      "    params: None\n",
      "model: \n",
      "    device: auto\n",
      "    dtype: float32\n",
      "    n_layer: 12\n",
      "    n_head: 12\n",
      "    n_embd: 768\n",
      "    vocab_size: 50257\n",
      "    block_size: 1024\n",
      "    dropout: 0.1\n",
      "trainer: \n",
      "    n_workers: 0\n",
      "    batch_size: 32\n",
      "    max_samples: None\n",
      "    grad_norm_clip: 1.0\n",
      "    optimizer: adamw\n",
      "    learning_rate: 0.0001\n",
      "    adamw_beta1: 0.9\n",
      "    adamw_beta2: 0.95\n",
      "    adamw_weight_decay: 0.1\n"
     ]
    }
   ],
   "source": [
    "# the config settings that we passed were used to update/override the ones imposed by the pretrained model we loaded:\n",
    "print(ben.get_config().dump(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d93421a-3b54-4433-b197-87fcd5f0498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How green was my valley?\" she said, \"I guess that I know how to find what I need in a small village.\" He grinned. \"I was just about there when the wagon got pulled over. There are few things you never know after two hundred miles of running.\" Her father was going to have to walk all of it for him to get a picture.\n",
      "\n",
      "\"I don't even know how long it takes to get from village to village,\" she said. \"Why do you ask?\"\n",
      "\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "# sample from a start text:\n",
    "ben.sample(\"How green was my valley\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb7fbbc6-9c1a-498a-9720-57d966273a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How green was my valley, my garden, my wildflowers?\n",
      "\n",
      "\n",
      "No, a green was the valley, my garden, as though it were my own, as though I were alone.\n",
      "\n",
      "\n",
      "Now I had lived my life only by the means of the tree that gave me an eye for something to grow in.\n",
      "\n",
      "\n",
      "How green your valley, your garden, your wildflowers?\n",
      "\n",
      "\n",
      "I am now born only for the soil planted around my green garden from a large, beautiful, very beautiful tree\n",
      "--------------------------------------------------------------------------------\n",
      "How green was my valley then?\n",
      "\n",
      "You say the earth is a little different now? Well, maybe that's true. Maybe it is only a part of our very, very existence. Maybe we know all this to be true, but we're all just speculating. There's no way to know.\n",
      "\n",
      "Do you think we're good or bad because we're not trying to be; we're trying to make ourselves good?\n",
      "\n",
      "I'm kind of a hippo about this.\n",
      "\n",
      "But this\n"
     ]
    }
   ],
   "source": [
    "# Sample twice from the same start_text. \n",
    "# Note that any function parameters are overriding config.sample.* entries - in this case config.sample.count\n",
    "ben.sample(\"How green was my valley\", count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa7783ed-d7d5-4acc-b36f-42bf165b2c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt mode: press Enter (single line mode), or Ctrl+D / Ctrl+Z (multiline mode) to submit starting text. Enter -help for available commands.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Flash!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash! With your help, you can help keep these great sites free.\n",
      "\n",
      "Click here to contact the author, or send us an e-mail at [email protected]\n",
      "\n",
      "Follow @joshua_maj\n",
      "\n",
      "Like this: Like Loading...\n",
      "\n",
      "Related<|endoftext|>The world's first, fully automated mobile operating system is scheduled to arrive soon on the shores of Japan. If this is all so exciting, you can rest assured that it is. Even though, most people don't know\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Flash!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash! You have to get your foot out and get back over there and do something amazing like that,\" he announced.<|endoftext|>A study by MIT showed that people who watched films and played video games had a lower risk of dying from cardiovascular diseases and stroke.\n",
      "\n",
      "Researchers said participants who watched more games, video games or games of chance and video games played with simulated blood pressure levels had a 14.1 percent lower risk of a death than people who played a non-video game of chance, as opposed to\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  -quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quitting\n"
     ]
    }
   ],
   "source": [
    "# Now trying the interactive prompt mode:\n",
    "ben.prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b8e90-11b2-4bfb-b4ad-4c812cbd2907",
   "metadata": {},
   "source": [
    "We could also run the above in a python script or from the command line by doing:\n",
    "```\n",
    "python -m gptbench.run -init=gpt2 -mode=prompt\n",
    "```\n",
    "\n",
    "All identifiers for pretrained gpt2 models are:\n",
    "- 'gpt2': 124M params\n",
    "- 'gpt2-medium': 350M params\n",
    "- 'gpt2-large': 774M params\n",
    "- 'gpt2-xl': 1558M params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568533b-01dd-4a8d-84cc-7d0239c3ea94",
   "metadata": {},
   "source": [
    "From here we could then train on new data to fine-tune this pretrained model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
