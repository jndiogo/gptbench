{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4637d54f-2278-492c-bb2d-7277fc1aee63",
   "metadata": {},
   "source": [
    "Let's train the gpt2 model on Shakespeare's plays.\n",
    "\n",
    "Because the model is already large (124M params), we'll set the batch_size to 1 and change the datatype to bfloat16.\n",
    "\n",
    "Trained on 167k lines of Shakespeare plays. Only contains plays, no sonnets or other forms. I tried but could not locate the source of this file and believe it's not copywrited.\n",
    "\n",
    "We won't have a validation dataset here, to maximize training samples, to catch the overall \"style\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98692b76-fe53-4b5a-a27f-71e8436b4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptbench import Train, empty_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b7ef1f3-de29-4e45-a589-e06075a8fae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from gpt2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: dummy 0 tokens\n",
      "Dataset: loading uint16 tokens\n",
      "Expanding initial dataset size of 1 (less than block_size+1) by 1025 times to size of 1025\n",
      "Dataset train_path: dummy empty dataset, val_path: None, train_split: 0.9, vocab_size: 50257\n",
      "Model params: 124.44M\n"
     ]
    }
   ],
   "source": [
    "ben = Train('gpt2')\n",
    "\n",
    "cfg = empty_config()\n",
    "cfg.model.dtype='bfloat16'\n",
    "cfg.trainer.batch_size=1\n",
    "\n",
    "ben.init_pretrained('gpt2', cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d93421a-3b54-4433-b197-87fcd5f0498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: encoding utf-8 to tokens\n",
      "Dataset: loading uint16 tokens\n",
      "Training\n",
      "Iters per epoch: 1394458\n",
      "Iter 0 (0.000 epoch): loss train=3.7150, val=inf, eval->3.7150\n",
      "==> Saving model at iter=0, eval loss->3.7150 \n",
      "Sampling:  considerable file names will grow and disappear when downloading in the last 10 seconds, because they are saved unless you close the browser, then open a new browser window you have read about before, while seeing how annoying that sounds.\n",
      "\n",
      "\n",
      "Text generated by valuable code within the audience.<|endoftext|>Resall-Property Records\n",
      "\n",
      "Mitchell National Forest is one of the biggest three-story military plants on the West Coast of America. Due to mining over the next 75 years, Mitchell National Forest has become a symbol of\n",
      "CUDA max memory used: 2103.92M\n",
      "...................................................................................................\n",
      "Iter 100 (0.000 epoch): loss train=3.5309, val=inf, eval->3.5309\n",
      "==> Saving model at iter=100, eval loss->3.5309 \n",
      "...................................................................................................\n",
      "Iter 200 (0.000 epoch): loss train=3.4936, val=inf, eval->3.4936\n",
      "==> Saving model at iter=200, eval loss->3.4936 \n",
      "...................................................................................................\n",
      "Iter 300 (0.000 epoch): loss train=3.4914, val=inf, eval->3.4914\n",
      "==> Saving model at iter=300, eval loss->3.4914 \n",
      "...................................................................................................\n",
      "Iter 400 (0.000 epoch): loss train=3.4650, val=inf, eval->3.4650\n",
      "==> Saving model at iter=400, eval loss->3.4650 \n",
      "...................................................................................................\n",
      "Iter 500 (0.000 epoch): loss train=3.5106, val=inf, eval->3.5106\n",
      "Sampling:  dogged the angry satire that she's attached to his concept, but who'd has left to his own morbidity what PRINCESS LUCIA and that wicked NEW BRITISH AUTHOR IN MEXICO as against her brilliant chancellor.\n",
      "\n",
      "WHITE:\n",
      "And you and your wife, despite these doubts, I will lie (though I give you severance) happy tidings with my sincere friendship; for about twenty paces, and ye see it jest a trifling reflection of old France\n",
      "...................................................................................................\n",
      "Iter 600 (0.000 epoch): loss train=3.4780, val=inf, eval->3.4780\n",
      "...................................................................................................\n",
      "Iter 700 (0.001 epoch): loss train=3.4269, val=inf, eval->3.4269\n",
      "==> Saving model at iter=700, eval loss->3.4269 \n",
      "...................................................................................................\n",
      "Iter 800 (0.001 epoch): loss train=3.4158, val=inf, eval->3.4158\n",
      "==> Saving model at iter=800, eval loss->3.4158 \n",
      "...................................................................................................\n",
      "Iter 900 (0.001 epoch): loss train=3.4172, val=inf, eval->3.4172\n",
      "..................................................................................................."
     ]
    }
   ],
   "source": [
    "# it loaded using a dummy dataset, because we didn't provide one  - let's set the shakespeare.txt data:\n",
    "# train_split=1 means no validation dataset, to maximize training data\n",
    "ben.set_datasets(class_name='gpt2', # GPT2TokensDataset class\n",
    "                 train_path='../data/shakespeare.txt', \n",
    "                 train_split=1.) \n",
    "\n",
    "# set training log periods to avoid cluttering the training output\n",
    "ben.set_train_log_periods(sample_period=500, dot_period=1, loss_period=0)\n",
    "\n",
    "# and train for 1000 iters\n",
    "ben.train(iter_count=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f509b5f1-cee1-445b-ab2a-20d32ad78a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So it goes, Lucius.\n",
      "Rail there, my lords; bid me know how to procure.\n",
      "\n",
      "ARTHUR:\n",
      "I beg your pardon, sir; after I am sore dumbed by the action,\n",
      "my reign being upon a very deadly high\n",
      "lynch, and I am beheld, over with sound,\n",
      "lukeno, a Roman judge in Naples,\n",
      "stablely lampiated at court, grace; and he counselled\n",
      "me: had ain to solicit Jupiter chucking\n"
     ]
    }
   ],
   "source": [
    "ben.sample(\"So it goes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d87a1dc9-a4e9-4a3f-9cbb-45104e21874d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bermuda:\n",
      "And bloom itself on the street of the emperor:\n",
      "And pray for you, Funtus, for those copper worms\n",
      "deduced from your sight.\n",
      "\n",
      "Perpetua:\n",
      "When all good men three were all twenty feet up,\n",
      "Yet springing seventy foot tall.\n",
      "And those translucent stars which reigned in your eyes\n",
      "Whereon they had all danced to Venus's powerful\n",
      "Austrian maids, Bernadette and Pisa,\n",
      "Whose dashing princes\n"
     ]
    }
   ],
   "source": [
    "ben.sample(\"Bermuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
