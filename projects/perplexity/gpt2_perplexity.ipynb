{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622e47e3-822a-4ab5-b86c-b0ac007b73a9",
   "metadata": {},
   "source": [
    "Measure perplexity (known as PPL) of gpt-2 models over [wikitext-2](https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/) dataset (the test split). Dataset license: [Creative Commons Attribution-ShareAlike 3.0 Unported](https://en.wikipedia.org/w/index.php?title=Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License&ref=blog.salesforceairesearch.com)\n",
    "\n",
    "Perplexity metric for gpt-2 and other models:\n",
    "https://paperswithcode.com/sota/language-modelling-on-wikitext-2\n",
    "\n",
    "Language Models are Unsupervised Multitask Learners:\n",
    "https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceedd206-1330-43b0-929e-cce759a61f24",
   "metadata": {},
   "source": [
    "Let's download the wikitext-2 dataset:\n",
    "```python\n",
    "python ../dataprep/prepare_wikitext2.py\n",
    "```\n",
    "\n",
    "This will download and create these files in the ../data/wikitext-2-raw/ folder:\n",
    "- wiki.train.raw\n",
    "- wiki.test.raw\n",
    "- wiki.valid.raw\n",
    "\n",
    "Although the extension is .raw, these are plain UTF-8 text files. We'll use the wiki.test.raw - the test dataset split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ce8152e-36b3-4829-b4e9-f3290a9111f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from gpt2\n",
      "Dataset: dummy 0 tokens\n",
      "Dataset: loading uint16 tokens\n",
      "Expanding initial dataset size of 1 (less than block_size+1) by 1025 times to size of 1025\n",
      "Dataset train_path: dummy empty dataset, val_path: None, train_split: 0.9, vocab_size: 50257\n",
      "Model params: 124.44M\n"
     ]
    }
   ],
   "source": [
    "from gptbench import Sample, GPT2TokensDataset\n",
    "\n",
    "# load the model\n",
    "ben = Sample(seed=0xA1BED0)\n",
    "\n",
    "# init with the pretrained gpt-2 smallest model - 124M params\n",
    "ben.init_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3696a683-c454-4875-b5a6-141c1a92cc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: encoding utf-8 to tokens\n"
     ]
    }
   ],
   "source": [
    "test_dataset = GPT2TokensDataset(ben.model.block_size, data_path='../data/wikitext-2-raw/wiki.test.raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c82784-345b-4d62-b8cd-192b566f90b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.047134441702813"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# measure model's perplexity from the test_dataset - the test split of wikitext-2,\n",
    "# stride=-1 means we'll measure along non-overlapping blocks of block_size tokens\n",
    "# if you get an out-of-memory exception, lower the max_batch_size param\n",
    "ben.measure_perplexity(test_dataset, stride=-1, max_batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc7b76-c962-4143-b6c3-95be428bc02f",
   "metadata": {},
   "source": [
    "The GPT-2 paper 'Language Models are Unsupervised Multitask Learners' lists PPL=29.41 for the smaller model, our results differ by little."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc3795-8532-4c67-b35f-ca0c5b0565a7",
   "metadata": {},
   "source": [
    "Let's now try with data that GPT-2 was not trained on: a public conference in Portuguese language (DIOGO ROSA MACHADO, year 1900):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1163119c-709a-4e0d-bb7e-30170ac83fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: encoding utf-8 to tokens\n"
     ]
    }
   ],
   "source": [
    "pt_dataset = GPT2TokensDataset(ben.model.block_size, data_path='../data/alexherc1900.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b566a61-c86c-4112-805b-c69b39ff1144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299.6833340903949"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ben.measure_perplexity(pt_dataset, stride=-1, max_batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa9d97-1e3e-4d71-8fe9-524703632f4a",
   "metadata": {},
   "source": [
    "We get much higher perplexity as GPT-2 was not trained in anything similar to this text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43bd85-d884-422d-9ea6-1edf4d56652d",
   "metadata": {},
   "source": [
    "Let's now try with a bigger model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6698ef01-403e-47e3-b1c2-003eb76e1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty cuda cache memory\n",
    "import torch\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb52ecd-64a6-4aa4-b12f-b770eab86d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model from gpt2-large\n",
      "Dataset: dummy 0 tokens\n",
      "Dataset: loading uint16 tokens\n",
      "Expanding initial dataset size of 1 (less than block_size+1) by 1025 times to size of 1025\n",
      "Dataset train_path: dummy empty dataset, val_path: None, train_split: 0.9, vocab_size: 50257\n",
      "Model params: 774.03M\n"
     ]
    }
   ],
   "source": [
    "# init with the pretrained gpt-2 large model - 774M params\n",
    "ben.init_pretrained('gpt2-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd44abd6-8b36-4eca-8835-93d2991e1412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.08091968456662"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you get an out-of-memory exception, lower the max_batch_size param\n",
    "ben.measure_perplexity(test_dataset, stride=-1, max_batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db973f71-b9ec-4213-8943-8928db47a9b2",
   "metadata": {},
   "source": [
    "The GPT-2 paper lists PPL=19.93 for the gpt2-large model and we obtained 18.08. Why the difference?\n",
    "\n",
    "According to this post from one of the authors, the GPT-2 paper results were taken with a stride of 32 (we're using a stride of 1024, due to the stride=-1 param), however using stride=32 should give us an even lower perplexity (losses from the beggining of context are not used). \n",
    "\n",
    "https://www.reddit.com/r/MachineLearning/comments/oye64h/r_struggling_to_reproduce_perplexity_benchmarks/\n",
    "\n",
    "If not due to the stride, it can be due to different dataset formatting, for example, sometimes dataset entries are joined with \"\\n\\n\", single space lines in the original raw text are reported as empty lines (both as in [here](https://huggingface.co/docs/transformers/perplexity)).\n",
    "\n",
    "PPL scores are hard to compare, unless conditions are exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c573d78-277c-4998-a2f4-3d0d05b208ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.317235740722698"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's measure with a 512 stride:\n",
    "ben.measure_perplexity(test_dataset, stride=512, max_batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c3e15-5dca-4624-ae86-3194d0c892d6",
   "metadata": {},
   "source": [
    "Lower perplexity - this happens because:\n",
    "\n",
    "GPT-2 was evaluated with a small stride: 32. The reason it gives lower perplexity is because transformer LMs (...) have a finite context size so when you do eval stride length = context length your model is always having to predict some subset of tokens with little to no context (the ones at the beginning of each stride / eval window). It's much harder to predict these tokens (since you have no context!) and empirically they have much higher loss. By using a full context sized window that slides by a smaller stride you're then only evaling the tokens at the end which have ~ full context.\n",
    "https://www.reddit.com/r/MachineLearning/comments/oye64h/r_struggling_to_reproduce_perplexity_benchmarks/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
